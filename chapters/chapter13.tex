\chapter{The category zipper}
\label{ch:the-category-zipper}

\epigraph{If you have two complementary strands of DNA, they zip up. That's what they do.}{— Sri Kosuri, quoted in \textit{Harvard Gazette} (2019)}

Part~III has examined four categories from the inside~-- countability, definiteness, lexical categories, gender~-- each with its own cluster, its own stabilizers, its own pattern of boundary behaviour. This chapter steps back and maps the architecture.

Section~\ref{sec:8:coupling} introduced a coupling continuum: hard-coupled HPCs, where form directly realizes contrastive value; loosely coupled HPCs, where form and meaning drift independently; re-unified HPCs, where constructions bind them together again. The question now is whether this continuum illuminates not just the grammatical categories of Part~III but the full stack of linguistic organization, from phoneme to construction. The answer is yes~-- and the pattern that emerges tells us something about where grammaticality lives and why.

We start at the hard end.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Hard coupling: phonemes}
\label{sec:13:phonemes}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The phoneme tier is where hard coupling is most visible. The three terms of the triadic structure (§\ref{sec:7:what-projection-is}) are distinct and independently measurable:

\begin{itemize}
    \item \textbf{Form}: acoustic signal~-- formant distributions, voice-onset time, spectral shape.
    \item \textbf{Object}: contrastive identity~-- /\ipa{k}/ as not-/\ipa{g}/, not-/\ipa{t}/, not-/\ipa{p}/. This is what the form \emph{is about} in the system.
    \item \textbf{Interpretant}: lexical access and downstream processing~-- hearing /\ipa{k}/ triggers retrieval of \mention{cat}, not \mention{gat}; inferences follow.
\end{itemize}

The coupling is transparent: form determines object determines interpretant, with minimal slippage. Errors stay inside the system~-- one phoneme for another~-- because the interpretant follows the object without further computation.

What maintains this tight coupling? Four stabilizer types. First, quantal regions: \textcite{stevens1989} showed that the articulatory-to-acoustic mapping is non-linear, with certain configurations producing stable outputs across a range of variation~-- natural \enquote{parking spots} that constrain what phoneme contrasts are \emph{possible}. Second, dispersion pressure: \textcite{lindblom1990} argued that vowel inventories disperse in acoustic space to maximize discriminability, shaping which of the possible phonemes a language \emph{selects}. Third, perceptual tuning: \textcite{kuhl1992} demonstrated that infant perception is warped by early exposure, with category prototypes acting as perceptual magnets that tune the individual speaker to the community standard. Fourth, community norms: social transmission across generations stabilizes inventories through the mechanisms that cultural-tool accounts emphasize \parencite{ekstrom2025}.

Evidence comes from PHOIBLE 2.0 \parencite{moran2019phoible}, a database of phoneme inventories for roughly 2,700 languages. Two patterns satisfy the diagnostics. Plotting kernel-density ridgelines of total inventory sizes by family reveals a \emph{stability band}: medians cluster between 20 and 50 segments across unrelated families, with thin tails beyond. This isn't an artifact of pooling; it's cross-family regularity enabling inventory-level projection. Meanwhile, the vowel /y/~-- a front rounded vowel requiring precise articulatory coordination~-- shows a \emph{scaling curve}: a logistic model predicting /y/-presence from vowel-inventory size (cross-validated, grouped by family) achieves ROC-AUC $\approx 0.70$. Marked segments lacking quantal robustness appear mainly in larger systems where there's acoustic room. This is exactly what the stabilizer story predicts.

The \mention{pin}/\mention{pen} merger provides a stress test. In much of the American South and parts of the Midland, /\ipa{ɪ}/ and /\ipa{ɛ}/ have merged before nasal consonants \parencite{labov2006}. Speakers produce the same vowel in \mention{pin} and \mention{pen}; they can't reliably distinguish the two words by ear. The conditioning environment is revealing: before nasals, the vowels are subject to nasalization, which smears the formant cues. In exactly the environment where acoustic cues are least reliable, the contrast collapses. This is form--object coupling failing in a predictable way. The interpretant follows~-- speakers can't distinguish the words~-- because two forms now map to the same object. The triadic structure makes the failure legible.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Opaque coupling: words}
\label{sec:13:words}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

Move one level up and the coupling loosens.

At the phoneme level, form directly realizes contrastive value. At the word level, form and object come apart. The form \mention{went} encodes pastness, but you can't read the object off the form~-- the connection is brute memory. This is \term{opaque coupling}.

The triadic structure at the word level:

\begin{itemize}
    \item \textbf{Form}: orthographic/phonological shape~-- \mention{went}, \mention{go}, \mention{dog}.
    \item \textbf{Object}: conventional pairing~-- pastness, motion, canine-kind. The connection is arbitrary; you can't read the object off the form.
    \item \textbf{Interpretant}: the inferences the word enables~-- temporal location, argument-structure expectations, encyclopedic knowledge activation.
\end{itemize}

The coupling is opaque at form--object (brute memory) but stable at object--interpretant (once you've accessed the object, the interpretant follows). Errors at this level are typically retrieval failures, not composition failures.

\textcite{miller2021} develops an HPC stance at the level of particular lexemes~-- \mention{dog}, \mention{run}, \mention{egregious}~-- rejecting essence-based individuation in favour of mechanism-indexed clusters that are historically delimited and population-relative. The lexeme \mention{dog} maintains its identity not through a platonic essence but because spelling conventions, pronunciation norms, semantic associations, and syntactic patterns travel together, stabilized by orthographic standardization (educational institutions, publishing practices), frequency entrenchment (repetition in memory automating retrieval; \citealt{bybee2001}), editorial norms (copy-editing workflows flagging nonstandard usage), and register licensing (genre conventions sanctioning certain words in certain contexts).

The question is whether words can change semantically and still remain HPC kinds. The HistWords COHA embeddings \parencite{hamilton2016} provide decade-binned distributional representations for English words over the 20th century. High-drift adjectives (top decile of average cosine displacement, with documented drift terms like \mention{nice}, \mention{sick}, \mention{gay}, \mention{awful} forced in) are compared to frequency-matched controls with minimal drift. Some drift adjectives retain organized neighbourhoods even as their centres move: for \mention{nice}, nearest neighbours shift from \{pretty, lovely, pleasant\} in the 1900s to \{cute, wonderful, really\} by the 2000s; for \mention{sick}, from \{ill, tired, hungry\} to \{hurt, drunk, upset\}. A prototype classifier trained on early decades (1900--1940) and tested on later decades (1950--2000) recovers word identity well above baseline (macro-F1 = 0.84 vs.\ 0.03). Combined with a cohesion cutoff, only a subset of high-drift adjectives satisfies both criteria. The framework tells us to withhold kindhood for those lexeme--time slices rather than forcing a positive verdict. For the subset that passes, past usage fixes expectations that carry forward~-- exactly what the HPC picture predicts.

The \mention{go}--\mention{went} pattern provides a stress test. If form--object pairings are maintained by frequency and analogy, high-frequency irregulars should resist regularization; low-frequency irregulars should be vulnerable. \textcite{bybee2001} documents exactly this. High-frequency irregulars (\mention{go}--\mention{went}, \mention{have}--\mention{had}) show no regularization pressure. Mid-frequency irregulars (\mention{weave}--\mention{wove}) show variable forms. Low-frequency irregulars (\mention{cleave}--\mention{clove}) have largely regularized. Frequency protects the arbitrary pairing; analogy regularizes the unprotected. The two stabilizers interact in predictable ways.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Loose coupling: the grammatical categories}
\label{sec:13:grammatical-categories}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

Between the hard coupling of phonemes and the re-unified coupling of constructions sits the territory Part~III has been exploring. Grammatical categories live where form and meaning are connected but separable~-- where the coupling is loose enough that the two poles can drift independently, maintained by different mechanisms at different strengths.

The four case studies illustrate different positions on this range.

\subsection{Countability: tight grammatical coupling}

Countability (Chapter~\ref{ch:countability}) is the closest to hard coupling that a grammatical category gets. The individuation cluster (semantic: atomic, enumerable, bounded) and the count cluster (morphosyntactic: plural marking, quantifier selection, determiner agreement) are coupled by bidirectional inference~-- count morphosyntax cues individuation in comprehension; individuated construal selects count morphosyntax in production. The coupling is tight enough to produce an implicational hierarchy: items that take the tightest constructions (\mention{a}, cardinal numerals) also take the looser ones (\mention{much}, \mention{less}), but not vice versa. Perturbation testing confirms the diagnosis. Heritage speakers with weakened transmission maintain the tightest couplings (basic count/mass) while the looser ones erode. The stress tests~-- \mention{two beers}, \mention{a furniture}, the re-analysis of \mention{pease} into \mention{pea}~-- show the bidirectional mechanism in action: when one pole shifts, pressure propagates to the other.

\subsection{Definiteness: overlapping HPCs}

Definiteness (Chapter~\ref{ch:definiteness-and-deitality}) is looser. The semantic cluster (identifiability, familiarity, uniqueness) and the form cluster (the article, demonstratives, possessives~-- what the chapter calls \term{deitality}) overlap but don't align. Weak definites (\mention{go to the hospital}) have the form without the semantics. Proper names have the semantics without the form. Generic definites (\mention{the whale is a mammal}) have the form with a reading orthogonal to identifiability. The coupling is maintained by Millikan's proper function~-- \mention{the} was selected for signalling identifiability, and it still performs that function often enough to sustain the convention~-- but derived functions exploit stable side effects, producing systematic slippage. Two overlapping HPCs, not one, each with its own stabilizer profile.

\subsection{Lexical categories: variable coupling}

Lexical categories (Chapter~\ref{ch:lexical-categories}) are variable. Noun and verb are tightly coupled across languages~-- the semantic poles (thing-like, event-like) and the morphosyntactic poles (argument structure, inflectional paradigms) cohere, maintained by robust mechanisms at every level. Adjective is language-dependent: English has a large, open, well-maintained adjective category; many languages fold adjectival semantics into verb or noun morphosyntax, with no independent adjective class. Adverb is a wastebasket~-- a fat class whose subclusters (manner, degree, sentence) have different coupling regimes. The pronoun family sits at an interesting point: the syntactic distribution is shared (NP substitution) but the semantic contributions diverge (personal reference, indefinite quantification, interrogation). The coupling holds where the mechanisms braid; it frays where they don't.

\subsection{Gender: designatum-driven coupling}

Gender (Chapter~\ref{ch:proform-gender}) adds a twist: the coupling follows the speaker's construal of the referent. The personhood cluster (semantic: attributed person-status) and the pro-form cluster (morphosyntactic: \mention{who}/\mention{which}, \mention{somebody}/\mention{something}, \mention{he}/\mention{she}/\mention{it}) are coupled by designatum-driven inference~-- the form tracks how the speaker is conceptualizing the referent, not a fixed grammatical property of the antecedent. Chain coherence enforces consistency within a discourse. The coupling is transparent where personhood is unambiguous (humans, inanimate objects) and gradient where it isn't (animals, ships, institutions). This is loose coupling with a live semantic link~-- the form follows meaning in real time, not by rote.

\subsection{The pattern}

What the four case studies share is a two-cluster architecture: a semantic pole and a morphosyntactic pole, coupled by inference and maintained by overlapping mechanisms. What they differ in is coupling tightness. Countability is tight; definiteness is loose; lexical categories are variable; gender is designatum-driven. The coupling continuum from §\ref{sec:8:coupling} isn't just a theoretical possibility. It's where Part~III has been living.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Re-unified coupling: constructions}
\label{sec:13:constructions}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

Above the level of individual categories, constructions re-bind what loose coupling lets drift. Conventionalized pairings of form and meaning~-- the ditransitive, the \mention{let alone} construction, the way-construction~-- rely on multiple converging cues that speakers recognize as a gestalt. The form--object coupling is \emph{re-unified}: built up from components, maintained by use, transmitted through interaction.

The triadic structure at the construction level:

\begin{itemize}
    \item \textbf{Form}: the formal template~-- word order, morphological marking, prosodic contour.
    \item \textbf{Object}: the semantic/pragmatic template~-- what the construction \emph{means} as a conventional pairing.
    \item \textbf{Interpretant}: the discourse expectations it sets up~-- what the hearer is licensed to infer, what follow-up is appropriate.
\end{itemize}

The coupling is re-unified: form--object links are conventional but compositional; interpretants must be computed from assembled pieces. When the composition fails~-- local form--object links are fine, but the global interpretant doesn't cohere~-- the result is a garden-path or a misparse.

Three stabilizer types maintain constructions. Frequency and entrenchment work on millisecond-to-week timescales: each token use strengthens memory traces, increasing production probability, generating more tokens in a self-reinforcing loop \parencite{bybee2001}. Cue redundancy provides robustness: multiple formal features converge on the same interpretation, so if parallelism fails in a rushed email, the anchor string and licensing context still signal the construction. Normative pressure operates on year-to-decade timescales: editorial practices and style guides reinforce the canonical pattern, especially in formal registers.

If the construction tier is to do more than demonstrate feasibility, the diagnostics have to survive contact with heterogeneous constructions. The confirmatory analysis uses a battery of ten constructions spanning four cue regimes, tested across UD English corpora (GUM, EWT, GUMReddit). Eight are treated as positive candidates; two are included as designed brakes cases~-- pooled resultatives (predicted \enquote{too fat}) and \mention{X much?} (predicted register-local)~-- to force the framework to say no when it should. For the \mention{or even} construction, a classifier trained on GUM and tested on EWT achieves PR-AUC of 0.886 (full bundle); dropping parallelism reduces PR-AUC to 0.612. Ablation signatures show parallelism as a dominant stabilizer for scalar-additive constructions. The pooled resultative shows weak cross-corpus transfer and washed-out ablation signatures~-- exactly what the \enquote{too fat} diagnosis predicts. The \mention{X much?} construction falls below prevalence thresholds outside informal registers, confirming register-locality. The implication is deliberately narrow: the construction tier doesn't license a blanket HPC claim. Kindhood is earned case-by-case.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{The stabilizer-weighting map}
\label{sec:13:map}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The stack~-- phonemes, words, grammatical categories, constructions~-- isn't a ladder. Its levels are regions in a stabilization space defined by how form couples to object to interpretant, and which links bear the load. Table~\ref{tab:13:stabilizer-map} summarizes the mapping.

\begin{table}[htbp]
\centering
\caption{Triadic coupling across the stack. Each level shows the coupling regime, the vulnerable link, what the interpretant does, and the dominant stabilizers.}
\label{tab:13:stabilizer-map}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Level} & \textbf{Coupling} & \textbf{Vulnerable link} & \textbf{Interpretant} & \textbf{Dominant stabilizers} \\
\midrule
Phoneme & Hard & Form--Object & Lexical access & Quantal regions, dispersion, perceptual magnets \\
Word & Opaque & Form--Object & Conceptual activation & Frequency, editorial norms, analogy \\
Grammar & Loose & Semantic--Formal & Bidirectional inference & Entrenchment, alignment, transmission \\
Construction & Re-unified & Object--Interpretant & Compositional inference & Cue redundancy, normative pressure \\
Discourse & Loose & Interpretant & Pragmatic uptake & Common ground, accommodation, repair \\
\bottomrule
\end{tabular}
\end{table}

The stabilizers form a braid, not a single cause. At the phoneme tier, biophysical constraints carve the design space, developmental learning binds cues, sociocultural norms transmit inventories. At the word tier, frequency entrenches forms, editorial standards enforce conventions, usage communities police extensions. At the construction tier, cue redundancy protects against noise, normative pressure corrects deviations, genre licensing regulates distribution. The mechanisms shift in their balance: articulatory constraints weigh heavily for phonemes, frequency and norms for words, cue redundancy and editorial pressure for constructions. But at every tier, multiple forces interact: body, cognition, and society always contribute.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{The mediation gradient}
\label{sec:13:mediation}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The coupling continuum has a Peircean interpretation. In the triadic structure of sign, object, and interpretant (§\ref{sec:7:what-projection-is}), the interpretant's role shifts as we move up the stack.

At the phoneme level, the interpretant is nearly automatic. Form determines object (contrastive identity); object determines interpretant (lexical access). The sign functions almost as a brute index~-- a connection with minimal interpretive work. This is correlation in Peirce's sense: the relation holds because of physical and perceptual constraints, not because the interpretant \emph{constitutes} it.

At the word level, the interpretant does more. The form--object pairing is conventional~-- brute memory, not acoustic necessity~-- and the interpretant must be learned, not merely perceived. But once activated, it follows reliably. The habit is stored, not computed.

At the grammatical-category level, mediation becomes central. The interpretant doesn't just follow from the form--object pairing; it \emph{mediates} between a semantic construal and a morphosyntactic form, coupling them through bidirectional inference. This is the territory of would-bes: the category generates inductive habits that extend to unencountered instances. The interpretant is what the maintenance buys.

At the construction level, the interpretant constitutes the unit. Without the conventionalized inferential habit, the form-meaning pairing doesn't exist as a construction~-- it's just a sequence of words. The habit is what makes the teeth engage.

The gradient isn't a ladder of complexity. It's a map of where the would-be bears the load. At the hard end, correlation does most of the work and mediation is minimal. At the re-unified end, mediation does all of it. The grammatical categories of Part~III sit in the middle~-- the zone where the interpretant mediates actively, where would-bes are conditional on field and purpose, and where the coupling is loose enough that different analytical perspectives can carve differently (§\ref{sec:7:field-relative}). This is why grammatical categories are the most interesting and the most contentious: the mediation is strong enough to sustain real projectibility, but loose enough to permit field-relative decomposition.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Negative cases: when the framework says no}
\label{sec:13:negative}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The HPC framework would be toothless if it said yes to everything. Here are three cases where it says no~-- each for a different reason.

\emph{Academic register} has a recognizable flavour: passive constructions, nominalizations, hedges like \mention{it has been argued}. The features cluster; experienced readers identify academic prose instantly. But this clustering isn't a homeostatic kind; it's a \term{conditioning effect} (see Chapter~\ref{ch:social-stabilization}). Formally, if a speaker's dialect provides a durable baseline parameterisation $\theta_D$, a register is a situational variable $R$ that shifts the probabilities of competing variants: $P(Y \mid R, \theta_D)$. The \enquote{Academic} cluster is just the output of this reweighting. Put the same researcher in front of a grant panel, a blog audience, and a conference poster, and the passives evaporate, the hedges reweight. The bundle doesn't resist perturbation; it \emph{is} the perturbation. It fails the diagnostic because it describes a temporary state of the system, not a stable component. It belongs in the same ontological category as \enquote{shouting}~-- real, consequential, but defined by the act, not the actor.

\emph{Indo-European} is a language family defined by historical descent. The cluster properties~-- cognate vocabulary, shared sound correspondences, similar grammatical patterns~-- are real. But ask what would push English \emph{back toward} Proto-Indo-European. Nothing. There's no homeostatic pressure maintaining Indo-European-ness. English drifts away from PIE continuously; contact with non-IE languages accelerates the drift. Historical kinds are defined by causal continuity with an origin; homeostatic kinds are defined by stabilizers that maintain covariance. Confusing them invites explanatory error.

\emph{Polysynthetic languages} are characterized by high morpheme-to-word ratios, incorporating structures, and complex verb templates. Mohawk, Chukchi, and Ainu are standard examples. But the same stabilizers don't maintain polysynthesis across families. Mohawk (Iroquoian) has noun incorporation for discourse-pragmatic reasons~-- incorporated nouns are non-referential \parencite{mithun1984}. A single Mohawk word like \textit{Washakotya'tawitsherahetkvhta'se} encodes what English requires a full clause for: `he made the thing that one puts on one's body ugly for her'~-- i.e., `he ruined her dress' \parencite[40]{baker1996}. Chukchi (Chukotko-Kamchatkan) has incorporation with different constraints and different discourse functions. The surface similarity~-- complex words~-- masks heterogeneous routes. \enquote{Polysynthetic} is a typological region label, useful for description but not for explanation: it doesn't name a mechanism-maintained kind.

The three cases fail for different reasons~-- conditioning effect rather than stable kind (academic register), historical rather than homeostatic maintenance (Indo-European), heterogeneous routes to surface similarity (polysynthetic). The HPC framework distinguishes these failure modes. That's the payoff of explicit diagnostics: you can say \emph{why} something doesn't qualify, not just that it doesn't.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Predictions and disconfirmers}
\label{sec:13:predictions}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The diagnostics generate falsifiable predictions beyond the case studies. Weakening a stabilizer should reduce cluster covariance before norms re-stabilize; for constructions, downsampling training data to 25\% should degrade PR-AUC substantially if the bundle depends critically on frequency. Just as /y/ appears preferentially in larger vowel inventories, rare constructional variants should concentrate in corpora with larger constructicon repertoires. If a pooled category like \enquote{resultative} is genuinely \enquote{too fat}, stratifying into subtypes should restore projectibility for well-maintained local kinds. These tests operationalize the core claim: linguistic categories qualify as HPCs when they project via identifiable mechanisms.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{The architecture of maintenance}
\label{sec:13:forward}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The same framework~-- property clusters, stabilizing mechanisms, the two-diagnostic test~-- applies from the phoneme to the construction. What changes across levels isn't the logic but the coupling: how tightly form and meaning bind, where the interpretant bears the load, and which stabilizers dominate.

This is the scale-invariant claim. The HPC diagnostics don't privilege a level. Phonemes, words, grammatical categories, and constructions all either pass or fail the same tests. The framework is a sieve, not a spotlight: it catches whatever the mechanisms hold together, at whatever grain they operate.

But one location has special status. Throughout Part~III~-- countability, definiteness, lexical categories, gender~-- the categories clustered at the morphosyntactic level. That's where form--object--interpretant coupling is both \emph{tight} and \emph{obligatory}. You can speak without using \mention{let alone}. In English, you can't speak without committing to number, tense, and definiteness.

Morphosyntax is the zone of maximum systematicity: the region of grammar where interpretant generation is enforced, not optional.

We can measure this tightness. \textcite{Reynolds2026} quantifies the \term{packaging tightness} of the determiner--head relationship in English~-- the link that the Left Branch Gap (§\ref{sec:8:negative}) fails to break. Using a dependency locality metric, he finds that determiners and their heads exhibit a packaging score of $k=4$ (extremely tight). The system treats $D+N$ not as neighbours but as a rigid transmission unit. This is what obligatory interpretant generation looks like in the data: a coupling so strong that the elements refuse to be separated.

Chapter~\ref{ch:grammaticality-itself} asks what happens when we take this observation seriously. If grammaticality is what emerges when form--object--interpretant coupling is obligatory, compositional, and learnable~-- then grammaticality itself is a kind. The interpretant isn't just what you get; it's what you \emph{must} get. The zipper doesn't just pull the teeth together; it locks them.
