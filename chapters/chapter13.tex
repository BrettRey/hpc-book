\chapter{The category zipper}
\label{ch:the-category-zipper}

\epigraph{If you have two complementary strands of DNA, they zip up. That's what they do.}{— Sri Kosuri, quoted in \textit{Harvard Gazette} (2019)}

Part~III has examined four categories from the inside~-- countability, definiteness, lexical categories, gender~-- each with its own cluster, its own stabilizers, its own pattern of boundary behaviour. This chapter steps back and maps the architecture: not four case studies in a row, but four points on a single mechanism-defined gradient.

Section~\ref{sec:8:coupling} introduced a coupling continuum: hard-coupled HPCs, where form directly realizes contrastive value; loosely coupled HPCs, where form and meaning drift independently; composite HPCs, where constructions bind them together conventionally. The question now is whether this continuum illuminates not just the grammatical categories of Part~III but the full length of the zipper, from phoneme to construction. It does.

The organizing question is constant across the stack: \emph{which link bears the load?} At some levels, form largely determines the object and the interpretant mostly follows. At others, the object has to be inferred and the interpretant does active mediating work. At others still, the unit exists only because a conventional inferential habit makes it exist.

So the chapter proceeds by a controlled ascent. We start at the hard end, where the system is so tightly coupled that even its mistakes are well-behaved. Then we move to words, where form--object pairing is arbitrary but retrieval is stable. Then to grammatical categories, where semantic and formal clusters negotiate alignment. Then to constructions, where cue bundles make a gestalt and the interpretant constitutes the unit. At each tier, the diagnostic question is the same: \emph{what keeps the teeth engaged, and what makes them slip?}

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Hard coupling: phonemes}
\label{sec:13:phonemes}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The phoneme tier is where hard coupling is most visible. The three terms of the triadic structure (§\ref{sec:7:what-projection-is}) are distinct and independently measurable:

\begin{itemize}
    \item \textbf{Form}: acoustic signal~-- formant distributions, voice-onset time, spectral shape.
    \item \textbf{Object}: contrastive identity~-- /\ipa{k}/ as not-/\ipa{g}/, not-/\ipa{t}/, not-/\ipa{p}/. This is what the form \emph{is about} in the system.
    \item \textbf{Interpretant}: lexical access and downstream processing~-- hearing /\ipa{k}/ triggers retrieval of \mention{cat}, not \mention{gat}; inferences follow.
\end{itemize}

The coupling is transparent: form largely determines the object, which in turn determines the interpretant, with minimal slippage. Errors predominantly stay inside the system~-- one phoneme for another~-- because the interpretant follows the object without further computation.

What maintains this tight coupling? The stabilizers are the causal-maintenance mechanisms. Here we find four stabilizer types. First, quantal regions: \textcite{stevens1989} showed that the articulatory-to-acoustic mapping is non-linear, with certain configurations producing stable outputs across a range of variation~-- natural \enquote{parking spots} that constrain what phoneme contrasts are \emph{possible}. Second, dispersion pressure: \textcite{lindblom1990} argued that vowel inventories disperse in acoustic space to maximize discriminability, shaping which of the possible phonemes a language \emph{selects}. Third, perceptual tuning acting as a stabilization mechanism: \textcite{kuhl1992} demonstrated that infant perception is warped by early exposure, with category boundaries becoming phonetic attractors that tune the individual speaker to the community standard. Fourth, community norms: social transmission across generations stabilizes inventories through the mechanisms that cultural-tool accounts emphasize \parencite{ekstrom2025}.

Evidence comes from PHOIBLE 2.0 \parencite{moran2019phoible}, a database capturing 3,020 inventories across 2,186 distinct languages. Two patterns satisfy the diagnostics. Plotting kernel-density ridgelines of total inventory sizes by family reveals a \emph{stability band}: medians often cluster between 20 and 50 segments across unrelated families, with thin tails beyond. This isn't an artifact of pooling; it's cross-family regularity enabling inventory-level projection. Meanwhile, the vowel /y/~-- a front rounded vowel requiring precise articulatory coordination~-- shows a \emph{scaling curve}: a logistic model predicting /y/-presence from the total vowel count of a language's inventory (cross-validated via grouped splits to block family-level confounding) discriminates effectively between inventories that have the sound and those that don't (in our full-stack analysis, Area Under the Receiver Operating Characteristic curve, ROC-AUC $\approx 0.70$, well above the 0.50 baseline of chance guessing; \citealt{Reynolds2026}). This predictive power confirms that the biological constraints of articulation objectively shape the inventory even as individual sounds shift. Marked segments lacking quantal robustness appear mainly in larger systems where there's acoustic room. This is exactly what the stabilizer story predicts.

The \mention{pin}/\mention{pen} merger provides a stress test. In much of the American South and parts of the Midland, /\ipa{ɪ}/ and /\ipa{ɛ}/ have merged before nasal consonants \parencite{labov2006}. Speakers produce the same vowel in \mention{pin} and \mention{pen}; many speakers can't reliably distinguish the two words by ear. The conditioning environment is revealing: before nasals, the vowels are subject to nasalization, which smears the formant cues. In exactly the environment where acoustic cues are least reliable, the contrast collapses. This is form--object coupling failing in a predictable way. The interpretant follows~-- speakers can't distinguish the words~-- because two forms now map to the same object. The triadic structure makes the failure legible.

Hard coupling is what you get when the vulnerable link is shallow: small changes in form change the object, and the interpretant follows with little mediation. When the link fails, it fails locally and predictably, because the system is built to keep contrasts audible.

Move one level up and the link that stays vulnerable is still form--object, but the reason changes. The next tier is driven not by acoustic necessity, but by arbitrary convention stabilized in memory.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Opaque coupling: words}
\label{sec:13:words}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

Move one level up and the coupling loosens.

At the phoneme level, form realizes contrast. At the word level, form merely points. The form \mention{went} is past by convention, not by decomposable form~-- the connection is brute memory. The system does not explain \mention{went}; it remembers it. This is \term{opaque coupling}.

The triadic structure at the word level:

\begin{itemize}
    \item \textbf{Form}: orthographic/phonological shape~-- \mention{went}, \mention{go}, \mention{dog}.
    \item \textbf{Object}: conventional pairing~-- pastness, motion, canine-kind. The connection is arbitrary; you can't read the object off the form.
    \item \textbf{Interpretant}: the inferences the word enables~-- temporal location, argument-structure expectations, encyclopedic knowledge activation.
\end{itemize}

The coupling is opaque at form--object (brute memory) but stable at object--interpretant (once you've accessed the object, the interpretant follows). Errors at this level are characteristically retrieval failures, not composition failures.

\textcite{miller2021} develops an HPC stance at the level of particular lexemes~-- \mention{dog}, \mention{run}, \mention{egregious}~-- rejecting essence-based individuation in favour of mechanism-indexed clusters that are historically delimited and population-relative. The lexeme \mention{dog} maintains its identity not through a platonic essence but because spelling conventions, pronunciation norms, semantic associations, and syntactic patterns travel together, stabilized by orthographic standardization (educational institutions, publishing practices), frequency entrenchment (repetition in memory automating retrieval; \citealt{bybee2001}), editorial norms (copy-editing workflows flagging nonstandard usage), and register licensing (genre conventions sanctioning certain words in certain contexts).

The question is whether words can change semantically and still remain HPC kinds. The HistWords COHA embeddings \parencite{hamilton2016} provide decade-binned distributional representations for English words over the 20th century. (Embeddings are not telepathy; they are distribution with delusions of grandeur.) High-drift adjectives (top decile of average cosine displacement, with documented drift terms like \mention{nice}, \mention{sick}, \mention{gay}, \mention{awful} forced in) are compared to frequency-matched controls with minimal drift. Some drift adjectives retain organized neighbourhoods even as their centres move: for \mention{nice}, nearest neighbours shift from \{pretty, lovely, pleasant\} in the 1900s to \{cute, wonderful, really\} by the 2000s; for \mention{sick}, from \{ill, tired, hungry\} to \{hurt, drunk, upset\}. A nearest-centroid prototype classifier trained on early decades (1900--1940) and tested on later decades (1950--2000) recovers semantic class assignments (over 33 WordNet supersense domains) with a balanced accuracy metric (in our analysis, a macro-averaged F1 score, macro-F1, of 0.84, heavily outperforming the 0.03 class-stratified random-guessing baseline; \citealt{Reynolds2026}). This strong classifier performance demonstrates that historical drift creates measurable, cohesive trajectories, not random isotopic decay. Combined with a cohesion cutoff, only a subset of high-drift adjectives satisfies both criteria. The framework tells us to withhold kindhood for those lexeme--time slices rather than forcing a positive verdict. For the subset that passes, past usage fixes expectations that carry forward~-- exactly what the HPC picture predicts.

The \mention{go}--\mention{went} pattern provides a stress test. If form--object pairings are maintained by frequency and analogy, high-frequency irregulars should resist regularization; low-frequency irregulars should be vulnerable. \textcite{bybee2001} documents exactly this. High-frequency irregulars (\mention{go}--\mention{went}, \mention{have}--\mention{had}) show no regularization pressure. Mid-frequency irregulars (\mention{weave}--\mention{wove}) show variable forms. Low-frequency irregulars (\mention{cleave}--\mention{clove}) have largely regularized. Frequency protects the arbitrary pairing; analogy regularizes the unprotected. The two stabilizers interact in predictable ways.

Opaque coupling is still coupling: once the object is retrieved, the interpretant is stable. What is fragile is the arbitrariness itself, and the stabilizers that protect it are the ones that protect conventions: frequency, analogy, and norms.

The next tier is different in kind. In grammatical categories, form--object arbitrariness is no longer the main problem. The problem is that there are \emph{two} objects in play~-- semantic and formal clusters~-- and the interpretant has to keep them aligned.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Loose coupling: the grammatical categories}
\label{sec:13:grammatical-categories}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

Between the hard coupling of phonemes and the composite coupling of constructions sits the territory Part~III has been exploring. What Part~III establishes is that \enquote{loose coupling} isn't structural autonomy; it's flexibility of mapping. The semantic and formal poles don't exist in ontological isolation. Instead, they form a two-cluster architecture coupled by inference and maintained by overlapping mechanisms.

The four case studies illustrate the spectrum of this flexibility. Rather than a monolithic grammar module, we see a topography of coupling tightness. The zipper metaphor is useful here because it forces the same question at each tooth: not \enquote{does it exist?} but \enquote{what keeps it engaged?} Table~\ref{tab:13:matrix} synthesizes the findings.

\begin{table}[htbp]
\centering
\caption{Comparison matrix of grammatical categories analyzed in Part~III. Each category occupies a distinct position on the coupling-tightness spectrum, with its own signature diagnostic facts and failure modes.}
\label{tab:13:matrix}
\begin{tabular}{@{}p{0.15\textwidth} p{0.15\textwidth} p{0.3\textwidth} p{0.3\textwidth}@{}}
\toprule
\textbf{Category} & \textbf{Coupling Tightness} & \textbf{Signature Diagnostic Fact} & \textbf{Failure Mode} \\
\midrule
Countability & Tight & Bidirectional inference forms an implicational hierarchy. Heritage speakers retain the core but lose the margins. & Independent drift (e.g., \mention{a furniture}), semantic pressure propagating to form (\mention{pease} $\rightarrow$ \mention{pea}). \\
Definiteness & Loose (Cross-cutting) & \term{Deitality} forms one cluster; identifiability another. They overlap extensively but not exhaustively. & Form without semantics (weak definites); semantics without form (proper names). \\
Lexical Categories & Variable & Noun and verb couple tightly cross-linguistically; adjective is thin and language-specific; adverb is a wastebasket. & Braids fraying where mechanism stabilization fails, yielding \enquote{fat} wastebasket classes. \\
Gender & Designatum-driven & Pro-form selection tracks active conceptualization of the referent (personhood), not a fixed grammatical property. & Anaphoric mismatch driven by situational shifts; gradient compressibility. \\
\bottomrule
\end{tabular}
\end{table}

Loose coupling is not looseness of structure; it is looseness of \emph{mapping}. The clusters are real and maintained, but the alignment is negotiated and field-relative, which is why the same lexical space can support multiple honest carvings. If the same data support several honest carvings, the problem is not that the data are fuzzy; it is that the world is doing more than one job at once.

If this were the whole story, the framework would be too generous. Before we climb to constructions, the framework has to earn its right to generalize by saying no where it should.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Negative cases: when the framework says no}
\label{sec:13:negative}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The HPC framework would be toothless if it said yes to everything. A framework that never says no is no more than a vibe. These negative cases are where it earns its right to generalize: not by hand-waving, not by stipulation, but by refusing three temptations.

\emph{Academic register} has a recognizable flavour: passive constructions, nominalizations, hedges like \mention{it has been argued}. The features cluster; experienced readers identify academic prose instantly. But this clustering isn't a homeostatic kind; it's a \term{conditioning effect} (see Chapter~\ref{ch:social-stabilization}). The \enquote{Academic} cluster is just the output of situational reweighting. Put the same researcher in front of a grant panel, a blog audience, and a conference poster, and the passives evaporate, the hedges reweight. The bundle doesn't resist perturbation; it \emph{is} the perturbation. It fails the diagnostic because it describes a temporary state of the system, not a stable component. It belongs in the same ontological category as \enquote{shouting}~-- real, consequential, but defined by the act, not the actor.

\emph{Indo-European} is a language family defined by historical descent. The cluster properties~-- cognate vocabulary, shared sound correspondences, similar grammatical patterns~-- are real. But ask what would push English \emph{back toward} Proto-Indo-European. Nothing. There's no homeostatic pressure maintaining Indo-European-ness. English drifts away from PIE continuously; contact with non-IE languages accelerates the drift. Historical kinds are defined by causal continuity with an origin; homeostatic kinds are defined by stabilizers that maintain covariance. Confusing them invites explanatory error.

\emph{Polysynthetic languages} are characterized by high morpheme-to-word ratios, incorporating structures, and complex verb templates. Mohawk, Chukchi, and Ainu are standard examples. But the same stabilizers don't maintain polysynthesis across families. Mohawk (Iroquoian) has noun incorporation for discourse-pragmatic reasons~-- incorporated nouns are non-referential \parencite{mithun1984}. A single Mohawk word like \textit{Washakotya'tawitsherahetkvhta'se} encodes what English requires a full clause for: `he made the thing that one puts on one's body ugly for her'~-- i.e., `he ruined her dress' \parencite[40]{baker1996}. Chukchi (Chukotko-Kamchatkan) has incorporation with different constraints and different discourse functions. The surface similarity~-- complex words~-- masks heterogeneous routes. There are likely local, genealogically bounded homeostatic kinds that yield polysynthetic-looking outputs, but polysynthesis as a global category is a \emph{multiply realizable} property, like \enquote{having wheels,} not a cross-family homeostatic kind. \enquote{Polysynthetic} is a typological region label, useful for description but not for explanation: it doesn't name a single, shared, mechanism-maintained kind.

The three cases fail for different reasons~-- conditioning effect rather than stable kind (academic register), historical rather than homeostatic maintenance (Indo-European), heterogeneous routes to surface similarity (polysynthetic). The HPC framework distinguishes these failure modes. That's the payoff of explicit diagnostics: you can say \emph{why} something doesn't qualify, not just that it doesn't.

\subsection{The null case}
If the framework is correct, we should also find a \emph{null case}: a robust semantic pole that simply never develops a morphosyntactic coupling in a given language. Consider grammatical definiteness in Russian. The semantic concepts of identifiability and uniqueness exist~-- Russian speakers track discourse referents perfectly well~-- but Russian does not conventionalize definiteness into an obligatory determiner slot the way English does (even though other Slavic languages, like Bulgarian and Macedonian, do). Russian speakers use demonstratives, word order, and bare nouns to manage information structure, but these are locally motivated strategies; what is absent is any obligatory morphosyntactic commitment in the NP. When coupling fails to develop, the communicative load is absorbed by lexical or constructional alternatives. The coupling continuum spans all the way to zero.

These failures matter because they are not all the same failure. A conditioning effect can be real without being a kind. A historical lineage can be real without being homeostatic. A typological label can be useful without naming a shared stabilizer. The point is not to thin the ontology; it is to keep the explanatory categories honest.

Now we can move to the tier where the unit is constituted by a conventional inferential habit: composite coupling in constructions.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Composite coupling: constructions}
\label{sec:13:constructions}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

Above the level of individual categories, constructions re-bind what loose coupling lets drift. Conventionalized pairings of form and meaning~-- the ditransitive, the \mention{let alone} construction, the way-construction~-- rely on multiple converging cues that speakers recognize as a gestalt. The form--object coupling is \emph{composite}: built up from components, maintained holistically by use, transmitted through interaction.

The triadic structure at the construction level:

\begin{itemize}
    \item \textbf{Form}: the formal template~-- word order, morphological marking, prosodic contour.
    \item \textbf{Object}: the semantic/pragmatic template~-- what the construction \emph{means} as a conventional pairing.
    \item \textbf{Interpretant}: the discourse expectations it sets up~-- what the hearer is licensed to infer, what follow-up is appropriate.
\end{itemize}

The object here is a conventional inference package, often polysemous but constrained; the diagnostic is whether cue bundles stabilize those constraints across contexts.

The coupling is composite: form--object links are conventional but compositional; interpretants must be computed from assembled pieces. When the composition fails~-- local form--object links are fine, but the global interpretant doesn't cohere~-- the result is a garden-path or a misparse.

Three stabilizer types maintain constructions. Frequency and entrenchment work on millisecond-to-week timescales: each token use strengthens memory traces, increasing production probability, generating more tokens in a self-reinforcing loop \parencite{bybee2001}. Cue redundancy provides robustness: multiple formal features converge on the same interpretation. Consider the \mention{or even} construction (\enquote{He didn't read the article, or even the abstract.}). The canonical cues include negative polarity in the anchor, the coordinator \mention{or}, the scalar particle \mention{even}, and structural parallelism. If parallelism diverges while grammaticality remains intact (\enquote{He didn't read the article, or even skimming the abstract was skipped}), the scalar inference requires costly repair. The bundle of cues maintains the cognitive gestalt. Normative pressure operates on year-to-decade timescales: editorial practices and style guides reinforce the canonical pattern, especially in formal registers.

If the construction tier is to do more than demonstrate feasibility, the diagnostics have to survive contact with heterogeneous constructions. This tier is demonstrated in English because the modelling pipeline is implemented there; the typological prediction is that form--meaning gestalts across languages will exhibit comparable stabilization signatures, and the disconfirmer would be a system where robust functional complexes show no measurable cue redundancy or normative stabilization. The confirmatory analysis uses a battery of ten constructions spanning four cue regimes, tested across UD English corpora (GUM, EWT, GUMReddit).\footnote{For transparency, Appendix~\ref{app:diagnostics} reports corpus sizes, positive-instance counts per construction per corpus, prevalence baselines for PR-AUC, and the feature inventory for each cue regime. The evaluation is strict transfer: models are trained on GUM (with a held-out development split for thresholding) and tested without retuning on EWT; GUMReddit serves as a register stress test.}
Eight are treated as positive candidates; two are included as designed brakes cases~-- pooled resultatives (predicted \enquote{too fat}) and \mention{X much?} (predicted register-local)~-- to act as theoretical controls. These are cases where constructionhood is either subtype-fragmented or register-parasitic, so HPC should fail, forcing the framework to say no when it should.\footnote{The negative-control criteria are pre-registered as follows: a candidate fails if (i) transfer performance collapses to near-baseline in at least one out-of-domain corpus, and (ii) ablation produces no stable cue-family signature (i.e., no cue family yields a consistent marginal drop across corpora).}
For the \mention{or even} construction, a classifier trained on GUM and tested on EWT correctly identifies true usages amidst noise with high precision and recall (in our extended analysis, an Area Under the Precision--Recall Curve, PR-AUC, of 0.886 for the full bundle; \citealt{Reynolds2026});\footnote{Appendix~\ref{app:diagnostics} reports $n$ for positives and negatives in both corpora, the prevalence baseline PR-AUC, and bootstrap intervals over documents. Thresholds are chosen on the GUM development split and fixed for EWT.}
dropping parallelism degrades the model's ability to find the construction (PR-AUC drops to 0.612).\footnote{\enquote{Parallelism} is operationalized as the subset of features encoding scalar alignment across conjuncts/alternatives (e.g., syntactic symmetry, matched category sequences, and comparative markers). The appendix reports the full ablation table and run-to-run variance so the parallelism drop can be distinguished from model noise.}
Ablation signatures show parallelism as a dominant stabilizer for scalar-additive constructions.\footnote{Here \emph{dominant} means: among cue-family ablations, parallelism yields the largest and most consistent marginal loss in PR-AUC across corpora for scalar-additive targets, while other cue families produce smaller, less stable losses.}
This outcome was genuinely surprising: we initially expected pooled resultatives to behave like well-behaved, stable constructions given their prominence in the theoretical literature. Instead, the pooled resultative shows weak cross-corpus transfer and washed-out ablation signatures~-- refusing to transfer across domains. It was only when viewed as a fragmented category lacking cohesive stabilization that the failure made sense, which is exactly what the \enquote{too fat} diagnosis predicts.\footnote{Appendix~\ref{app:diagnostics} reports transfer PR-AUC relative to baseline and shows that no cue-family ablation yields a reproducible large drop, consistent with heterogeneous subtypes. Where subtype-stratification is feasible, the appendix reports sharper cue signatures within subtypes.}
The \mention{X much?} construction falls below prevalence thresholds outside informal registers, confirming register-locality.\footnote{The prevalence threshold is set at a minimum of $m$ attested tokens per corpus (and a minimum positive rate of $p$\% of candidate contexts), to avoid over-interpreting unstable estimates from sparse data. Appendix~\ref{app:diagnostics} reports corpus-by-corpus counts, including orthographic and punctuation variants.}
The implication is deliberately narrow: the construction tier doesn't license a blanket HPC claim. Kindhood is earned case-by-case.

Composite coupling is where the interpretant does the most visible work. The unit is not merely retrieved; it must be inferred as a coherent gestalt. The stabilizers that matter are therefore the ones that protect that composition: cue redundancy, and~-- for orthography-dependent or high-prestige variants~-- norm enforcement and genre licensing.

The remaining question is not whether the tiers differ, but how the differences line up. That is what the stabilizer-weighting map is for.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{The stabilizer-weighting map}
\label{sec:13:map}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The zipper~-- phonemes, words, grammatical categories, constructions~-- isn't just a vertical list. Its levels are regions in a stabilization space defined by how form couples to object to interpretant, and which links bear the load. Table~\ref{tab:13:stabilizer-map} summarizes the mapping.

\begin{table}[htbp]
\centering
\caption{Triadic coupling across the stack. Each level shows the coupling regime, the vulnerable link, what the interpretant does, and the dominant stabilizers.}
\label{tab:13:stabilizer-map}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Level} & \textbf{Coupling} & \textbf{Vulnerable link} & \textbf{Interpretant} & \textbf{Dominant stabilizers} \\
\midrule
Phoneme & Hard & Form--Object & Lexical access & Quantal regions, dispersion, perceptual magnets \\
Word & Opaque & Form--Object & Conceptual activation & Frequency, editorial norms, analogy \\
Grammar & Loose & Semantic--Formal & Bidirectional inference & Entrenchment, alignment, transmission \\
Construction & Composite & Object--Interpretant & Compositional inference & Cue redundancy, normative pressure \\
Discourse & Loose & Interpretant & Pragmatic uptake & Common ground, accommodation, repair \\
\bottomrule
\end{tabular}
\end{table}

The stabilizers act like the slider on a zipper, not a single cause. When the slider is doing its job you don't notice it; you only notice it when it snags. At the phoneme tier, biophysical constraints carve the design space, developmental learning binds cues, sociocultural norms transmit inventories. At the word tier, frequency entrenches forms, editorial standards enforce conventions, usage communities police extensions. At the construction tier, cue redundancy protects against noise, normative pressure corrects deviations, genre licensing regulates distribution. The mechanisms shift in their balance: articulatory constraints weigh heavily for phonemes, frequency and norms for words, cue redundancy and editorial pressure for constructions. But at every tier, multiple forces interact: body, cognition, and society always contribute.

Table~\ref{tab:13:stabilizer-map} is a map of vulnerable links and stabilizer families. But it also points to a deeper regularity: as we move up the stack, what changes is not merely \emph{which} stabilizers dominate, but \emph{how much mediation is required} for the system to do its work. That is the Peircean gradient the next section makes explicit.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{The mediation gradient}
\label{sec:13:mediation}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The coupling continuum has a Peircean interpretation. In the triadic structure of sign, object, and interpretant (§\ref{sec:7:what-projection-is}), the interpretant's role shifts as we move along the zipper.

At the phoneme level, the interpretant is nearly automatic. Form determines object (contrastive identity); object determines interpretant (lexical access). The sign functions almost as a brute index~-- a connection with minimal interpretive work. The relation holds because of tight constraint-based physical and perceptual mappings, not because the interpretant \emph{constitutes} it.

At the word level, the interpretant does more. The form--object pairing is symbolic and conventional~-- brute memory, not acoustic necessity~-- and the interpretant must be learned, not merely perceived. But once activated, it follows reliably. The habit is stored, not computed.

At the grammatical-category level, mediation becomes central. The interpretant doesn't just follow from the form--object pairing; it \emph{mediates} between a semantic construal and a morphosyntactic form, coupling them through bidirectional inference. This is the territory of would-bes: the category generates inductive habits that extend to unencountered instances. The interpretant is what the maintenance buys.

At the construction level, we have habit-constituted symbols: the interpretant constitutes the unit. Without the conventionalized inferential habit, the form-meaning pairing doesn't exist as a construction~-- it's just a sequence of words. The habit is what makes the teeth engage.

The gradient isn't a ladder of complexity. It's a map of where the would-be bears the load. At the hard end, correlation does most of the work and mediation is minimal. At the composite end, mediation does all of it. The grammatical categories of Part~III sit in the middle~-- the zone where the interpretant mediates actively, where would-bes are conditional on field and purpose, and where the coupling is loose enough that different analytical perspectives can carve differently (§\ref{sec:7:field-relative}). This is why grammatical categories are the most interesting and the most contentious: the mediation is strong enough to sustain real projectibility, but loose enough to permit field-relative decomposition.


%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{Predictions and disconfirmers}
\label{sec:13:predictions}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The diagnostics generate falsifiable predictions beyond the case studies. Weakening a stabilizer should reduce cluster covariance before norms re-stabilize; for constructions, downsampling training data to 25\% should degrade PR-AUC substantially if the bundle depends critically on frequency. Just as /y/ appears preferentially in larger vowel inventories, rare constructional variants should concentrate in corpora with larger constructicon repertoires. If a pooled category like \enquote{resultative} is genuinely \enquote{too fat}, stratifying into subtypes should restore projectibility for well-maintained local kinds. These tests operationalize the core claim: linguistic categories qualify as HPCs when they project via identifiable mechanisms.

%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--
\section{The architecture of maintenance}
\label{sec:13:forward}
%~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--  ~--

The same framework~-- property clusters, stabilizing mechanisms, the two-diagnostic test~-- applies from the phoneme to the construction. What changes across levels isn't the logic but the coupling: how tightly form and meaning bind, where the interpretant bears the load, and which stabilizers dominate.

This is the scale-invariant claim. But note precisely what is invariant: the \emph{diagnostic criteria} remain constant across levels, while the \emph{realizer families} and the \emph{explanatory depth} of the stabilizers vary radically. The HPC diagnostics don't privilege a level. The ontological character of the stabilizers shifts~-- from biophysics for phonemes, to cognitive frequency for words, to social norms for constructions~-- but the diagnostic logic of stabilization remains invariant. Phonemes, words, grammatical categories, and constructions all either pass or fail the same tests. The tests are invariant in form, but their operationalization is purpose-relative; that's why negative controls and cross-corpus transfer are not optional. The framework is a zipper, not a spotlight: it engages whatever the mechanisms hold together, at whatever grain they operate.

But one location has special status. Throughout Part~III~-- countability, definiteness, lexical categories, gender~-- the categories clustered at the morphosyntactic level. That's where form--object--interpretant coupling is both \emph{tight} and \emph{obligatory}. You can speak without using \mention{let alone}. In English, finite clauses force tense; many NP environments force number; determiner--head packaging is unusually tight even when determiners are not obligatory.

Morphosyntax is the zone of maximum systematicity: the region of grammar where a \emph{semantic} interpretant is enforced, unlike phonemes where only a discriminatory interpretant is obligatory.

Composite coupling makes it natural to ask whether some parts of the system behave as if they were \emph{packaged} for transmission: if constructions are maintained by bundles of cues, are there domains where the bundle is so tight that separation is systematically resisted? One place to look is inside the nominal domain.

We can measure this tightness. \textcite{Reynolds2026} quantifies the \term{packaging tightness} of the determiner--head relationship in English~-- the link that the Left Branch Gap (§\ref{sec:8:negative}) fails to break. Using a dependency-locality metric, he finds that determiners and their heads exhibit a packaging score of $k=4$ (extremely tight).\footnote{To make $k$ interpretable, the appendix defines the metric (units, normalization, and typical range) and reports comparison scores for other head--dependent relations (e.g., adjective--head, auxiliary--verb), along with corpus, sample size, and genre sensitivity.}
The system treats $D+N$ not as neighbours but as a rigid transmission unit. This is what obligatory interpretant generation looks like in the data: a coupling so strong that the elements refuse to be separated.

Chapter~\ref{ch:grammaticality-itself} asks what happens when we take this observation seriously. If grammaticality is what emerges when form--object--interpretant coupling is obligatory, compositional, and learnable~-- then grammaticality itself is a kind. The interpretant isn't just what you get; it's what you \emph{must} get.

Kosuri's remark about DNA is literal, but the metaphor is exact. Complementary strands zip up because the stabilizers make that configuration the easy one, the default one, the one the system returns to after perturbation. In language, the teeth engage wherever the mechanisms make engagement cheap and failure costly. The zipper doesn't just pull categories together. It makes some pairings the ones that \emph{zip up}. That's what they do.
