\chapter{Lexical categories and their maintenance}
\label{ch:lexical-categories}

\epigraph{The resemblance of one animal to another is of exactly the same essential nature as the resemblance to a leaf, or to bark, or to desert sand, and answers exactly the same purpose.}{— Alfred Russel Wallace, \textit{Mimicry, and Other Protective Resemblances Among Animals} (1867)}

\section{Introduction}
\label{sec:11:intro}

A noun is a word that names a person, place, or thing.

You've heard this. Everyone has. It's the first thing they tell you in school, and the last thing they tell you in an introductory linguistics class~-- right before they take it away. The standard move goes like this: \mention{sincerity} names a quality, not a person, place, or thing, yet it's a noun. \mention{Destroy} doesn't name anything, yet it denotes an action~-- and so does \mention{destruction}, which \emph{is} a noun. The meaning changed; the syntactic behaviour didn't. Conclusion: whatever makes something a noun, it isn't naming. We need distributional tests~-- morphological and syntactic diagnostics that track \emph{how} a word behaves rather than \emph{what} it refers to. The semantic definition was, at best, a useful heuristic; at worst, a confusion of two different questions.

This is a genuine insight, and it has built the infrastructure of modern linguistics. Descriptive grammars, parsers, annotation schemes, and computational models all rest on the distributional turn: categories are defined by structural behaviour, not by meaning. The student who arrives saying \enquote{a noun names a thing} and leaves saying \enquote{a noun heads a noun phrase} has learned something real.

But the standard lesson overshoots. In the rush to separate syntax from semantics, it discards something it should've kept.

\mention{Name} is a perfectly good category. It has a cluster of properties that travel together: names pick out individuals rather than describing them; they resist descriptive modification (\mention{tall Kim} is odd in a way that \mention{tall student} isn't); they create referential opacity (\mention{Lois believes Superman can fly} doesn't entail \mention{Lois believes Clark can fly}); they support rigid reference across contexts. These properties cluster because the cognitive and communicative functions that names serve~-- tracking individuals, sustaining reference across conversations and communities~-- require them to cluster. The clustering is real, maintained by real mechanisms, and projectible: knowing that something is a name tells you things about how it will behave semantically.

\mention{Noun} is also a perfectly good category. It heads noun phrases, takes determiners, inflects for number, fills argument slots, triggers agreement. These properties cluster because of distributional pressures: words that share some of these properties tend, over time, to share more of them. The clustering is maintained by acquisition, entrenchment, and structural analogy.

The extensions overlap heavily. Most names are nouns (\mention{Kim}, \mention{London}, \mention{Tuesday}). Most prototypical nouns do name things (\mention{dog}, \mention{chair}, \mention{idea}). The overlap is what makes the schoolroom definition feel right. But as Chapter~\ref{ch:projectibility} argued, overlapping extensions don't make overlapping categories. A syntactician and a semanticist can carve the same words differently without either being wrong~-- because they're tracking different HPCs, maintained by different mechanisms, projectible for different purposes.

This is exactly the architecture we saw with definiteness and deitality in Chapter~\ref{ch:definiteness-and-deitality}. The definiteness cluster (semantic: identifiability, uniqueness, familiarity) and the deitality cluster (morphosyntactic: \mention{there}-resistance, partitive licensing, nonrestrictive hosting) overlap substantially~-- most definite referents get marked as such~-- but they can decouple. Weak definites have deitality without definiteness; proper names have definiteness without deitality. Neither anomaly made sense while we treated them as one category. Both made sense once we recognized them as two.

The schoolroom definition of \enquote{noun} commits the same conflation. It treats the naming cluster and the noun cluster as a single category, then wonders why the definition has exceptions. The exceptions aren't exceptions~-- they're the visible evidence of play in the joint between two distinct but coupled HPCs. \mention{Sincerity} is a noun without naming a thing. \mention{The big house on the corner} names a thing without being a noun. The definition fails not because the semantics was wrong, but because two categories were hiding inside a single definition~-- noun and name in a trenchcoat, attempting to pass as a single lexical category at the border.

The deeper question, then, isn't \enquote{what is a noun?}~-- that question has good answers, all distributional. The question is: \emph{why does linguistics focus on noun rather than name?} Why is the syntactic category the one we build our grammars around, rather than the semantic one?

The short answer: because noun projects better~-- for grammar. Learning that something is a noun~-- that it heads noun phrases, takes determiners, inflects for number~-- lets you predict its syntactic behaviour with extraordinary reliability across novel instances. Name projects too, but in a different domain. Knowing that something is a name is enormously important for memory, cognition, and social interaction~-- for tracking individuals, managing relationships, sustaining reference across years of shared experience. But it tells you relatively little about morphosyntax. The grammarian's preference for noun over name isn't a quality judgment; it's field-relative projectibility in action. Grammars are built around noun because noun is the category that projects for grammatical purposes. Name is the cognitive scientist's and the social psychologist's category. Each is real; each earns its keep in its own domain. (Why linguistics itself is organized around morphosyntax rather than around naming or reference~-- why the discipline carved where it did~-- is a question I return to in Chapter~\ref{ch:grammar-itself}.)

But this answer opens a further question. If nouns are thick HPCs, what about the other word classes? Are they all equally well maintained? Do they all project equally well?

They don't.

\bigskip

This chapter is a dissection. It opens the dictionary's inventory of word classes and asks which of them are genuine HPCs~-- categories maintained by converging mechanisms, projectible to novel instances~-- and which are something less: convenient labels, historical accidents, or administrative fiction.

The answer doesn't fall neatly along traditional lines. Some categories that look fundamental turn out to be thin. Others that look like ragbag leftovers turn out to conceal multiple genuine kinds. The exercise will reveal three configurations that keep recurring across the linguistic hierarchy, each with its own diagnostic signature:

\begin{itemize}
    \item \textbf{Robust kinds} where multiple mechanisms converge on a thick, projectible cluster. \term{Noun} and \term{verb} are the paradigm cases~-- the skeletal categories that recur across languages because the discourse functions they serve (reference and predication) demand them.
    \item \textbf{Thin kinds} where the clustering is real but the mechanisms are fewer and less tightly coupled. \term{Adjective} is the pivotal example: a category with genuine distributional coherence that nonetheless varies dramatically across languages in its independence and scope.
    \item \textbf{Fat labels} where a single term obscures multiple unrelated clusters. \term{Adverb} is the classic case~-- what Quirk called \enquote{the dustbin of the parts of speech}, a category with excellent storage capacity and terrible explanatory power. And the traditional \term{pronoun} turns out to be another: a surface-distribution class that masks at least three distinct convergent-evolution stories.
\end{itemize}

The Wallace epigraph at the head of this chapter is the key. The resemblance of one animal to another, Wallace saw, \enquote{answers exactly the same purpose} as the resemblance of an animal to a leaf or to desert sand. The resemblance isn't accidental~-- it's functionally driven. But the functions can differ profoundly. Two species can look alike because they share ancestry; or because different selection pressures converged on the same solution; or because one is parasitizing the other's signal. The resemblance is real in every case. What differs is the machinery that produces it.

Word classes pose the same question. When two categories look alike on the surface~-- when they share distributional profiles, occupy similar slots, attract similar labels~-- the question is: \emph{why?} Do they resemble each other because the same mechanisms maintain them (genuine kin)? Because different mechanisms converge on the same output (mimics)? Or because a taxonomist needed somewhere to put the leftovers (filing convention)? This chapter is about learning to ask that question, and about what the answers reveal.


\section{The skeleton: nouns and verbs}
\label{sec:11:skeleton}

Nouns and verbs, every language has 'em.

Or do they? Typologists have debated it for decades, and the qualification matters: what every language has is a way to refer to things and a way to predicate properties or events of them. When applied across languages, \term{noun} and \term{verb} denote comparative concepts~-- analyst-constructed categories that allow comparison, not universal entities projected from English (\citealt{Haspelmath2010}; Chapter~\ref{ch:comparanda}). But the comparative concepts keep earning their keep. Wherever linguists look~-- across genealogical families, across typological profiles, across millennia of documented change~-- they find categories specialized for reference and categories specialized for predication. The labels are ours; the functional pressure is not.

Why? The maintenance view offers an answer that essentialism can't. The essentialist says: nouns exist because nounhood is a primitive of the language faculty~-- a formal feature, perhaps \textsc{[+N]}, hardwired into universal grammar \citep{baker2003}. This explains the universality but not the clustering. Why should a word that bears \textsc{[+N]} also take determiners, inflect for number, head argument phrases, and attract case marking? The feature is a label, not a mechanism. It names the regularity without explaining it.

The HPC account says: nouns exist because multiple mechanisms converge on the same clustering. Language has two jobs that recur in every communicative ecology: \emph{building references to things} and \emph{saying things about those references}. These aren't formal stipulations; they're functional pressures arising from the structure of discourse. Every utterance that does more than emote needs to identify what it's about and say something about it. The words that specialize in identification cluster together, and the words that specialize in predication cluster together, because the functional demands are tight enough to keep the clustering going.

What demands, exactly? The mechanisms are familiar from earlier chapters, but their convergence on nouns and verbs is unusually strong. It's worth walking through them carefully, because the contrast with thinner categories later in the chapter depends on seeing just how many independent forces are at work here.

\begin{itemize}
    \item \textbf{Discourse frequency.} Reference and predication are the backbone of every clause. In any substantial corpus, nouns and verbs together account for more than half of all tokens~-- not because linguists have decided to count them that way, but because the communicative jobs they do are the ones that recur most relentlessly. You can construct a clause without an adjective, without an adverb, without a preposition. You cannot construct one without something that refers and something that predicates. That relentless frequency breeds the entrenchment described in Chapter~\ref{ch:stabilisers}, and entrenchment breeds tight distributional profiles.
    \item \textbf{Morphological agreement.} Once a language develops agreement~-- subject-verb marking, noun-adjective concord, case systems~-- the agreement itself becomes a mechanism that stabilizes the categories it links. Agreement demands a controller and a target; the controller is typically a noun, the target a verb or adjective. Consider Swahili, where the noun-class prefix \mention{m-} on \mention{mtoto} (`child') triggers concordant prefixes on verbs (\mention{a-nasoma}, `s/he reads'), adjectives (\mention{m-zuri}, `good'), and demonstratives (\mention{h-uyu}, `this'). The system doesn't just \emph{mark} the noun-verb distinction; it \emph{enforces} it. Every new word entering the language must slot into a noun class or a verb paradigm, because the agreement morphology has no slot for anything else. The morphological lock-in stabilizes the category boundary from both sides: nouns are the things that control agreement; verbs are the things that show it.
    \item \textbf{Acquisition.} Children learn nouns and verbs early, and they learn them as categories~-- not just as individual words. The evidence is overgeneralization: children who say \mention{I goed} or \mention{two mouses} have extracted a category-level pattern, not memorized a form. \citet{tomasello1992} showed that early verb use is organized around ``verb islands''~-- initially, each verb is its own construction, with its own argument frame. But by age three, children have abstracted across the islands, extracting the verb category as a generalization. The abstraction happens because the functional pressure (predication) keeps presenting the same distributional pattern across different lexical items~-- and what gets abstracted early gets entrenched early.
    \item \textbf{Structural analogy.} Novel words get slotted into pre-existing categories. You encounter \mention{to shlep} for the first time and immediately know it inflects (\mention{she shleps}, \mention{they shlepped}), takes objects (\mention{shlep the suitcase}), appears in verb-phrase constructions (\mention{keep shlepping}). You don't need to be told; you \emph{predict}. Structural analogy extends the category to new members, maintaining the cluster from the open end.
    \item \textbf{Semantic recruitment.} The world keeps producing entities that need to be referred to and events that need to be predicated~-- and the naming function, the very thing the introduction distinguished from nounhood, is itself a feeder mechanism. Name and noun are distinct HPCs, but the semantic pressure of the first continuously recruits members into the second. The open-class nature of nouns and verbs isn't a stipulation; it's a consequence of the fact that the world's referents and events are open-ended.
\end{itemize}

No single mechanism explains the universality. What explains it is the convergence. Consider what would happen if only one mechanism were operating. If discourse frequency were the only force, we'd expect nouns and verbs to be entrenched but not necessarily morphologically distinct~-- frequent patterns can coexist without forming separate paradigms. If agreement were the only force, we'd expect morphological categories but not necessarily semantic coherence~-- agreement can track arbitrary classes (as Swahili's noun classes demonstrate). If acquisition were the only force, we'd expect early-learned categories to persist but not necessarily to attract new members. Each mechanism alone would produce some clustering, but the clusters would be partial, fragile, and variable.

What makes nouns and verbs robust is that all five mechanisms push in the same direction. Functional pressure, morphological lock-in, early acquisition, structural analogy, and semantic recruitment all converge on the same partition of the lexicon. Remove any one of them and the category would still survive, maintained by the others. This is the signature of a robust HPC~-- not a single causal thread but a cable of independent strands, each sufficient to maintain some clustering, collectively sufficient to maintain it all.

The metaphor I want here is the \term{skeleton}. \citet{zimmer2015} observed that sharks and dolphins look strikingly alike~-- streamlined bodies, dorsal fins, tapered snouts~-- despite having diverged hundreds of millions of years ago. The resemblance isn't ancestry; it's physics. Hydrodynamic drag imposes the same penalty on every aquatic body plan, and the skeleton~-- the load-bearing architecture underneath~-- is what gets reshaped to meet it. Different lineages, same functional pressure, same skeletal solution. Nouns and verbs are the grammatical skeleton. Different language families, same communicative pressure~-- reference and predication~-- same categorical solution. The forms recur not because a blueprint mandates them but because the jobs they do are so fundamental that no language survives without them.

This explains something that essentialism treats as primitive: why nouns and verbs are the categories around which grammars are organized. Not because universal grammar stipulates them. Because the forces that maintain them are the strongest, the most convergent, and the most resistant to perturbation in the entire grammatical system. They're the last categories standing when a language simplifies under contact, pidginization, or creolization. They're the first categories children acquire. They're the categories that agreement systems lock into place.

The skeleton metaphor also sets up what comes next. Not every organ is skeletal. Some are essential but variable~-- lungs take different forms in reptiles and mammals. Some are ornamental~-- plumage, pigmentation, display structures. And some resist classification altogether~-- the spleen has puzzled anatomists for centuries. Word classes, we'll see, show the same gradient.


\section{Thin kinds: the adjective as plumage}
\label{sec:11:thin-kinds}


If nouns and verbs are the skeleton, adjectives are the plumage.

Some birds have made plumage into a spectacle~-- the peacock's tail, the bird of paradise's display feathers. Others have almost dispensed with it~-- the kiwi, ground-dwelling and shaggy, barely has feathers worth mentioning. But every bird has a skeleton. The plumage is variable, optional, sometimes magnificent, sometimes vestigial. The skeleton is non-negotiable.

Languages show the same pattern. English has a large, open adjective class numbering in the thousands, with its own comparative morphology (\mention{taller}, \mention{tallest}), its own degree modification (\mention{very}, \mention{too}, \mention{extremely}), and two distinct syntactic homes: attributive (\mention{the tall student}) and predicative (\mention{the student is tall}). English is the snowy owl, with plumage so thick and functional it looks structural.

But many languages are kiwis. \citet{dixon2004} surveyed adjective classes across hundreds of languages and found a striking gradient. At one end are languages like English, with large, open adjective classes numbering in the thousands~-- languages where the adjective category is as robust as the noun category in terms of productivity and distributional coherence. At the other end are languages with no identifiable adjective class at all. Between them sits the majority: languages with small, closed adjective classes, often numbering fewer than two dozen items, typically covering the semantic domains Dixon identified as the ``core'' property concepts~-- dimension (\mention{big}, \mention{small}), age (\mention{old}, \mention{new}), value (\mention{good}, \mention{bad}), and colour (\mention{black}, \mention{white}).

Where English says \mention{the tall building}, Mandarin uses a stative verb construction: \mention{gāo de dàlóu} uses the property concept \mention{gāo} (`tall') as a predicate-like modifier, not as a member of a separate adjective paradigm. Where English says \mention{the red door}, Bantu languages like Swahili manage with a handful of true adjectives~-- perhaps a dozen~-- and route most property concepts through relative-clause constructions: `a door which is red' rather than `a red door'. The \term{property-concept} function~-- attributing size, age, colour, value to a referent~-- exists in every language. A dedicated lexical category for doing it does not.

Dixon's survey makes a further point that matters for the HPC framework. Even in languages with small adjective classes, the \emph{core} members~-- dimension, age, value, colour~-- tend to be the ones that survive. The category doesn't shrink randomly; it shrinks from the periphery inward, losing less frequent and less functionally essential members first. This is exactly what the maintenance view predicts: the mechanisms are strongest where the functional pressure is highest, and they weaken as the pressure drops. The core property concepts are the ones most frequently needed in attributive modification; the peripheral ones can be handled by other means without communicative loss.

Why doesn't the full category stabilize? Compare the mechanisms:

\begin{itemize}
    \item \textbf{Discourse function.} Nouns and verbs serve the two primitive discourse operations: reference and predication. Every clause needs them. Adjectives serve \emph{attribution}~-- narrowing a reference or elaborating a predication. This is useful but never essential: you can always build a reference without an adjective (\mention{the student who is tall} instead of \mention{the tall student}). The adjective provides a shortcut~-- packing a relative clause's worth of information into a single prenominal slot~-- but shortcuts, by definition, are dispensable. The discourse pressure is real but shallow: it rewards efficiency, not necessity. And shallow pressure produces shallow clustering.
    \item \textbf{Acquisition.} Children acquire nouns and verbs before adjectives, and the delay is not trivial. Property concepts like colour and value are more abstract than objects and actions~-- they require comparing an entity to a scale, which presupposes the entity concept (\mention{big} needs something to be big). Dimensional adjectives emerge earlier than evaluative ones; colour terms are notoriously late and unstable. The acquisition trajectory mirrors Dixon's typological gradient: the property concepts that are easiest to ground (dimension, age) stabilize first; the harder ones (value, colour) are more variable. Later entry into the system means fewer cycles of entrenchment before the transmission bottleneck begins filtering~-- and less resistance to the competing strategy of handling property concepts through verbs or nouns.
    \item \textbf{Morphological glue.} Nouns have case, gender, and number. Verbs have tense, aspect, and agreement~-- and crucially, the agreement sits on top of inflectional categories that belong to verbs alone. The verb's morphological identity is thick before agreement even enters the picture. Adjectives, in many languages, have no such foundation. They carry no inflectional categories of their own~-- no adjectival equivalent of tense or aspect. What morphology they have is borrowed: concord with the noun's gender, number, and case. When the only morphological glue holding a category together is copied from another category, the bond is weaker~-- because the morphology doesn't differentiate the adjective from the noun; it assimilates the adjective \emph{to} the noun. In Latin, the adjective \mention{bonus} declines through the same paradigm as the noun \mention{dominus}. In Swahili, the handful of true adjectives take the same noun-class prefixes as the nouns they modify. The morphology says: ``this word agrees with a noun.'' It does not say: ``this word belongs to a distinct category.'' And it shows: across attested language change, adjective classes shrink as stative-verb or denominal strategies expand to fill the same functional niche.
\end{itemize}

Even in English, where the adjective cluster is thick, you can see play in the joints. Many adjectives resist the inflectional comparative (\mention{more curious}, not \mention{*curiouser}; \mention{more beautiful}, not \mention{*beautifuller}). The comparative split itself is a distributional fracture within the category: monosyllabic adjectives take \mention{-er}/\mention{-est}; polysyllabic ones take \mention{more}/\mention{most}; disyllabic ones waver (\mention{commoner} or \mention{more common}?). This isn't the behaviour of a category with tight internal cohesion. It's the behaviour of a category held together by a few shared properties (gradability, attributive position, predicative position) while differing on others.

The positional split is equally telling. Some adjectives are restricted to attributive position (\mention{the main reason} but not \mention{*the reason is main}; \mention{the mere thought} but not \mention{*the thought is mere}); others to predicative (\mention{the child is asleep} but not \mention{*the asleep child}; \mention{the patient is well} but not \mention{*the well patient}). \citet{matthews2014} argues that the split runs deep enough to warrant treating attributive and predicative adjectives as distinct categories. The argument has force: the two groups differ not just in position but in meaning~-- \mention{old} means `elderly' predicatively (\mention{the professor is old}) but can mean `former' attributively (\mention{my old professor}); \mention{late} means `deceased' attributively (\mention{the late president}) but `not on time' predicatively (\mention{the president was late}). Position and meaning have partially decoupled.

But most adjectives do both jobs happily~-- degree modification, comparative morphology, and property-concept semantics work across both positions~-- and the core semantic type (property of individuals) is shared. The edges fray; the centre holds. That's the thin-kind signature: shared mechanisms at the core, loosening at the margins. A fat label, by contrast, has no shared mechanisms at all~-- only a shared filing convention.

This is what a \term{thin} HPC looks like. The clustering is real~-- in English, knowing that something is an adjective lets you predict a great deal of its syntactic behaviour. But the mechanisms maintaining the cluster are fewer and less tightly coupled than for nouns and verbs. Remove the morphological glue (as many languages do), and the category thins. Remove the dedicated syntactic slot (as stative-verb languages do), and it thins further. Remove both, and the category dissolves~-- not because adjectives are unreal, but because the mechanisms that held them together were never as robust as the ones that hold nouns and verbs.

The adjective sits lower on the naturalization gradient (Chapter~\ref{ch:comparanda}). As a comparative concept, it earns its keep: it identifies a real functional niche across languages, and linguists can meaningfully ask whether a given language fills that niche with a dedicated class. But as a language-internal category, it ranges from thick (English, with thousands of members and dedicated morphology) to absent (languages where property concepts are just stative verbs). And even within a single language, the thickness of the cluster varies across communicative situations (Chapter~\ref{ch:social-stabilization}). The adjective class that grammarians describe is the one stabilized in the most general com-sit~-- the standard variety, written and formal. In narrower com-sits, the category may thin further. Casual spoken English leans harder on verbal strategies for property concepts (\mention{that sucks} rather than \mention{that is bad}; \mention{it stinks} rather than \mention{it is unpleasant}), and child-directed speech relies on a smaller, more repetitive set of adjectives than adult written prose. The reference grammar captures the thickest version of the cluster~-- the snowy owl in full winter plumage. Other com-sits see the feathers thin. The category is a local habit, and the locality is finer-grained than ``a language.''

But at least plumage, where it exists, is real. Thin or thick, it has structure~-- feathers that interlock, pigments that signal, insulation that functions. What about the category that has none of this? The one that is just a drawer label~-- a name for whatever didn't fit anywhere else?


\section{The wastebasket: adverbs and what's inside}
\label{sec:11:wastebasket}

Consider what travels under the label \mention{adverb}: \mention{quickly} functions as a modifier in the VP, describing how an action is performed. \mention{Very} functions as degree modifier in AdjP or AdvP, scaling a property. \mention{However} connects propositions, doing the work of a coordinator from a different syntactic address. \mention{Frankly} can modify a speech act (\mention{frankly, I don't care}) or describe a manner of speaking (\mention{she spoke frankly})~-- different categories wearing the same morphological costume. \mention{Only} associates with focus, operating on information structure. \mention{Yesterday}~-- in CGEL's analysis~-- is a noun functioning as adjunct \citep{huddleston2002}. Six words, six jobs, one label.

This is Chapter~\ref{ch:failure-modes}'s fat-category signature. No shared proper function, no converging mechanisms, no cluster of properties that travel together across the class. Knowing that something is \enquote{an adverb} tells you it isn't a noun, a verb, or an adjective~-- and almost nothing else. The label's projectibility is exhausted by its negative definition.

But the wastebasket isn't empty. Crack it open and at least one genuine category falls out.

Manner adverbs~-- \mention{quickly}, \mention{carefully}, \mention{beautifully}, \mention{abruptly}~-- have a real cluster, maintained by real mechanisms. First, a semantic niche: events can be performed in ways, and languages need to express this. The manner-of-event function exists in every language, whether it's filled by adverbs, serial verbs, ideophones, or verbal morphology. The functional pressure is universal; the categorical realization depends on whether enough mechanisms converge.

In English, they do. Productive \mention{-ly} derivation keeps the class open: any adjective that can characterise an event gets a ticket in. Syntactic positioning is independently maintained~-- \mention{fast}, \mention{well}, and \mention{hard} occupy the same VP-internal slots without \mention{-ly}, so the distributional cluster isn't a morphological shadow~-- and dependency-distance minimisation reinforces it \citep{Gibson2026}: processing cost rises with the syntactic distance between a modifier and the predicate it semantically modifies, penalising placement far from the VP and keeping the cluster positionally tight. And gradability works across the class (\mention{more carefully}, \mention{less abruptly}), connecting manner adverbs to the degree-modification system. That's three or four independent mechanisms converging on a single cluster~-- comparable to adjectives, thinner than nouns and verbs~-- and the cross-linguistic prediction follows the same pattern as Dixon's adjective gradient: manner expression recurs universally; a dedicated manner-adverb class doesn't.

The \mention{-ly} morphology is particularly revealing, because it feeds two categories, not one. \mention{She spoke frankly} is manner~-- it describes how she spoke. \mention{Frankly, she's wrong} is speech-act modification~-- it characterises the speaker's communicative stance, not the event. Same derivational process, different functional niche, different syntactic behaviour, different category. One mechanism producing forms that land in different clusters because the other mechanisms pulling on them diverge. The name/noun architecture from the introduction, reprised.

The remaining groups are thinner still. Degree modifiers (\mention{very}, \mention{quite}, \mention{rather}) form a small functional class~-- coherent but mostly closed, maintained by entrenchment rather than productive recruitment. Focusing adverbs (\mention{only}, \mention{even}, \mention{just}) share an alternatives-based semantics, but they're a handful of items, not a productive class. Connective adverbs (\mention{however}, \mention{moreover}, \mention{consequently}) are doing a coordinator's work from adjunct position~-- less a natural kind than a syntactic disguise. Each group projects in its own domain, but none has the mechanism convergence that manner adverbs do.

The sub-classification itself is diagnostic. CGEL sub-classifies adverbs by semantics~-- manner, degree, focusing, connective~-- and rightly so: the semantics is genuinely central to what distinguishes these groups. But the need for semantic sub-classification is telling. For nouns and verbs, distributional criteria do the heavy lifting; the semantics comes along for the ride because the mechanisms converge. For adverbs, distribution alone can't sort the contents, because the groups don't share enough distributional coherence for distribution to track. The grammar reaches for semantics not as a supplement to distributional classification but as its replacement~-- which is exactly what the introduction's lesson about nouns and names warned us to notice.

The wastebasket doesn't just group unrelated items; it actively misclassifies items that belong elsewhere. Even CGEL~-- the most careful descriptive grammar of English~-- treats \mention{more} and \mention{less} as determinatives in most environments but categorizes them as adverbs in analytic comparatives (\mention{more interesting}, \mention{less quickly}). The sole grounds: \mention{more} supposedly fails to contrast with \mention{much} as a degree modifier in these contexts. But the contrast does exist~-- with comparative governors (\mention{much different} alongside \mention{more different}), with participial adjectives (\mention{much improved} alongside \mention{more improved}), and in contexts that CGEL's restricted conception of analytic comparatives excludes \citep{reynolds2024}. The distributional patterns that motivated the reclassification turn out to follow from the pragmasemantics of scale structure: \mention{more} establishes a reference point on a scale where none was salient; \mention{much} requires one already in place. No category switch is needed. A single diagnostic thread~-- lack of distributional contrast~-- was carrying the entire adverb categorization. When it frayed, the classification had nothing else to stand on. One thread is not a cable.

The projectibility gap between label and contents is wider for \mention{adverb} than for any other traditional lexical category. \mention{Noun} and \mention{verb} project because they track genuine kinds. \mention{Adjective} projects, with more noise, because it tracks a thinner kind. \mention{Adverb} barely projects at all~-- whatever predictive power exists lives in the groups the label conceals.

Why, then, doesn't the wastebasket Balkanize? If the groupings are real and the fat label is diagnostically near-useless, why do we still have it?

I can only speculate, but I think it's because \term{lexical category} is itself an HPC~-- and one of its maintained properties is a small category count. How small? Cross-linguistically, major open-class inventories range from about one to four \citep{rijkhoff2007}: some languages use a single flexible class for both reference and predication; most distinguish nouns from verbs; fewer add a dedicated adjective class; fewer still a distinct adverb class. No language has been described as needing fifteen or fifty. The bound isn't conventional~-- it reflects the small number of fundamental discourse functions (reference, predication, modification) that create niches for dedicated categories \citep{hopperthompson1984, croft2001}. Languages partition the functional space differently, but the space itself is bounded.

The clustering that makes \mention{lexical category} a useful concept~-- distributional coherence, morphological paradigm, headed phrase structure~-- is maintained by the same compression pressures that shape individual categories: the transmission bottleneck favours systems that can be learned from finite input, and fewer categories means a shorter grammar. Splitting \mention{adverb} into six sub-categories inflates the inventory, and each new category costs descriptive bits. The cost of six thin categories can exceed the predictive gain over one fat label~-- especially when most of the groups are small enough that listing their members is cheaper than defining a class.

The resistance is real but not absolute. Computational tagsets~-- Penn Treebank, Universal Dependencies~-- have already begun the Balkanization, distinguishing adverb sub-types where finer prediction justifies the cost. That the number \emph{does} expand when the purpose changes is itself telling: the traditional inventory is the one that projects for grammatical description. The fat label survives not because it's accurate but because it's cheap, and because the system it sits in penalises proliferation.

Adverbs, then, are a label concealing heterogeneity. What follows is the reverse: a label concealing convergence~-- categories that look identical on the surface but are built by entirely different mechanisms.
