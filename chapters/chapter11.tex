\chapter{Lexical categories and their maintenance}
\label{ch:lexical-categories}

\epigraph{The resemblance of one animal to another is of exactly the same essential nature as the resemblance to a leaf, or to bark, or to desert sand, and answers exactly the same purpose.}{— Alfred Russel Wallace, \textit{Mimicry, and Other Protective Resemblances Among Animals} (1867)}

\section{Introduction}
\label{sec:11:intro}

A noun is a word that names a person, place, or thing.

This definition is wrong often enough to be interesting, and right often enough to be immortal.

You've heard this. Everyone has. It's the first thing they tell you in school, and the last thing they tell you in an introductory linguistics class~-- right before they take it away. The standard move goes like this: \mention{sincerity} names a quality, not a person, place, or thing, but it's a noun. \mention{Destroy} doesn't name anything, but it denotes an action~-- and so does \mention{destruction}, which \emph{is} a noun. The meaning changed; the syntactic behaviour didn't. Conclusion: whatever makes something a noun, it isn't naming. We need distributional tests~-- morphological and syntactic diagnostics that track \emph{how} a word behaves rather than \emph{what} it refers to. The semantic definition was, at best, a useful heuristic; at worst, a confusion of two different questions.

This is a genuine insight, and it has built the infrastructure of modern linguistics. Descriptive grammars, parsers, annotation schemes, and computational models all rest on the distributional turn: categories are defined by structural behaviour, not by meaning. The student who arrives saying \enquote{a noun names a thing} and leaves saying \enquote{a noun heads a noun phrase} has learned something real.

But the standard lesson can overshoot. The distributional turn was a rational compression: it bought extraordinary predictive power for grammars, parsers, and annotation schemes by keeping the ontology close to morphosyntax. The cost is that a second, equally real cluster~-- one that projects in cognition and in discourse~-- gets treated as pedagogical noise rather than as a legitimate target of theory.

\mention{Name} is a perfectly good category. It has a cluster of properties that travel together: names pick out individuals rather than describing them; they resist descriptive modification (\mention{tall Kim} requires a contrastive context~-- \mention{tall Kim, not short Kim}~-- where \mention{tall student} doesn't); they create referential opacity (if Lois doesn't know that Superman is Clark Kent, \mention{Lois believes Superman can fly} doesn't entail \mention{Lois believes Clark can}); they support rigid reference across contexts. These properties cluster because the cognitive and communicative functions that names serve~-- tracking individuals, sustaining reference across conversations and communities~-- require them to cluster. The clustering is real, maintained by real mechanisms, and projectible: knowing that something is a name tells you things about how it will behave semantically.

\mention{Noun} is also a perfectly good category. It heads noun phrases, takes determiners, inflects for number, fills argument slots, triggers agreement. These properties cluster because of distributional pressures: words that share some of these properties tend, over time, to share more of them. The clustering is maintained by acquisition, entrenchment, and structural analogy.

The extensions overlap heavily. Most names are nouns (\mention{Kim}, \mention{London}, \mention{Tuesday}). Most prototypical nouns do name things (\mention{dog}, \mention{chair}, \mention{idea}). The overlap is what makes the schoolroom definition feel right. But as Chapter~\ref{ch:projectibility} argued, overlapping extensions don't make overlapping categories. A syntactician and a semanticist can carve the same words differently without either being wrong~-- because they're tracking different HPCs, maintained by different mechanisms, projectible for different purposes.

We saw the same architecture with definiteness and deitality in Chapter~\ref{ch:definiteness-and-deitality}. The definiteness cluster (semantic: identifiability, uniqueness, familiarity) and the deitality cluster (morphosyntactic: \mention{there}-resistance, partitive licensing, nonrestrictive hosting) overlap substantially~-- most definite referents get marked as such~-- but they can decouple. Weak definites have deitality without definiteness; proper names have definiteness without deitality. Neither anomaly made sense while we treated them as one category. Both made sense once we recognized them as two: two clusters, two maintenance profiles, two projection targets.

The schoolroom definition of \enquote{noun} commits the same conflation. It treats the naming cluster and the noun cluster as a single category, then wonders why the definition has exceptions. The exceptions aren't exceptions~-- they're the visible evidence of play in the joint between two distinct but coupled HPCs. \mention{Sincerity} is a noun without naming a thing. \mention{The big house on the corner} picks out an individual without containing a name. The definition fails not because the semantics was wrong, but because two categories were hiding inside a single definition~-- noun and name in a trenchcoat, attempting to pass as a single lexical category at the border.

The deeper question, then, isn't \enquote{what is a noun?}~-- that question has good answers, all distributional. The question is: \emph{why does linguistics focus on noun rather than name?} Why is the syntactic category the one we build our grammars around, rather than the semantic one?

The answer is embarrassingly practical: noun projects better~-- for grammar. Learning that something is a noun~-- that it heads noun phrases, takes determiners, inflects for number~-- lets you predict its syntactic behaviour with extraordinary reliability across novel instances.

Name projects too, but in a different domain. Knowing that something is a name is enormously important for memory, cognition, and social interaction~-- for tracking individuals, managing relationships, sustaining reference across years of shared experience. But it tells you relatively little about morphosyntax. The grammarian's preference for noun over name isn't a quality judgment; it's field-relative projectibility in action. Grammars are built around noun because noun is the category that projects for grammatical purposes. Name is the cognitive scientist's and the social psychologist's category. Each is real; each earns its keep in its own domain. (Why linguistics itself is organized around morphosyntax rather than around naming or reference~-- why the discipline carved where it did~-- is a question I return to in Chapter~\ref{ch:grammaticality-itself}.)

But this answer opens a further question. If nouns are thick HPCs, what about the other word classes? Are they all equally well maintained? Do they all project equally well?

They don't.

\bigskip

This chapter is a dissection. Nobody will be harmed except a few traditional labels. It opens the dictionary's inventory of word classes and asks which of them are genuine HPCs~-- categories maintained by converging mechanisms, projectible to novel instances~-- and which are something less: convenient labels, historical residues, or bookkeeping categories that survive because they are cheap.

The answer doesn't fall neatly along traditional lines. Some classes that look fundamental turn out to be thin. Others that look like ragbag leftovers turn out to conceal multiple genuine categories. The exercise will reveal three configurations that keep recurring across the linguistic hierarchy, each with its own diagnostic signature:

\begin{itemize}
    \item \textbf{Robust categories} where multiple mechanisms converge on a thick, projectible cluster. \term{Noun} and \term{verb} are the paradigm cases~-- the skeletal categories that recur across languages because the discourse functions they serve (reference and predication) demand them.
    \item \textbf{Thin classes} where the clustering is real but the mechanisms are fewer and less tightly coupled. \term{Adjective} is the pivotal example: a category in English, with genuine distributional coherence, but one that varies dramatically across languages in its independence and scope~-- a category in some languages, absent in others.
    \item \textbf{Fat labels} where a single term obscures multiple unrelated clusters. \term{Adverb} is the classic case~-- what Quirk called \enquote{the dustbin of the parts of speech}, a class with excellent storage capacity and terrible explanatory power~-- a perfectly good drawer, provided you don't mistake it for a theory. And the traditional \term{pronoun} turns out to be another: a surface-distribution class that masks at least three distinct convergent-evolution stories.
\end{itemize}

The Wallace epigraph at the head of this chapter is the key. The resemblance of one animal to another, Wallace saw, \enquote{answers exactly the same purpose} as the resemblance of an animal to a leaf or to desert sand. The resemblance isn't accidental~-- it's functionally driven. But the functions can differ profoundly. Two species can look alike because they share ancestry; or because different selection pressures converged on the same solution; or because one is parasitizing the other's signal. The resemblance is real in every case. What differs is the machinery that produces it.

Word classes pose the same question. When a group of words looks like a category on the surface~-- when they share distributional profiles, occupy similar slots, or just attract the same label~-- the question is: \emph{why?} Do they cohere because the same mechanisms actively maintain them (genuine kin)? Because different trajectories happened to converge on the same structural space (mimics)? Or is the unity entirely an illusion~-- a filing convention where a taxonomist shoved the leftovers? This chapter is about learning to ask that question, and about what the answers reveal.

Section~\ref{sec:8:coupling} introduced a continuum from hard coupling (phonemes) through loose coupling (grammatical categories) to re-unified coupling (constructions). Here that continuum earns its keep within a single level of description. Nouns and verbs are tight~-- multiple mechanisms converge, and the categories resist perturbation the way hard-coupled systems do. Adjectives are genuinely loose~-- the mechanisms are fewer, the category dissolves in languages that don't reinforce it. Adverbs aren't on the continuum at all; the label names a filing convention, not a coupling. What varies across word classes isn't importance but mechanism density~-- and mechanism density is what the coupling continuum measures.


\section{The skeleton: nouns and verbs}
\label{sec:11:skeleton}

Nouns and verbs, every language has 'em.

Or do they? Typologists have debated it for decades, and the qualification matters. What every language has is a way to refer and a way to predicate. What many languages have is a fairly stable partition of those jobs across recurrent resources. What some languages have is a stable lexical category split that a learner can treat as pre-partitioned \term{noun} and \term{verb}. When applied across languages, \term{noun} and \term{verb} denote comparative concepts~-- analyst-constructed categories that allow comparison, not universal entities projected from English (\citealt{Haspelmath2010}; Chapter~\ref{ch:comparanda}). But the comparative concepts keep earning their keep: across families, profiles, and documented change, linguists keep finding specialized resources for reference and specialized resources for predication. The labels are ours; the functional pressure is not.

A typologist's caution: even where the comparative concepts keep earning their keep, the stabilised clustering need not take the form of a stable lexical category. In languages argued to have highly flexible roots, the same lexical item can participate in referential and predicative constructions with minimal or no derivational marking, and the strongest distributional regularities live in the constructions themselves~-- clause-typing morphology, argument-indexing patterns, selectional profiles of particles and auxiliaries. The question then isn't whether the lexicon is pre-partitioned into nouns and verbs, but whether the \emph{constructional ecology} enforces a durable partition of \emph{uses} into referential and predicative profiles. On the HPC view, that's still a thick cluster~-- but the thickness is located where the mechanisms converge. A language can be thin in lexical-category terms and thick in constructional terms, without contradiction.

Why? The maintenance view offers an answer that essentialism can't. Formal feature systems are often excellent \emph{bookkeeping}: they compress robust distributional generalisations and make sharp predictions in the grammar they are designed to model. The question here isn't whether that bookkeeping works, but why it is so stable for nouns and verbs and so much less stable elsewhere.

The essentialist says: nouns exist because nounhood is a primitive of the language faculty. The most developed version, \citet{baker2003}, derives the cluster from a semantic-syntactic interface property~-- a criterion of identity for nouns, predication for verbs~-- and this gets the extension right: reference and predication really are the core functions. But even this derivation runs from a postulated primitive to a set of consequences. What maintains the connection? Why does the criterion of identity continue to correlate with determiners, number, argument structure, and case? The interface condition names the functional niche; it doesn't identify the mechanisms that keep the niche populated.

The HPC account says: nouns exist because multiple mechanisms converge on the same clustering. Language has two jobs that recur in every communicative ecology: \emph{building references to things} and \emph{saying things about those references}. These aren't formal stipulations; they're functional pressures arising from the structure of discourse. Every utterance that does more than emote needs to identify what it's about and say something about it. The words that specialize in identification cluster together, and the words that specialize in predication cluster together, because the functional demands are tight enough to keep the clustering going.

What demands, exactly? The mechanisms are familiar from earlier chapters, but their convergence on nouns and verbs is unusually strong. It's worth walking through them carefully, because the contrast with thinner categories later in the chapter depends on seeing just how many independent forces are at work here.

\begin{itemize}
    \item \textbf{Discourse frequency.} Reference and predication are the backbone of every clause. In any substantial corpus, nouns and verbs together typically account for the largest share of tokens in running text~-- not because linguists have decided to count them that way, but because the communicative jobs they do are the ones that recur most relentlessly. You can construct a clause without an adjective, without an adverb, without a preposition. You cannot construct one without something that refers and something that predicates. That relentless frequency breeds the entrenchment described in Chapter~\ref{ch:stabilisers}. But the robustness has two sources: high \emph{token} frequency~-- every clause needs reference and predication~-- and massive \emph{type} frequency~-- thousands of distinct nouns and verbs filling the same structural slots. Token frequency entrenches individual forms (hence the irregular survival of \mention{go}/\mention{went}); type frequency builds the schematic category, because the more distinct items a learner encounters in a frame like \mention{the \_\_\_}, the stronger the abstraction.
    \item \textbf{Morphological agreement.} Once a language develops agreement~-- subject-verb marking, noun-adjective concord, case systems~-- the agreement itself becomes a mechanism that stabilizes the categories it links. Agreement demands a controller and a target; the controller is typically a noun, the target a verb or adjective. Consider Swahili, where the noun-class prefix \mention{m-} on \mention{mtoto} (`child') triggers concordant prefixes on verbs (\mention{a-nasoma}, `s/he reads'), adjectives (\mention{m-zuri}, `good'), and demonstratives (\mention{h-uyu}, `this'). The system doesn't just \emph{mark} the noun-verb distinction; it \emph{enforces} it. Every new word entering the language must slot into a noun class or a verb paradigm, because the agreement morphology has no slot for anything else. The morphological lock-in stabilizes the category boundary from both sides: nouns are the things that control agreement; verbs are the things that show it.
    \item \textbf{Acquisition.} Children learn nouns and verbs early, and they learn them as categories~-- not just as individual words. The evidence is overgeneralization: children who say \mention{I goed} or \mention{two mouses} have extracted a category-level pattern, not memorized a form. \citet{tomasello1992} showed that early verb use is organized around \enquote{verb islands}~-- initially, each verb is its own construction, with its own argument frame. But by age three, children have abstracted across the islands, extracting the verb category as a generalization. The abstraction happens because the functional pressure (predication) keeps presenting the same distributional pattern across different lexical items~-- and what gets abstracted early gets entrenched early.
    \item \textbf{Structural analogy.} Novel words get slotted into pre-existing categories. You encounter \mention{to shlep} for the first time and immediately know it inflects (\mention{she shleps}, \mention{they shlepped}), takes objects (\mention{shlep the suitcase}), appears in verb-phrase constructions (\mention{keep shlepping}). You don't need to be told; you \emph{predict}. Structural analogy extends the category to new members, maintaining the cluster from the open end. But this mechanism is emergent, not primitive. The verb-island data just described show that two-year-olds don't analogize across verbs; the leap requires a critical mass of exemplars. Once reached, it's self-reinforcing~-- analogy doesn't initiate the category but amplifies one that has begun consolidating through the other mechanisms.
    \item \textbf{Semantic recruitment.} The world keeps producing entities that need to be referred to and events that need to be predicated~-- and the naming function, the very thing the introduction distinguished from nounhood, is itself a feeder mechanism. Name and noun are distinct HPCs, but the semantic pressure of the first continuously recruits members into the second. The open-class nature of nouns and verbs isn't a stipulation; it's a consequence of the fact that the world's referents and events are open-ended.
\end{itemize}

No single mechanism explains the universality. What explains it is the convergence. Consider what would happen if only one mechanism were operating. If discourse frequency were the only force, we'd expect nouns and verbs to be entrenched but not necessarily morphologically distinct~-- frequent patterns can coexist without forming separate paradigms. If agreement were the only force, we'd expect morphological categories but not necessarily semantic coherence~-- agreement can track arbitrary classes (as Swahili's noun classes demonstrate). If acquisition were the only force, we'd expect early-learned categories to persist but not necessarily to attract new members. Each mechanism alone would produce some clustering, but the clusters would be partial, fragile, and variable. Partial clusters invite drift; drift invites reanalysis; reanalysis invites new partitions. Keep that counterfactual template in view: we'll reuse it to diagnose why \emph{adjective} and manner adverbs thicken only partway toward skeletonhood, and why \emph{adverb} never thickens at all.

What makes nouns and verbs robust is that all five mechanisms push in the same direction. Functional pressure, morphological lock-in, early acquisition, structural analogy, and semantic recruitment all converge on the same partition of the lexicon. Remove any one of them and the category would still survive, maintained by the others. This is the signature of a robust HPC~-- not a single causal thread but a cable of independent strands, each sufficient to maintain some clustering, collectively sufficient to maintain it all.

The metaphor I want here is the \term{skeleton}. \citet{zimmer2015} observed that sharks and dolphins look strikingly alike~-- streamlined bodies, dorsal fins, tapered snouts~-- despite having diverged hundreds of millions of years ago. The resemblance isn't ancestry; it's physics. Hydrodynamic drag imposes the same penalty on every aquatic body plan, and the skeleton~-- the load-bearing architecture underneath~-- is what gets reshaped to meet it. Different lineages, same functional pressure, same skeletal solution. Nouns and verbs are the grammatical skeleton. Different language families, same communicative pressure~-- reference and predication~-- same categorical solution. The analogy invites a question: what is the grammatical equivalent of drag? The answer is a bundle of constraints~-- working-memory limits on how long a referent can stay unanchored, coordination costs that penalise ambiguity between who-does-what and what-happens, learnability bottlenecks that filter out systems without recurrent structural landmarks. These aren't metaphorical pressures; they're measurable ones, and they converge on the same partition for the same reasons that drag converges on the same body plan. Drag doesn't mandate a blueprint; it makes some blueprints survivable. Discourse doesn't mandate nouns and verbs; it makes languages without them unspeakable.

This explains something that essentialism treats as primitive: why nouns and verbs are the categories around which grammars are organized. Not because universal grammar stipulates them. Because the forces that maintain them are the strongest, the most convergent, and the most resistant to perturbation in the entire grammatical system. They're the last categories standing when a language simplifies under contact, pidginization, or creolization. They're the first categories children acquire. They're the categories that agreement systems lock into place.

The strongest challenge comes from languages argued to lack the distinction entirely~-- languages where any root can serve referential or predicative functions without dedicated morphology. The HPC response isn't that these analyses are wrong, but that the functional pressure should still leave distributional traces: frequency asymmetries in discourse, processing differences between referential and predicative deployment, acquisitional ordering that tracks the functional distinction even where the formal one is absent. If a language showed no such asymmetries~-- truly symmetrical distribution, no processing cost for function-switching, no acquisitional priority~-- the convergence story would need qualification. The comparative concept would survive; the universal-mechanism claim would not.

The skeleton metaphor also sets up what comes next. Not every organ is skeletal. Some are essential but variable~-- lungs take different forms in reptiles and mammals. Some are ornamental~-- plumage, pigmentation, display structures. And some resist classification altogether~-- the spleen has puzzled anatomists for centuries. Word classes, we'll see, show the same gradient.


\section{Thin classes: the adjective as plumage}
\label{sec:11:thin-kinds}


If nouns and verbs are the skeleton, adjectives are the plumage.

Some birds have made plumage into a spectacle~-- the peacock's tail, the bird of paradise's display feathers. Others have almost dispensed with it~-- the kiwi, ground-dwelling and shaggy, barely has feathers worth mentioning. But every bird has a skeleton. The plumage is variable, optional, sometimes magnificent, sometimes vestigial. The skeleton is non-negotiable.

Languages show the same pattern. English has a large, open adjective class numbering in the thousands, with its own comparative morphology (\mention{taller}, \mention{tallest}), its own degree modification (\mention{very}, \mention{too}, \mention{extremely}), and two distinct syntactic homes: attributive (\mention{the tall student}) and predicative (\mention{the student is tall}). English is the snowy owl, with plumage so thick and functional it looks structural.

But many languages are kiwis. \citet{dixon2004} surveyed adjective classes across hundreds of languages and found a striking gradient. At one end are languages like English, with large, open adjective classes numbering in the thousands~-- languages where the adjective category is as robust as the noun category in terms of productivity and distributional coherence. At the other end are languages with no independently diagnosable adjective lexical category~-- property concepts are expressed, but through verbal or nominal strategies rather than a dedicated word class. Between them sits the majority: languages with small, closed adjective classes, often numbering fewer than two dozen items, typically covering the semantic domains Dixon identified as the \enquote{core} property concepts~-- dimension (\mention{big}, \mention{small}), age (\mention{old}, \mention{new}), value (\mention{good}, \mention{bad}), and colour (\mention{black}, \mention{white}).

Where English says \mention{the tall building}, Mandarin uses a stative verb construction: \mention{gāo de dàlóu} uses the property concept \mention{gāo} (`tall') as a predicate-like modifier, not as a member of a separate adjective paradigm. Where English says \mention{the red door}, Bantu languages like Swahili manage with a small closed set of true adjectives and route most property concepts through relative-clause constructions: `a door which is red' rather than `a red door'. The \term{property-concept} function~-- attributing size, age, colour, value to a referent~-- exists in every language. A dedicated lexical category for doing it does not.

Dixon's survey makes a further point that matters for the HPC framework. Even in languages with small adjective classes, the \emph{core} members~-- dimension, age, value, colour~-- tend to be the ones that survive. The category doesn't shrink randomly; it shrinks from the periphery inward, losing less frequent and less functionally essential members first. This is exactly what the maintenance view predicts: the mechanisms are strongest where the functional pressure is highest, and they weaken as the pressure drops. The core property concepts are the ones most frequently needed in attributive modification; the peripheral ones can be handled by other means without communicative loss.

Why doesn't the full category stabilize? Compare the mechanisms:

\begin{itemize}
    \item \textbf{Discourse function.} Nouns and verbs serve the two primitive discourse operations: reference and predication. Every clause needs them. Adjectives serve \emph{attribution}~-- narrowing a reference or elaborating a predication. This is useful but never essential: you can always build a reference without an adjective (\mention{the student who is tall} instead of \mention{the tall student}). The adjective provides a shortcut~-- packing a relative clause's worth of information into a single prenominal slot~-- but shortcuts, by definition, are dispensable. The discourse pressure is real but shallow: it rewards efficiency, not necessity. And shallow pressure produces shallow clustering.
    \item \textbf{Acquisition.} Children acquire nouns and verbs before adjectives, and the delay is not trivial. Property concepts like colour and value are more abstract than objects and actions~-- they require comparing an entity to a scale, which presupposes the entity concept (\mention{big} needs something to be big). Dimensional adjectives emerge earlier than evaluative ones; colour terms are notoriously late and unstable. The acquisition trajectory mirrors Dixon's typological gradient: the property concepts that are easiest to ground (dimension, age) stabilize first; the harder ones (value, colour) are more variable. Later entry into the system means fewer cycles of entrenchment before the transmission bottleneck begins filtering~-- and less resistance to the competing strategy of handling property concepts through verbs or nouns.
    \item \textbf{Morphological glue.} Nouns have case, gender, and number. Verbs have tense, aspect, and agreement~-- and the agreement sits on top of inflectional categories that belong to verbs alone. The verb's morphological identity is thick before agreement even enters the picture. Adjectives, in many languages, have no such foundation. They carry no inflectional categories of their own~-- no adjectival equivalent of tense or aspect. What morphology they have is borrowed: concord with the noun's gender, number, and case. When the only morphological glue holding a category together is copied from another category, the bond is weaker~-- because the morphology doesn't differentiate the adjective from the noun; it assimilates the adjective \emph{to} the noun. In Latin, the adjective \mention{bonus} declines through the same paradigm as the noun \mention{dominus}. In Swahili, the handful of true adjectives take the same noun-class prefixes as the nouns they modify. The morphology says: \enquote{this word agrees with a noun.} It does not say: \enquote{this word belongs to a distinct category.} And it shows: across attested language change, adjective classes shrink as stative-verb or denominal strategies expand to fill the same functional niche.
\end{itemize}

Even in English, where the adjective cluster is thick, you can see play in the joints. Many adjectives resist the inflectional comparative (\mention{more curious}, not \mention{*curiouser}; \mention{more beautiful}, not \mention{*beautifuller}). The comparative split itself is a distributional fracture within the category: monosyllabic adjectives take \mention{-er}/\mention{-est}; polysyllabic ones take \mention{more}/\mention{most}; disyllabic ones waver (\mention{commoner} or \mention{more common}?), and English declines to issue a ruling. This isn't the behaviour of a category with tight internal cohesion. It's the behaviour of a category held together by a few shared properties (gradability, attributive position, predicative position) while differing on others.

The positional split is equally telling. Some adjectives are restricted to attributive position (\mention{the main reason} but not \mention{*the reason is main}; \mention{the mere thought} but not \mention{*the thought is mere}); others to predicative (\mention{the child is asleep} but not \mention{*the asleep child}; \mention{the patient is well} but not \mention{*the well patient}).

\citet{matthews2014} argues that the split runs deep enough to warrant treating attributive and predicative adjectives as distinct categories. The argument has force: the two groups differ not just in position but in meaning~-- \mention{old} means `elderly' predicatively (\mention{the professor is old}) but can mean `former' attributively (\mention{my old professor}); \mention{late} means `deceased' attributively (\mention{the late president}) but `not on time' predicatively (\mention{the president was late}). Position and meaning have partially decoupled.

But most adjectives do both jobs happily~-- degree modification, comparative morphology, and property-concept semantics work across both positions~-- and the core semantic type (property of individuals) is shared. The edges fray; the centre holds. That's the thin-class signature: shared mechanisms at the core, loosening at the margins. A fat class, by contrast, has no shared mechanisms at all~-- only a shared filing convention.

This is what a \term{thin} HPC looks like. The clustering is real~-- in English, knowing that something is an adjective lets you predict a great deal of its syntactic behaviour. But the mechanisms maintaining the cluster are fewer and less tightly coupled than for nouns and verbs. Remove the morphological glue (as many languages do), and the category thins. Remove the dedicated syntactic slot (as stative-verb languages do), and it thins further. Remove both, and the category dissolves~-- not because adjectives are unreal, but because the mechanisms that held them together were never as robust as the ones that hold nouns and verbs.

The adjective sits lower on the naturalization gradient (Chapter~\ref{ch:comparanda}). As a comparative concept, it earns its keep: it identifies a real functional niche across languages, and linguists can meaningfully ask whether a given language fills that niche with a dedicated class. But as a language-internal category, it ranges from thick (English, with thousands of members and dedicated morphology) to absent (languages where property concepts are just stative verbs).

Even within a single language, the thickness of the cluster varies across communicative situations (Chapter~\ref{ch:social-stabilization}). The adjective class that grammarians describe is the one stabilized in the most general com-sit~-- the standard variety, written and formal. In narrower com-sits, the category may thin further. Casual spoken English leans harder on verbal strategies for property concepts (\mention{that sucks} rather than \mention{that is bad}; \mention{it stinks} rather than \mention{it is unpleasant}), and child-directed speech relies on a smaller, more repetitive set of adjectives than adult written prose. The reference grammar captures the thickest version of the cluster~-- the snowy owl in full winter plumage. Other com-sits see the feathers thin. The category is a local habit, and the locality is finer-grained than \enquote{a language}.

The plumage story has a disconfirmation condition. It predicts that adjective-class thickness should track mechanism density: languages with fewer supporting mechanisms should have thinner adjective classes. If a language showed a tiny, closed adjective class~-- a dozen items~-- that still maintained its own morphological paradigm (not borrowed concord), its own syntactic exclusivity, and diachronic stability without expanding, that would be a category sustained by something other than mechanism convergence. The thin-class diagnosis would be wrong; the category would be skeletal after all, just small.

But at least plumage, where it exists, is real. Thin or thick, it has structure~-- feathers that interlock, pigments that signal, insulation that functions. What about the class that has none of this? The one that is just a drawer label~-- a name for whatever didn't fit anywhere else?


\section{The wastebasket: adverbs and what's inside}
\label{sec:11:wastebasket}

Consider what travels under the label \mention{adverb}: \mention{quickly} modifies a VP, specifying manner; \mention{very} modifies a gradable head, specifying degree; \mention{however} links propositions, specifying discourse relation; \mention{frankly} targets a speech act, specifying stance; \mention{only} associates with focus, specifying alternatives; \mention{yesterday}~-- in CGEL's analysis~-- is a noun functioning as adjunct \citep{huddleston2002}, specifying time. Six words, six jobs, one label, doing no work whatsoever.

This is Chapter~\ref{ch:failure-modes}'s fat-class signature. No shared proper function, no converging mechanisms, no cluster of properties that travel together across the class. Knowing that something is \enquote{an adverb} tells you it isn't a noun, a verb, or an adjective~-- and almost nothing else. The label's projectibility is exhausted by its negative definition.

But the wastebasket isn't empty. Crack it open and at least one genuine category falls out.

Manner adverbs~-- \mention{quickly}, \mention{carefully}, \mention{beautifully}, \mention{abruptly}~-- have a real cluster, maintained by real mechanisms. First, a semantic niche: events can be performed in ways, and languages need to express this. The manner-of-event function exists in every language, whether it's filled by adverbs, serial verbs, ideophones, or verbal morphology. The functional pressure is universal; the categorical realization depends on whether enough mechanisms converge.

In English, they do. Productive \mention{-ly} derivation keeps the class open: any adjective that can characterise an event gets a ticket in. Syntactic positioning is independently maintained~-- \mention{fast}, \mention{well}, and \mention{hard} occupy the same VP-internal slots without \mention{-ly}, so the distributional cluster isn't a morphological shadow~-- and dependency-distance minimisation reinforces it \citep{Gibson2026}: processing cost rises with the syntactic distance between a modifier and the predicate it semantically modifies, penalising placement far from the VP and keeping the cluster positionally tight. And gradability works across the class (\mention{more carefully}, \mention{less abruptly}), connecting manner adverbs to the degree-modification system.

That's three or four independent mechanisms converging on a single cluster~-- comparable to adjectives, thinner than nouns and verbs~-- and the cross-linguistic prediction follows the same pattern as Dixon's adjective gradient: manner expression recurs universally; a dedicated manner-adverb class doesn't.

The \mention{-ly} morphology is particularly revealing, because it feeds two categories, not one. \mention{She spoke frankly} is manner~-- it describes how she spoke. \mention{Frankly, she's wrong} is speech-act modification~-- it characterises the speaker's communicative stance, not the event. Same derivational process, different functional niche, different syntactic behaviour, different category. One mechanism producing forms that land in different clusters because the other mechanisms pulling on them diverge. The name/noun architecture from the introduction, reprised.

The remaining groups are thinner still. Degree modifiers (\mention{very}, \mention{quite}, \mention{rather}) form a small functional class~-- coherent but mostly closed, maintained by entrenchment rather than productive recruitment. Focusing adverbs (\mention{only}, \mention{even}, \mention{just}) share an alternatives-based semantics, but they're a handful of items, not a productive class. Connective adverbs (\mention{however}, \mention{moreover}, \mention{consequently}) are doing a coordinator's work from adjunct position~-- less a category than a syntactic disguise. Each group projects in its own domain, but none has the mechanism convergence that manner adverbs do.

The sub-classification itself is diagnostic. CGEL sub-classifies adverbs by semantics~-- manner, degree, focusing, connective~-- and rightly so: the semantics is genuinely central to what distinguishes these groups. But the need for semantic sub-classification is telling. For nouns and verbs, distributional criteria do the heavy lifting; the semantics comes along for the ride because the mechanisms converge. For adverbs, distribution alone can't sort the contents, because the groups don't share enough distributional coherence for distribution to track. The grammar reaches for semantics not as a supplement to distributional classification but as its replacement~-- which is exactly what the introduction's lesson about nouns and names warned us to notice.

The wastebasket doesn't just group unrelated items; it actively misclassifies items that belong elsewhere. Even CGEL~-- the most careful descriptive grammar of English~-- treats \mention{more} and \mention{less} as determinatives in most environments but categorizes them as adverbs in analytic comparatives (\mention{more interesting}, \mention{less quickly}). The sole grounds: \mention{more} supposedly fails to contrast with \mention{much} as a degree modifier in these contexts. But the contrast does exist~-- with comparative governors (\mention{much different} alongside \mention{more different}), with participial adjectives (\mention{much improved} alongside \mention{more improved}), and in contexts that CGEL's restricted conception of analytic comparatives excludes \citep{reynolds2024}.

The distributional patterns that motivated the reclassification turn out to follow from the pragmasemantics of scale structure: \mention{more} establishes a reference point on a scale where none was salient; \mention{much} requires one already in place. No category switch is needed. A single diagnostic thread~-- lack of distributional contrast~-- was carrying the entire adverb categorization. When it frayed, the classification had nothing else to stand on. One thread is not a cable.

The projectibility gap between label and contents is wider for \mention{adverb} than for any other traditional word class. \mention{Noun} and \mention{verb} project because they track genuine categories. \mention{Adjective} projects, with more noise, because it tracks a thinner category. \mention{Adverb} barely projects at all~-- whatever predictive power exists lives in the groups the label conceals.

The wastebasket diagnosis has an alternative worth confronting. One could argue that \mention{adverb} names a coherent functional supercategory~-- \enquote{clausal modifier}~-- with its own cross-linguistic stability. If that were right, we'd expect two things the HPC account doesn't predict: cross-linguistic persistence of adverb-as-class comparable to noun-verb, and acquisition of the heterogeneous class as a unit rather than piecemeal by sub-type. The evidence on both counts favours the wastebasket: adverb inventories vary wildly across languages, and children acquire manner, degree, and connective functions on independent timelines. But the falsifier is clear, and it is testable: if \mention{adverb} were stable across languages, stable in acquisition, stable as a unit, the wastebasket story would fail.

Why, then, doesn't the wastebasket Balkanize? If the groupings are real and the fat label is diagnostically near-useless, why do we still have it?

A hypothesis: \term{lexical category} is itself an HPC, and one of its maintained properties is a small category count. If so, we should expect grammars and tagsets to preserve a few high-level labels even when finer partitions would improve local prediction, and to push subdivision \emph{inside} inherited labels rather than dissolving them. How small? Cross-linguistically, major open-class inventories range from about one to four \citep{rijkhoff2007}: some languages use a single flexible class for both reference and predication; most distinguish nouns from verbs; fewer add a dedicated adjective class; fewer still a distinct adverb class. No language has been described as needing fifteen or fifty. The bound isn't conventional~-- it reflects the small number of fundamental discourse functions (reference, predication, modification) that create niches for dedicated categories \citep{hopperthompson1984, croft2001}. Languages partition the functional space differently, but the space itself is bounded.

The clustering that makes \mention{lexical category} a useful concept~-- distributional coherence, morphological paradigm, headed phrase structure~-- is maintained by the same pressures that shape individual categories: the transmission bottleneck favours systems that can be learned from finite input, and fewer categories means a shorter grammar.

That's the ontology. But the terminology faces a parallel pressure. A grammar is a compression of a language, and the linguist's label system has its own economy. Splitting the \mention{adverb} label into six classes inflates the descriptive inventory, and each additional class costs descriptive bits. The cost of six narrow labels can exceed the predictive gain over one broad one~-- especially when most of the groups are small enough that listing their members is cheaper than defining a class.

The fat label survives not because it's accurate but because it's cheap, and because the sociology of grammar teaching entrenches it. Computational tagsets~-- Penn Treebank, Universal Dependencies~-- have subdivided \mention{adverb} into finer types, but the subdivision happens \emph{within} the wastebasket rather than dissolving it. Arguably they shouldn't have: the contents could be distributed to the categories where they actually belong. That the traditional label persists even when finer distinctions would improve prediction is field-relative projectibility in action~-- the label projects for grammatical description partly because it's entrenched, not because it carves at joints.

Adverbs, then, are a label concealing heterogeneity. What follows is the reverse: a label concealing convergence~-- categories that look identical on the surface but are built by entirely different mechanisms.


\section{Pronouns and the braid}
\label{sec:11:mimics}

Start with the uncontroversial cases. The core personal pronouns~-- \mention{I}, \mention{you}, \mention{she}, \mention{he}, \mention{we}, \mention{they}~-- are a tight cluster. They belong to a closed category, participate in the system of case (\mention{she}/\mention{her}/\mention{hers}), head NPs without a determiner (\mention{she left}, not \mention{*the she left}), and are referentially self-sufficient: each gets its reference from the speech situation (deictic) or from a prior mention (anaphoric), not from descriptive content. They also participate in English gender (Chapter~\ref{ch:proform-gender}). CGEL formalizes the syntactic profile: pronouns are a subcategory of nouns.

The label \mention{pronoun} relates to the concept \term{pro-form} the way the label \mention{noun} relates to \mention{name}. This chapter opened with that parallel: noun and name are distinct HPCs, one syntactic and one functional, overlapping in the core cases and diverging at the edges. The same structure holds here. \mention{Pronoun} names a syntactic cluster: heads NPs, case-marked, closed category. \mention{Pro-form} names a semantic one: referentially dependent, light in descriptive content, taking its value from context. In the core personal pronouns both are present~-- \mention{she} has the syntax and the semantics. Further out they come apart: \mention{today} has pronoun syntax but carries its own descriptive content; \mention{do so} has pro-form semantics but belongs to a different syntactic category entirely. Two clusters, one label trying to name their overlap.

Where did this overlap come from? The items that ended up in the synchronic pronoun category grew from different roots. The personal pronouns descend from the demonstrative stock: Old English \mention{hē} and its relatives were demonstrative in origin; \mention{she} likely derives from the OE demonstrative \mention{sēo}; \mention{they} was borrowed wholesale from Old Norse \mention{þeir}, itself demonstrative.

Interrogative \mention{who} has an entirely separate ancestry: PIE \mention{*kʷis}/\mention{*kʷod}, the interrogative stem~-- no demonstrative origin, no deictic anchoring, no connection to the pointing-and-tracking system. And relative \mention{who} is newer still: Old English used the particle \mention{þe} or demonstratives for relativization, not the interrogative form; the use of \mention{who} as a relative developed through Middle English, following a well-documented grammaticalization pathway (interrogative~$\rightarrow$~relative). The diachronic split even leaves a phonological trace, as if the two lineages were still wearing their family colours: the demonstrative-stock items cluster around /ð/~-- \mention{the}, \mention{this}, \mention{that}, \mention{there}, \mention{then}, \mention{they}, \mention{them}~-- the same /ð/-initial cohort that Chapter~\ref{ch:definiteness-and-deitality} traced through the deitality system; the interrogative-stock items cluster around /w/~-- \mention{who}, \mention{what}, \mention{which}, \mention{where}, \mention{when}.

Two diachronic lineages, one synchronic territory. In biology, when unrelated lineages end up with similar property clusters, the question for HPC theory is whether the similarity reflects homology (same category, grounded in shared descent) or analogy (different categories with convergent outputs). The standard answer is that homologous structures constitute the same category because they share genealogical maintenance mechanisms, while analogous structures~-- eyes in vertebrates and cephalopods, wings in bats and birds~-- are distinct categories maintained by different mechanisms that happen to produce similar results.

But there's a further possibility that the analogy framework misses: functional categories. \enquote{Eye} is a legitimate scientific category that supports induction and explanation regardless of whether a given eye is vertebrate or cephalopod, because the functional constraints~-- light detection, focusing, image formation~-- maintain the property cluster independently of the genealogical lineage. The category isn't grounded in shared descent; it's grounded in shared operational demands.

The pronoun territory exhibits exactly this structure~-- not one category but two types of category operating on different planes.

The first is a lexical category. Pronoun, as CGEL uses the term, is a lexical category, and morphological integration is one of the mechanisms that hold it together. The pronoun paradigm is the last stronghold of productive case morphology in English: nominative/accusative/genitive (\mention{I}/\mention{me}/\mention{my}, \mention{she}/\mention{her}/\mention{hers}, \mention{who}/\mention{whom}/\mention{whose})~-- a three-way distinction that no other word category in the language still maintains. Add person and number, encoded by suppletive forms (\mention{I} vs \mention{we}, not \mention{*is}), reflexive formation (\mention{myself}, \mention{herself}, \mention{themselves}), and the genitive split (\mention{my}/\mention{mine} vs the \mention{'s} that nouns use), and the paradigmatic structure is dense.

Section~\ref{sec:11:skeleton} argued that morphological glue is a maintenance mechanism. For adjectives, the glue is borrowed~-- concord with the noun, not a paradigm of their own~-- which is why the category thins cross-linguistically. Pronouns have their own morphology, and it's thick. That paradigmatic integration goes a long way towards making pronoun a robust lexical category, maintained by the same sorts of mechanisms that maintain nouns and verbs: entrenchment, analogical pressure within the paradigm, constructional selection.

Consider \mention{who}'s credentials concretely. On the lexical-category face, the core properties are present: \mention{who} inflects for case (\mention{who}/\mention{whom}/\mention{whose}), heads NPs, resists determiners, and belongs to a closed inventory. But the morphological integration is partial. \mention{Who} is morphologically invariant for person and number~-- uniform where personal pronouns supplete (\mention{I}/\mention{we}, \mention{he}/\mention{they})~-- forms no reflexive (\mention{*whomself}), and its accusative \mention{whom} is moribund in most varieties while the personal-pronoun case distinctions (\mention{I}/\mention{me}, \mention{she}/\mention{her}) remain robust.

\mention{Who} belongs to the pronoun category, but at the edge of its paradigmatic core, not the centre~-- gradient membership, exactly as HPC predicts. On the semantic face, the picture also splits. In a relative construction~-- \mention{the person who called}~-- \mention{who} is a pro-form: anaphorically bound to its antecedent, referentially dependent, substitutable. It has the syntax of a pronoun and the semantics of a pro-form~-- both faces align.

In an interrogative construction~-- \mention{who called?}~-- \mention{who} is not a pro-form: it introduces a variable over alternatives; it's not referentially dependent on any antecedent, not substituting for a fuller expression. It has much of the morphosyntax of a pronoun but not the semantics of a pro-form. Same word, same lexical category, different semantic status depending on the construction. The lexical category and the semantic concept come apart within a single word.

That's the lexical category. The second type of category operating in the pronoun territory is operational, not lexical. \mention{Who}, \mention{which}, \mention{what}, \mention{where}, \mention{when}, and \mention{how} share a cluster of structural properties: fronting, gap-binding, scope interaction, prosodic focus. Traditional grammar often calls them all \enquote{interrogative pronouns}~-- projecting the lexical-category label onto items that share the operational properties but not the paradigmatic ones. The label \enquote{question words} is narrower but still misses: the same forms serve relative and exclamative functions too~-- \mention{who} in a relative construction doesn't interrogate~-- so the label is too narrow. The label \mention{wh-word}, the linguist's alternative, is phonologically defined and English-specific. Call the operational family \term{IRE words}: interrogative, relative, exclamative.

This IRE family cross-cuts the lexical categories: \mention{who} is a pronoun, \mention{which} a determinative, \mention{where} and \mention{when} prepositions, \mention{how} an adverb. No single lexical category contains them~-- and the morphological boundaries confirm the split. \mention{Who} inflects for case, forms reflexives, and has a genitive; \mention{where} and \mention{when} do none of these. The pronoun morphology stops where the lexical category stops, even though the operational properties (fronting, gap-binding, scope) continue across the boundary.

Knowing that something belongs to the IRE family lets you predict fronting, gap-binding, scope-taking, and prosodic focus~-- regardless of the item's lexical category. That's genuine projectibility. The IRE family is an operational category~-- maintained not by paradigmatic descent but by shared operational constraints. The mechanisms that make \mention{who} front are the same mechanisms that make \mention{where} front, even though the two items belong to different lexical categories, just as the optical constraints that shape a vertebrate eye also shape a cephalopod eye, even though the two structures have no common ancestor.

The family has misfits on both sides. \mention{How} carries all the structural properties~-- it fronts, binds gaps, takes scope~-- but lacks the \mention{wh}-phonology. It's inside the operational category but outside the phonological family. Conversely, \mention{whether} has the \mention{wh}-phonology and appears clause-initially, but it introduces a yes/no alternative rather than binding a constituent gap~-- it shares the positional profile but not the dependency structure. It's inside the phonological family but at the edge of the operational category. The misfits confirm that the phonological label and the structural category are distinct: they mostly overlap but come apart at the edges, exactly as pronoun and pro-form do.

The operational signature just described~-- fronting, gap-binding, scope~-- is English-specific. Cross-linguistically, the same functional family is realized through different mechanisms: scope particles and clause-typing morphology in wh-in-situ languages, nominalization strategies for relativisation, complementisers rather than relative pronouns. The cross-linguistic generalization isn't fronting but a restricted set of items that systematically participate in operator-like dependency formation and clause-typing. English fronting is one realization of that broader operational niche~-- the one that happens to be most visible in the language this book draws on most often.

A third strand runs through both. The IRE operations~-- fronting, scope, prosodic focus~-- are syntactic mechanisms, but what they implement are discourse functions: information packaging, speech-act typing, focus management. Fronting a \mention{wh}-word doesn't just change the syntax; it restructures the information flow, marking a constituent as the locus of the hearer's attention. The operational category is syntactically maintained but discourse-motivated.

Personal pronouns serve a different discourse function: reference tracking. Anaphoric \mention{she} maintains a referent across utterances; deictic \mention{this} anchors one to the speech situation; \mention{who} in a relative construction binds a referent to an antecedent across a clause boundary. These are discourse operations too, but different ones~-- tracking rather than packaging~-- and they're maintained by lexical-category mechanisms (the pronoun paradigm) rather than operational ones. The discourse strand braids through both the lexical category and the operational category without being reducible to either.

The near-identity of the interrogative and relative extensions confirms the analysis. Strip away the semantics and the two families have almost the same membership: \mention{who}, \mention{which}, \mention{what}, \mention{where}, \mention{when}, \mention{how}. The overlap reflects shared dependency and clause-typing pressures~-- operator-like participation in interrogation and relativisation, however those pressures are realised in a given grammar~-- that pull items into partly the same distributional space regardless of whether the function is questioning or relativizing. And exclamatives add a third strand: \mention{how tall she is!}, \mention{what a mess!}~-- two of the same forms, fronted again, but serving neither questioning nor relativizing, and including only a small subset of the family. Three functions, partially overlapping membership, partially shared mechanisms. A braid, not a monolith.

The braid's development confirms its architecture. Children don't acquire the strands together. Deictic items (\mention{there}, \mention{this}) appear in the holophrase stage~-- before two-word combinations, before anything resembling syntax \autocite{clark1978,diessel1999}. The child points; the speech situation saturates the reference. Interrogative \mention{wh}-forms emerge later, through formulaic frames: \mention{what's that?} as a stored chunk before \mention{what} is extracted as a general-purpose restrictor \autocite{tomasello2003,rowlandpine2000}. \mention{Who} in relative constructions comes latest, because binding demands complex syntax~-- a gap, an antecedent, a dependency spanning clause boundaries \autocite{diessel2004}. The acquisition order recapitulates the diachronic order: the demonstrative lineage's items first, the interrogative lineage's items second, the grammaticalized relative forms last.

The HPC account predicts, further, that mastering one strand shouldn't bootstrap the other. A child who has acquired the personal-pronoun paradigm~-- case, person, number~-- shouldn't thereby acquire fronting and gap-binding, because those are maintained by operational mechanisms, not paradigmatic ones. Conversely, a child who has learned \mention{what's that?} as an interrogative chunk shouldn't thereby gain access to the pronoun case system, because the operational frame doesn't carry paradigmatic structure. The strands are independently acquired because they are independently maintained. The developmental evidence is consistent: children acquire deictic reference, interrogative frames, and relative-clause binding on separate timelines, with limited evidence of cross-strand bootstrapping \autocite{diessel2004}.

This territory, then, is where lexical, semantic, and operational categories overlap, with discourse pressure threading through all three. The lexicon supplies paradigms; paradigms stabilise distributions; distributions enable operations; operations reshape discourse; discourse, in turn, recruits the lexicon.

\begin{table}[htbp]
\centering
\caption{Three types of category in the pronoun territory, three maintenance profiles. \checkmark~= active; (\checkmark)~= partial; $\times$~= absent.}
\label{tab:11:category-types}
\small
\begin{tabular}{@{}lccc@{}}
\toprule
& Pronoun (lexical) & Pro-form (semantic) & IRE (operational) \\
\midrule
Morphological paradigm & \checkmark & $\times$ & $\times$ \\
Syntactic position & \checkmark & $\times$ & (\checkmark) \\
Phonological cohort & $\times$ & $\times$ & (\checkmark) \\
Semantic recruitment & $\times$ & \checkmark & $\times$ \\
Discourse function & (\checkmark) & \checkmark & \checkmark \\
Operational constraints & $\times$ & $\times$ & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/lyrebird.jpg}
  \caption{A male Superb Lyrebird (\textit{Menura novaehollandiae}) in full display. A single structure~-- the tail~-- serves the avian body plan, the biomechanics of courtship posture, and sexual signalling: three maintenance regimes, one anatomy. Nature is comfortable with multi-use parts; taxonomies tend to be less so. (Photo by Fir0002, CC BY-SA~3.0)}
  \label{fig:11:lyrebird}
\end{figure}

The lexical category (pronoun) is maintained by paradigmatic mechanisms~-- case morphology, distributional entrenchment, inventory closure~-- and serves the discourse function of reference tracking. The operational category (IRE family) is maintained by phonology and operational mechanisms~-- fronting, gap-binding, scope~-- and serves the discourse functions of information packaging and speech-act typing. The semantic concept (pro-form) is maintained by discourse pressure and semantic recruitment, not by morphosyntax at all.

\mention{Who} sits at the intersection~-- three maintenance regimes converging on a single item, the way a lyrebird's tail (Figure~\ref{fig:11:lyrebird}) is simultaneously (i)~part of a conserved body plan, (ii)~constrained by the biomechanics of a stereotyped display posture, and (iii)~recruited into a signalling economy that selects for salience. The traditional taxonomy, which has only one axis~-- lexical category~-- can't represent this overlap. It has to decide whether \mention{who} is \enquote{really} a pronoun or \enquote{really} an IRE word. The HPC framework doesn't face the dilemma. The pronoun properties are maintained by pronoun mechanisms; the IRE properties by IRE mechanisms; the discourse functions by discourse pressure. One item, three types of category, three strands of maintenance, operating on different planes.


\section{Looking forward}
\label{sec:11:transition}

The dissection is complete. Table~\ref{tab:11:mechanisms} makes the gradient operational.

\begin{table}[htbp]
\centering
\caption{Maintenance mechanisms by group. \checkmark~= active; (\checkmark)~= partial or borrowed; $\times$~= absent. The \mention{adverb} column scores the label as a whole, not any sub-group.}
\label{tab:11:mechanisms}
\small
\begin{tabular}{@{}lcccc@{}}
\toprule
& N / V & Adjective & Manner adv. & \mention{Adverb} \\
\midrule
Dedicated morphology & \checkmark & (\checkmark) & (\checkmark) & $\times$ \\
Agreement ecosystem & \checkmark & (\checkmark) & $\times$ & $\times$ \\
Exclusive syntactic slot & \checkmark & (\checkmark) & (\checkmark) & $\times$ \\
Early acquisition & \checkmark & $\times$ & $\times$ & $\times$ \\
Early predictive commitments & \checkmark & \checkmark & \checkmark & $\times$ \\
Diachronic stability & \checkmark & (\checkmark) & (\checkmark) & $\times$ \\
\bottomrule
\end{tabular}
\end{table}

A caveat before the count: a table with checkmarks looks like settled science. It isn't. A typology this tidy can install joints as easily as it discovers them. The skeleton/plumage/wastebasket/braid labels are diagnostic shorthands, not explanations. Each earns its keep only to the extent that it makes independent predictions~-- about acquisition order, diachronic trajectories, processing signatures, and cross-linguistic distribution~-- that a rival classification wouldn't. The count isn't the point. Mechanisms aren't equal in strength; the checkmarks compress real variation. But the contrast is the point: nouns and verbs recruit nearly every available mechanism; adjectives recruit a subset, often borrowed; manner adverbs recruit a comparable subset through different channels; and the \mention{adverb} label recruits nothing, because there's no category-level coupling to maintain.

The HPC account and a classical definitional account come apart most clearly on three diagnostics. A definitional account predicts relatively sharp, context-invariant boundaries once the definition is met; limited sensitivity to the ecological supports that surround a category; and relatively uniform acquisition once the defining cues are available. The maintenance view predicts graded membership where supports are partial; measurable plasticity in thin regions under short-term distributional perturbation (register shifts, contact, instruction); and super-additive gains in generalisation when multiple supports converge. If thin categories remain invariant under changes to their supposed supports, or if categories crystallise sharply without convergent supports, the maintenance story is overfitting.

Three predictions follow. First, where degree morphology and predicative/attributive alternations are both absent, \mention{adjective} should collapse into verb-like or noun-like strategies~-- and Dixon's gradient confirms this: the typological distribution of adjective classes tracks the availability of supporting mechanisms, not the functional importance of property concepts. Second, where a productive derivational pathway like English \mention{-ly} exists and positional constraints are stable, a manner-adverb cluster should thicken toward category status; where it doesn't, manner expression should disperse into serial verbs, ideophones, or verbal morphology. Third, where NP-internal agreement carries the gender and definiteness load, the pronominal system should thin in those dimensions~-- which is the transition to Chapter~\ref{ch:proform-gender}: English pro-form gender is prominent precisely because English lacks the NP-internal agreement that French and German use to carry the same distinctions.

The gradient has a second axis. Table~\ref{tab:11:mechanisms} ranks word classes by how many mechanisms converge~-- a one-dimensional measure. But the pronoun territory revealed that the same lexical space can be maintained by different types of mechanism operating on different planes. \mention{Who} isn't just a thick or thin category; it sits at the intersection of three: a lexical category maintained by paradigmatic mechanisms (case, closed inventory), an operational category maintained by structural constraints (fronting, gap-binding, scope), and a semantic concept maintained by discourse pressure (reference tracking, information packaging). The three are independently maintained and independently acquired. Mechanism density tells you how robust a category is. Category type tells you what kind of robustness it has~-- and why a taxonomy with room for only one axis keeps generating boundary disputes.

This gradient~-- tight, thin, fat, braided~-- makes the field-relative projectibility argument concrete. A morphologist, a syntactician, and a typologist looking at the same language will carve \mention{adjective} differently, because each tracks a different slice of the cluster. The morphologist asks whether a word shares the comparative paradigm; the syntactician asks whether it fills both attributive and predicative slots; the typologist asks whether the functional niche is filled by a dedicated class or by verbs doing double duty. Each cut projects for its own purposes. The debates that fill the typological literature~-- \enquote{does language X have adjectives?}~-- persist not because the question is unanswerable but because it has several answers, one per analytical purpose. Name and noun, one more time.

The next chapter takes the pro-form gender system~-- the narrowest basin in Part~III~-- and shows the coupling working in fine detail. After that, Chapter~\ref{ch:the-category-zipper} pulls the threads together, asking not how individual categories work but how the system of categories holds together~-- and, with Wallace in the background, asking of every resemblance the same question: what machinery makes it worth trusting?
