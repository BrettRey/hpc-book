% Chapter 7: The Stabilisers
% Complete rewrite 2025-12-08: biological explanatory style throughout

\chapter{The Stabilisers}
\label{ch:stabilisers}

\epigraph{The general rule would establish itself insensibly, and by slow degrees, in consequence of that love of analogy and similarity of sound, which is the foundation of by far the greater part of the rules of grammar.}{Adam Smith, \textit{Considerations Concerning the First Formation of Languages} (1761)}


\section{The cluster}
\label{sec:7:the-cluster}

Start where immunologists start: with the cluster.

When a biologist asks what a macrophage is, they don't look for a definition. They look for properties that tend to co-occur. A macrophage is picked out by a constellation: typical functions (phagocytosis, antigen presentation), marker profiles, morphologies, response tendencies, developmental origins. None of these is strictly necessary and sufficient. Some macrophages don't phagocytose; some share markers with dendritic cells; morphology varies with tissue context. But the properties cluster reliably enough to make the category useful -- to make predictions about new instances, to design experiments, to coordinate research programmes.

This is exactly the situation for grammatical categories.

Take nouns. What properties tend to co-occur? Items we call nouns typically take determinatives (\mention{the dog}, \mention{a problem}), pluralise (\mention{dogs}, \mention{problems}), function as heads of phrases in argument positions, refer to entities or entity-like abstractions, and enter into modification relations with adjectives. None of these is strictly necessary: \mention{cattle} doesn't pluralise; \mention{honesty} rarely takes \mention{a}; names resist determinatives in English; some nouns never appear in argument positions. And similar properties cluster under different labels in different languages -- Mandarin nouns don't pluralise the way English nouns do, and yet typologists recognise something noun-like in both systems.

The question is not: what is the essence of noun-ness? We tried that. It generated boundary disputes and competing definitions but no stable resolution.

The question is: what keeps the cluster clustered?


\section{Stabilisers at multiple scales}
\label{sec:7:stabilisers-at-scales}

When immunologists answer the analogous question for cell types, they don't point to a single cause. They narrate a multi-level story.

\textbf{Gene regulatory networks} produce relatively robust ``attractor'' states -- configurations the cell tends to settle into and return to after perturbation. These are not definitions; they are dynamical basins. The cell can be pushed around within a basin, but it takes a strong signal to tip it into a different one.

\textbf{Developmental lineage} constrains which basins are reachable. A cell's history matters: what precursor states it passed through, what signals it received at critical windows. The present state is not intelligible without the trajectory that led to it.

\textbf{Microenvironmental signalling} -- local cytokines, tissue context -- pushes cells into different regions of the same basin, producing variation within the type. A classically activated macrophage and an alternatively activated macrophage are both macrophages; the difference is which signals they've received and which region of the state space they occupy.

\textbf{Functional feedback}: what the cell does alters its microenvironment, which in turn sustains or shifts its state. The stabilisation is not one-way. The system is reciprocal.

Port this directly to language.

\textbf{Cognitive architectures and processing biases} are the analogue of gene regulatory networks. Some configurations of form-meaning pairings are easier to learn, store, and retrieve than others. Processing constraints produce attractor-like states: patterns that the system tends to fall into and return to. A construction that is frequent enough becomes entrenched -- retrieved as a unit rather than computed by rule. This is not definitional; it's dynamical.

\textbf{Acquisition pathways and learning biases} are the analogue of developmental lineage. A speaker's knowledge of English nouns was not installed by fiat; it was built up through exposure, starting with high-frequency exemplars and generalising outward. The order of acquisition matters. Early-learned patterns anchor later ones. Critical windows exist -- not strict Chomskyan critical periods, but sensitive phases where input has disproportionate effect.

\textbf{Discourse context and pragmatic ecology} are the analogue of microenvironmental signalling. A noun in subject position behaves differently from a noun in vocative position or in a compound. Same category, different activation state. The discourse environment pushes the token into a particular region of the distributional space. Variation is not noise to be explained away; it's expected, given the mechanisms.

\textbf{Functional feedback}: what speakers do with categories alters the input the next generation receives, which alters what categories stabilise. Usage entrenches representations; entrenched representations shape usage. The maintenance is not one-way. The system is reciprocal.

This is already a richer picture than \enquote{a noun is defined by such-and-such properties}. The definition tells you what the cluster looks like at a moment. The stabilising story tells you why it persists.


\section{Variation as activation states}
\label{sec:7:variation-activation-states}

The immunologist's key move: variation within a category is not embarrassing fuzziness to be explained away. It's the expected signature of a kind that is maintained by context-sensitive mechanisms.

A macrophage in inflamed tissue shows different markers than a macrophage in healthy tissue. Both are macrophages. The difference is activation state -- which signals the cell has received, which region of the phenotypic space it currently occupies. The category is real; the variation is real; both are explained by the stabilising dynamics.

Apply this to linguistic categories.

Consider the English word \mention{fun}. Is it a noun or an adjective? It takes determinatives like a noun (\mention{the fun we had}), but it also modifies nouns like an adjective (\mention{a fun party}) and resists pluralisation (*\mention{funs}). Textbook debates ask: which category is it really?

The immunologist's framing reinterprets the question. \mention{Fun} is a lexeme whose current state occupies a region of distributional space that overlaps two basins. It shows noun-like activation in some contexts, adjective-like activation in others. The category boundaries are not failed definitions; they are regions where multiple attractor states are close enough to produce mixed behaviour under different environmental signals.

This is not a dodge. It's a prediction: items in overlap regions should show higher inter-speaker variation, more sensitivity to discourse context, and faster historical change. And they do. \mention{Fun} is precisely the kind of word that shows register-dependent category behaviour and whose syntactic possibilities have shifted within living memory.

The same framing applies to aspect in Polish. As Chapter~\ref{ch:projectibility} showed, 90\% of verbs strongly prefer one aspect -- they're deep in a single basin. The remaining verbs occupy shallower regions where contextual signals matter more. The verbs that require explicit contextual cues to disambiguate are the ones where entrenchment is weaker, where the tense-frame signal has more work to do. That's the activation-state picture applied to grammatical aspect.


\section{One case in depth}
\label{sec:7:one-case}

Rather than enumerate mechanisms abstractly, let's trace the full stabilising story for one category: the English irregular past tense.

\subsection{The cluster}

What properties co-occur? Irregular verbs form past tenses by vowel change (\mention{sing/sang}, \mention{run/ran}), consonant change (\mention{teach/taught}), suppletion (\mention{go/went}), or zero derivation (\mention{cut/cut}). They're morphologically idiosyncratic. They're also, overwhelmingly, high-frequency. There are hundreds of regular verbs that could pattern irregularly but don't; there are very few irregular verbs that are low-frequency.

Why does this cluster persist?

\subsection{Stabilisers}

\textbf{Entrenchment.} High-frequency verbs are retrieved as units. \mention{Went} is stored and accessed directly, not computed from \mention{go} by rule. This retrieval-based processing is faster and more robust. It makes the irregular form resistant to analogical levelling.

\textbf{Acquisition dynamics.} Children overgeneralise: they say \mention{goed} before they settle on \mention{went}. What tips them toward the irregular form is input frequency. They hear \mention{went} often enough that it gets stored, overriding the analogical pull. Low-frequency irregulars -- \mention{holp}, \mention{wrought} -- don't get enough input to survive the pressure toward regularity.

\textbf{Community transmission.} Every generation learns from the previous one. Forms that are regular enough to be reliably reconstructed from the pattern survive transmission easily. Forms that are irregular but frequent enough to be directly stored also survive. Forms that are irregular and infrequent fall into a gap: they don't get relearned consistently, and they drift toward regularity. This is the regularisation bottleneck that Kirby's iterated learning models predict.

\textbf{Sociolinguistic maintenance.} Some irregular forms persist partly because they're unmarked variants in a prestige register. \mention{Dove} vs \mention{dived}, \mention{snuck} vs \mention{sneaked} -- neither is stigmatised, both are maintained. \mention{Holp} died partly because no community maintained it; there was no prestige or identity value attached to preserving it.

\textbf{Functional feedback.} The irregular forms that survive are used more; being used more, they're entrenched more deeply; being entrenched more deeply, they survive. The loop is self-reinforcing. High-frequency irregulars stabilise themselves.

\subsection{Variation and activation}

Not all irregular verbs are equally stable. \mention{Went} is deep in its basin -- no one says \mention{goed} past childhood. \mention{Dove/dived} occupies an overlap region: both forms are attested, both are used by competent speakers, and the choice can depend on register, region, or individual history. This is variation as activation state: the same category-membership, different current configuration depending on the environmental signals.

The prediction: verbs in overlap regions should show more inter-speaker variation, more sensitivity to discourse formality, and more historical flux. They do.

\subsection{What if a mechanism were absent?}

If entrenchment were the only stabiliser, we'd expect irregular persistence to correlate with frequency regardless of all other factors. We'd predict no regional or register variation, no sociolinguistic maintenance effects. We don't observe this.

If transmission bottlenecks were the only stabiliser, we'd expect regularisation to proceed at the same rate for all low-frequency irregulars. Instead we see that social prestige can slow or halt regularisation for particular forms.

If sociolinguistic maintenance were the only stabiliser, we'd expect that any form could persist indefinitely if a community valued it. But low-frequency forms regularise even when no stigma attaches to them -- there's not enough input to sustain them.

The observed pattern -- correlation with frequency, mediated by register and region, with slow leakage at low frequencies -- requires the full braid. No single mechanism is sufficient.


\section{How to test whether a mechanism is real}
\label{sec:7:robustness-tests}

The biological approach gives us a natural framework for testing mechanistic claims. Not hand-waving that \enquote{entrenchment maintains the category}, but operational tests that distinguish genuine causal structure from convenient labels.

\textbf{Learning transfer.} If entrenchment is real, speakers who learn aspect marking from some verbs should generalise to new verbs in predictable ways. This is exactly what the Polish models show: training on one subset, testing on another, the predictions transfer. Learning transfer is evidence that the category boundaries track causal structure, not just filing conventions.

\textbf{Intervention stability.} If frequency-based entrenchment stabilises irregular forms, then manipulating frequency should shift the pattern. We can't easily do this experimentally for established languages, but we see it in acquisition studies: children under-exposed to irregulars regularise more. We see it in contact situations: languages in intense contact show accelerated regularisation as input frequencies change.

\textbf{Cross-context generalisation.} If the category is maintained by stable mechanisms, predictions should hold across contexts -- different registers, different tasks, different populations. Labels without mechanisms should fragment: what works in careful speech should fail in casual speech; what works for Standard Polish should fail for dialectal Polish. The Polish aspect models show robustness across tense frames and lemma contexts. That's cross-context generalisation. That's mechanism.

These tests are what distinguish \enquote{I found a category} from \enquote{I found a mechanism that maintains a category}. The former is descriptive; the latter is explanatory.


\section{Degrees of projectibility}
\label{sec:7:degrees-projectibility}

The stabilising story explains why projectibility comes in degrees.

A category deep in a single basin -- maintained by entrenchment, transmission, functional pressure, and social reinforcement, all pulling in the same direction -- is strongly projectible. You can learn about nouns from a few exemplars and generalise reliably to new nouns. The mechanisms reinforce each other across timescales.

A category in an overlap region -- where mechanisms pull in different directions, or where entrenchment is weak -- is weakly projectible. Predictions work for typical cases but fail for edge cases. The degree of projectibility tracks the degree of mechanistic support.

A label with no mechanisms behind it is not projectible at all. A wastebasket category defined by what it's not, a traditional term inherited from earlier analyses, a filing convenience without causal grounding -- these should fragment under the robustness tests. They do.

This is the operational content of \enquote{mechanism-maintained kinds}. Not a metaphor. A measurable property: how strongly do predictions transfer across novel instances, contexts, and populations?


\section{What this commits us to}
\label{sec:7:commitments}

If this picture is right, we're committed to some substantive claims.

\textbf{Process ontology.} Categories are not static objects. They're dynamically sustained patterns -- standing waves, not sculptures. What exists is the stabilising process; the category is what the process makes legible.

\textbf{Interventionist realism.} Kinds are real to the extent that tracking or manipulating them changes expectations. This is stronger than description: it says that category distinctions track causal structure, not just organise files.

\textbf{Reciprocal realism.} Mechanisms and categories co-construct each other. Categories shape what gets entrenched; entrenchment shapes what categories survive. This is not a vicious circle; it's a self-organising dynamic. The same style of feedback that immunologists model for cell states applies to grammatical categories.

\textbf{Variation as signal.} Differences across contexts, speakers, and registers are not noise. They're diagnostic -- evidence about which region of the state space a token occupies, which activation state is active. The gradient nature of projectibility is a feature of maintenance, not a defect of the category.

\textbf{Cross-level coherence.} A category theory should deliver compatible predictions whether we analyse a phenomenon at the level of subpatterns, the construction, the category, or the wider system. If treating X as a bundle of micro-regularities yields different predictions than treating it as a member of a broader category, we've found something real: a heterogeneous grouping, a mislocated mechanism, or a label whose scope is historically inherited rather than causally grounded.

\textbf{Measurable metaphysics.} These claims have operational teeth. Entropy reduction, cross-context generalisation accuracy, inter-speaker agreement as a function of frequency and entrenchment -- these are not hand-waving gestures toward mechanism. They're measurable properties of stabilised categories.


\section{Redemption, not replacement}
\label{sec:7:redemption}

A word for the essentialists.

Nothing here says that traditional grammatical description was wrong. The definitions and classifications that fill reference grammars clustered the phenomena well enough that we can now ask: what maintains the clustering?

The definitional work was preparatory. It mapped the landscape. The mechanistic work explains why the landscape holds together -- why certain hills and valleys recur across languages, why some boundaries are sharp and others fuzzy, why predictions fail where they do and succeed where they do.

This is not \enquote{we've been wrong about categories}. It's \enquote{we finally have a way to explain why the categories that work keep working}.


\section{The most telling facts}
\label{sec:7:failure-modes-preview}

A grammatical category is not a thing you find; it's a regime you maintain. Arguments over definitions are, at bottom, arguments over stabilisers.

And the most telling facts about categories live in their failure modes -- where boundaries blur, where judgments diverge, where the stabilising dynamics show their seams.

But if categories are maintained, they can be undermaintained. The mechanisms can fail to cluster, or cluster too loosely, or cluster in ways that don't project.

The next chapter asks: how do we know when we don't have a kind?
