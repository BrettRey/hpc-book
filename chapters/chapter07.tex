% Chapter 7: The Stabilisers
% Restructured 2025-12-07: robustness criteria, level selection, worked case

\chapter{The Stabilisers}
\label{ch:stabilisers}

% SLOGAN: Mechanisms maintain the clustering that definitions merely describe.
% Sticky sentence: "A grammatical category is not a thing you find; it's a regime you maintain."

\epigraph{The general rule would establish itself insensibly, and by slow degrees, in consequence of that love of analogy and similarity of sound, which is the foundation of by far the greater part of the rules of grammar.}{Adam Smith, \textit{Considerations Concerning the First Formation of Languages} (1761)}


In Old English, \mention{pease} was a mass noun, like \mention{rice}. You could have much pease or little pease, but not three peases. Sometime in the seventeenth century, English speakers began treating \mention{pease} as a plural -- and invented a singular to match. \mention{Pea} never existed until the category demanded it. The word is a fossil of a mechanism: speakers maintaining a count/mass distinction by backforming what the paradigm seemed to require.

This is what it looks like when a grammatical category repairs itself. But what exactly is doing the repair?

Chapter~\ref{ch:projectibility} left the question hanging: if categories earn their keep by supporting induction, what exactly is doing the keeping? If talk of mechanisms sounds like a metaphysical upgrade too far, good -- the argument has to earn that upgrade by showing explanatory and predictive traction that definitional tidy-up can't deliver. The point is not that definitions are useless; they are often excellent summaries of what we already know. The point is that definitions don't explain stability. They don't tell us why a cluster of properties keeps reappearing across speakers, contexts, and generations, nor why certain generalisations remain a good bet even when boundary cases proliferate. The homeostatic picture shifts attention from classificatory checklists to maintenance regimes: the interacting feedback loops -- acquisition and entrenchment, analogy and alignment, discourse-functional pressures and institutional norms -- that make particular patterns resilient, transmissible, and epistemically useful. When a label projects, it's because these stabilisers keep the relevant similarities in circulation; when it doesn't, that failure is not a nuisance to be patched with a cleverer definition but a clue to where the maintenance regime is thin, conflicted, or absent.

This chapter does three things. First, it asks what kind of thing a mechanism is -- not to catalogue mechanisms but to provide criteria for identifying them. Second, it applies those criteria to one case in depth, showing how multiple stabilising processes braid together. Third, it spells out what accepting this picture commits us to.


\section{What kind of thing is a mechanism?}
\label{sec:7:what-is-mechanism}

Before we can identify mechanisms, we need to know what we're looking for. The word \term{mechanism} carries philosophical baggage: gears, levers, clockwork determinism. That's not what we mean.

A mechanism, in the sense relevant here, is an organised pattern of causal activity that produces or maintains a phenomenon. The key features:

\begin{itemize}
    \item \textbf{Organised}: not a random collection of causes but a structured arrangement where parts and activities are coordinated.
    \item \textbf{Causal}: actually doing something in the world, not merely a description or correlation.
    \item \textbf{Productive or maintaining}: either bringing something about or keeping it in place over time.
\end{itemize}

This is the standard view in philosophy of science \citep{machamer2000thinking, bechtel2005explanation, craver2007explaining}. What makes it useful for linguistics is that mechanisms can be identified at multiple levels -- neural, cognitive, social, institutional -- without requiring reduction to any single substrate. A mechanism can be a neural circuit, a learning dynamic, a social coordination pattern, or an institutional practice. What matters is that it does stabilising work. When mechanisms do maintenance work on categories, we'll call them \emph{stabilisers}.

Three clarifications prevent misunderstanding.

First, mechanisms are not essences in disguise. An essence is supposed to be necessary, sufficient, and intrinsic to category members. A mechanism is none of these. It's a process that operates on and among category members, not a hidden property lurking inside them. The category \term{noun} doesn't have a mechanism the way a clock has a spring; rather, noun-ness is maintained by mechanisms the way a riverbed is maintained by the water flowing through it.

Second, mechanisms can themselves be HPC kinds. The mechanism-type \term{entrenchment} doesn't have a sharp necessary-and-sufficient definition. It's a cluster of causal features -- frequency effects, processing facilitation, resistance to analogical levelling -- that tend to co-occur across contexts. Mechanism kinds are just as fuzzy at their boundaries as the categories they maintain. This recursion isn't a problem; it's the theory showing its depth.

Third, a single category can be maintained by multiple mechanisms, none of which is individually necessary. This is \term{multi-mechanism realism}: the stabilising story is typically a braid of contributory causal strands rather than a single master gear. Polish aspect, as we saw in Chapter~\ref{ch:projectibility}, is maintained by lexeme-specific entrenchment, tense-conditioned expectations, and prefixal cueing -- not by the semantic definition that textbooks provide.


\section{How to identify a stabiliser}
\label{sec:7:identifying-stabilisers}

The world is full of causal processes. Not all of them are stabilisers. How do we tell which ones are doing the work?

\subsection{Level selection is question-relative}
\label{sec:7:level-selection}

The first principle: you choose a level of description because it answers your question, not because it flatters your category.

A pedicab can be explained by engineering (frame geometry, gear ratios) or by physics (forces, friction, momentum). Both levels are real; both describe genuine causal structure. Your question selects the level. But the causal structure at that level determines whether your answer is correct. Interest picks the question; it doesn't fabricate the mechanism that answers it.

Consider the English irregular past tense. Why does \mention{went} persist while \mention{holp} (past of \mention{help}) disappeared?

At the level of individual processing, the answer involves entrenchment: \mention{went} is so frequent that its irregular form is over-learned, retrieved as a unit rather than computed by rule. At the level of acquisition, the answer involves the interaction between exemplar storage and analogical pressure: children initially say \mention{goed}, then acquire \mention{went} through exposure, but low-frequency irregulars like \mention{holp} don't get enough exposure to survive the pressure toward \mention{helped}. At the level of community transmission, the answer involves what Kirby's iterated learning models predict: irregularity survives only when frequency is high enough to beat the regularisation bottleneck. At the level of sociolinguistics, the answer involves prestige and standardisation: \mention{dove} vs \mention{dived} can persist as variants because neither is stigmatised; \mention{holp} died partly because no community maintained it.

These aren't competing answers. They're compatible answers at different timescales and grain sizes. Each level has genuine causal structure. None is the ``real'' mechanism with the others mere epiphenomena.

But -- and this is the critical point -- you can't pick a level arbitrarily. If someone claims that \mention{went} persists because of ``verb essence'' or ``the deep structure of English'', they've chosen a level that doesn't do any causal work. The question isn't answered by invoking a label; it's answered by mechanisms that can be tested. Does the alleged essence predict which irregulars survive? Can we intervene on the alleged deep structure and observe the predicted effect? If not, the level is decorative, not explanatory.

What's illegitimate is choosing a level because it makes your favourite category look real. If you claim that \term{strong verb} is a natural kind, you need mechanisms that actually stabilise strong-verb-ness across speakers and generations -- frequency patterns, paradigm pressure, community maintenance. If the mechanisms point elsewhere -- if high-frequency irregulars survive regardless of their historical verb class -- then \term{strong verb} is a historical label, not a stabilised kind.

\subsection{Robustness criteria}
\label{sec:7:robustness-criteria}

The second principle: a genuine stabiliser, as opposed to a convenient label, should pass robustness tests.

Three tests matter:

\textbf{Learning transfer.} Train on one subset, test on another. If speakers who learn the category from some exemplars can generalise to new exemplars, the mechanism is doing real work. If generalisation fails, the \enquote{category} is just a list.

\textbf{Intervention stability.} Perturb the mechanism and observe whether the pattern shifts as predicted. If entrenchment stabilises a form, then reducing exposure should weaken the pattern. If alignment coordinates speakers, then isolating them should produce drift. Stability under perturbation -- and predictable instability when the mechanism is disrupted -- is the mark of causal reality.

\textbf{Cross-context generalisation.} Apply the category to a new context -- different register, different genre, different task. Does the generalisation hold? Labels without mechanisms should fragment under these pressures; mechanisms should persist.

This is constrained realism. It's not permissive (\enquote{whatever predicts is real}). It's constrained by stability under perturbation. The Polish aspect results from Chapter~\ref{ch:projectibility} illustrate exactly this: lemma-concrete models transfer to unseen verbs; cue-outcome associations generalise across tense frames. That's robustness. That's mechanism.


\section{One case: Polish aspect}
\label{sec:7:polish-aspect-case}

Rather than enumerate mechanisms -- a list that would inevitably be incomplete, and that earlier chapters have already sketched -- let's watch the stabilisers work together on one case we already know.

Polish aspect, as we saw, resists textbook definition but rewards prediction. The perfectivity of a given verb-in-context can be predicted with 90\% accuracy from distributional cues. What mechanisms make this possible?

\subsection{The braid}

\textbf{Lexeme-specific entrenchment.} Ninety percent of Polish verbs strongly prefer one aspect. This isn't arbitrary: high-frequency verbs have their aspectual preferences reinforced on every encounter. The imperfective \mention{robiÄ‡} `to do' appears so often in imperfective contexts that its aspect is over-learned, deeply grooved. Low-frequency verbs, or verbs that occur in varied contexts, have weaker entrenchment and show more contextual flexibility.

\textbf{Tense-frame conditioning.} Past-tense frames predict perfective; present-tense frames predict imperfective. This isn't a rule speakers consciously apply; it's a cue-outcome association built from exposure. The mechanism is statistical learning: track what predicts what, weight cues by reliability, update on every encounter.

\textbf{Prefixal cueing.} Certain prefixes strongly bias toward perfectivity. This is partly morphological (prefix = perfectiviser) and partly distributional (prefixed forms cluster in certain contexts). The mechanism here is pattern-based: the learner extracts not just word-level associations but morpheme-level associations.

\textbf{Frequency effects across the paradigm.} The overall frequency of a verb affects how tightly entrenched its properties are. High-frequency verbs resist analogical levelling; low-frequency verbs are more labile. This interacts with aspect: a low-frequency verb in an aspectual grey zone will drift more readily.

\textbf{Discourse function.} Perfective aspect is recruited for foregrounded events; imperfective for backgrounding. This functional correlation biases what forms appear in what positions, reinforcing the statistical patterns that learners extract.

\subsection{Interactions}

None of these mechanisms is sufficient alone. A verb with strong lexeme-specific entrenchment but weak tense-frame conditioning would behave differently from one with the opposite profile. The prediction accuracy comes from the \emph{combination} -- the mechanisms pulling in the same direction.

And when they don't -- when entrenchment says one thing and discourse function says another -- we get the 10\% of verbs that are genuinely aspectually flexible. Those are the cases where the stabilising braid is looser, where context has to do more work, where speakers show more variation.

\subsection{What if one mechanism were absent?}

If entrenchment were the only stabiliser, we'd expect aspect to correlate with frequency but not with tense-frame or discourse position. We'd expect low-frequency verbs to behave randomly. We don't observe this.

If tense-frame conditioning were the only stabiliser, we'd expect aspect to be fully predictable from tense, with no lexeme-specific variation. The 90\% figure would be higher or lower depending on how clean the tense-aspect correlation is. We don't observe this either.

The observed pattern -- high accuracy with lexeme-specific variation, mediated by tense and discourse -- is what you get from multiple mechanisms braiding. The prediction follows.


\section{Degrees of projectibility}
\label{sec:7:degrees-of-projectibility}

If different categories have different mechanism profiles, they should differ in how strongly they project.

A category held by acquisition, entrenchment, alignment, transmission, and functional pressure -- all pulling in the same direction -- is strongly projectible. Learn about nouns from a few exemplars, and you can generalise to new nouns. The mechanisms reinforce each other across timescales.

A category held by one mechanism but abandoned by others is fragile in specific ways. A register-specific construction maintained mainly by institutional norms will persist in educated speech but erode in casual registers. A category maintained by acquisition but not by frequency effects will be sharp in child speech and fuzzy in adult speech.

A label with no mechanisms behind it -- a wastebasket category defined by what it's not, or a traditional term inherited from earlier analyses -- will fragment under the robustness tests. Train on one subset, test on another: no transfer. Perturb the context: no coherence. The label isn't wrong; it's mechanistically unsupported.

This is the framework's payoff. Projectibility comes in degrees because mechanism support comes in degrees. The degree of support is measurable: entropy reduction, cross-context generalisation accuracy, inter-speaker agreement as a function of frequency and entrenchment. These are the operational teeth of the framework.


\section{What this commits us to}
\label{sec:7:commitments}

Accepting this picture involves some commitments -- not just empirical bets but philosophical stances.

First, \textbf{process ontology}. A grammatical category is not a static object waiting to be discovered. It's a dynamically sustained pattern in a population-level causal field. Categories are more like standing waves than like sculpture: real, but real because something is actively maintaining them.

Second, \textbf{interventionist realism}. A kind is real to the extent that tracking or manipulating it reliably changes expectations about related properties. This is stronger than mere description: it says that the category distinctions we draw are tracking causal structure in the world, not just organising our files conveniently.

Third, \textbf{measurable metaphysics}. If categories differ in how strongly they're maintained, we should be able to measure that difference. Entropy reduction, cross-context generalisation accuracy, inter-speaker agreement as a function of frequency and entrenchment -- these are the operational teeth of the framework.

Fourth, \textbf{plural grounding}. The same category label across languages -- \term{noun}, \term{adjective} -- doesn't mean the same essence. It means similar maintenance packages producing similar projective behaviour. Crosslinguistic sameness is convergent maintenance, not shared constitution.

Fifth, \textbf{reciprocal realism}. Mechanisms aren't hidden beneath categories waiting to be discovered. They're partially sculpted by the categories they maintain. Learning shapes usage shapes learning. The maintenance is bidirectional.

Sixth, \textbf{cross-level coherence}. A category theory should deliver compatible predictions whether we analyse a phenomenon at the level of subpatterns, the construction, the category, or the wider system. If treating X as a bundle of micro-regularities yields different predictions than treating it as a member of a broader category, we've found something real: a heterogeneous or lumpy grouping, a mislocated mechanism, or a label whose scope is historically inherited rather than causally grounded. Failure of cross-level consistency isn't a nuisance -- it's a diagnostic.


\section{Redemption, not replacement}
\label{sec:7:redemption}

A word for the essentialists.

The definitional work wasn't wasted. It was the empirical base on which mechanistic explanation becomes possible. Definitions clustered the phenomena well enough that we can now ask: what maintains the clustering? Traditional grammarians weren't wrong. They were preparatory.

What changes is the explanatory aspiration. Definitions describe categories. Mechanisms explain why they hold together. The shift is from asking \enquote{what is a noun?} to asking \enquote{what keeps noun-ness coherent across speakers, contexts, and generations?} The answer is not a single thing. It is a conspiracy of mechanisms, each partial, together sufficient.

This also reframes old disputes. Where traditional theory argues over definitions -- what's the right characterisation of aspect, or of grammatical relations, or of the adjective/adverb boundary? -- the mechanism view asks: what causal story would make this category projectible? Many definitional disputes dissolve when reframed as stabiliser disputes. The combatants weren't wrong about the patterns; they were wrong about what kind of explanation those patterns required.


\section{What stabilisers are not}
\label{sec:7:what-stabilisers-are-not}

Three clarifications to prevent misapplication.

First, stabilisers are not excuses for hand-waving. Saying \enquote{entrenchment maintains the category} is not an explanation unless you can specify what entrenchment is, how it operates, and what predictions it makes. The framework is only as good as the stabilisers it identifies are measurable and manipulable.

Second, stabilisers are not deterministic. They bias outcomes; they don't guarantee them. A category can be well-maintained and still exhibit gradient membership, boundary disputes, and individual variation. The gradience isn't noise; it's the mechanisms operating with finite force on noisy input.

Third, stabilisers are not beneath categories in some foundational sense. They're alongside categories, interacting with them. Categories shape what gets entrenched; entrenchment shapes what categories stabilise. The picture is ecological, not architectural.


\bigskip

A grammatical category is not a thing you find; it's a regime you maintain. Arguments over definitions are, at bottom, arguments over stabilisers. And the most telling facts about categories live in their failure modes -- where boundaries blur, where judgments diverge, where the stabilising dynamics show their seams.

But if categories are maintained, they can be undermaintained. The mechanisms can fail to cluster, or cluster too loosely, or cluster in ways that don't project. The next chapter asks: how do we know when we don't have a kind?
