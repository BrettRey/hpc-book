\chapter{What changes}
\label{ch:what-changes}

% TODO: Write chapter content
% NOTE: Chapter 6 (sec:6:what-framework-offers) references this chapter for agent-based modelling
%       as a methodological consequence of the maintenance view. See notes/chapter-feedback-deferred.md
%       for planned ABM content: testing basin dynamics, boundary cases, convergence vs homology, etc.

\section{The status of the framework}

What \emph{kind} of kind is an \term{HPC category}? It is a \term{second-order explanatory kind}: a kind whose members are themselves kinds, unified not by shared first-order properties but by a shared explanatory role and stabilization pattern. Its members are things like \term{species}, \term{chemical elements}, \term{phonemes}, and \term{constructions}~-- categories that support projectible generalizations, are stabilized by multiple partially independent mechanisms, and tolerate property variation without collapse.

What makes \term{HPC category} itself a kind is that these properties cluster reliably across domains. It is not a natural kind in the same sense as \term{gold}, nor a merely stipulative classificatory label, nor a purely philosophical artifact. It is a meta-level homeostatic cluster over explanatory practices.

This recursion is \emph{typed}, not flat. Compare \term{gene} (a kind whose instances are molecular entities) with \term{functional gene} (a kind whose instances are ways of carving interactions for explanation), or \term{model organism} (a kind whose instances are organisms plus institutional practices). None of these collapses because they occupy different explanatory grains. Likewise, \term{noun} is an HPC kind in English; \term{lexical category} is an HPC kind across languages; and \term{HPC category} is an HPC kind across scientific taxonomies.

Crucially, the higher-order kind is not stabilized by the same mechanisms as its members. HPC categories \emph{in the world} are stabilized by causal mechanisms, developmental pathways, and communicative coordination. The \term{HPC category} kind is stabilized by repeated success of explanation, methodological convergence across sciences, robustness under theory change, and the survival of the concept under critical scrutiny (against eliminativism or essentialism). Its homeostasis is epistemic and methodological rather than biological or physical.

This framing distinguishes the theory from a mere framework or model. Frameworks can be swapped without residue; HPC categories resist that. Once the pattern is identified~-- why strict definitions fail, why exceptions cluster~-- we can make reliable predictions about where gradience will appear, where sharp boundaries will re-emerge, and where eliminativist arguments will systematically overreach. That predictive success is exactly what licenses kindhood.

A natural objection is that if \term{HPC category} is itself an HPC kind, the theory is self-validating or circular. This would be true only if HPC theory claimed \emph{a priori} necessity. It does not. It makes a fallible empirical claim about the structure of successful scientific kinds. If HPC categories stopped supporting prediction, coordination, or explanation, the kind would dissolve~-- by its own lights. That is not circularity; it is reflexive risk.

\section{Overlap as principled}
\label{sec:15:overlap}

When physicists calculate the motion of a planet, they face a choice. They can treat the planet as a single coherent object, or as a vast aggregate of $10^{50}$ atoms. For the theory to be consistent, the answer must be the same either way.

Newton worried about this. As \textcite{barandes2024} notes, this mereological consistency constraint~-- the requirement that the laws work equally well for parts and for wholes~-- acts as a theoretical guardrail. It forces the inclusion of Newton's Third Law: for every internal action between particles, there must be an equal and opposite reaction. Without this, the internal forces wouldn't cancel out, and a composite object could accelerate itself just by the interaction of its parts. Mereology constrains physics.

Linguistics has no such guardrail.

Instead, we have a loose federalism. We divide the field into sub-disciplines~-- phonetics, phonology, morphology, syntax, semantics, pragmatics~-- and treat them as distinct territories. We talk about \enquote{interfaces} (the syntax--semantics interface, the phonology--morphology interface) as if they were national borders to be policed rather than contact zones to be mapped. We assume a tree-like structure: a phenomenon belongs to syntax \emph{or} semantics, but ideally not both.

The HPC framework suggests this picture is wrong.

If linguistic categories are homeostatic property clusters, then disciplinary subfields are too. \term{Psycholinguistics}, \term{Construction Grammar}, and \term{Experimental Semantics} are not mutually exclusive territories. They are bundles of phenomena, methods, and theoretical commitments, stabilized by institutional mechanisms (journals, hiring lines, training pipelines). And crucially, these bundles overlap.

\subsection{Typed parthood}

The mistake is assuming that parthood is a single relation. In standard hierarchies, if $A$ is part of $B$, it's part of $B$ generally. But in scientific practice, parthood is \term{typed}.

A subfield can be part of linguistics in different ways:
\begin{itemize}
    \item \textbf{Phenomenon-part ($\leq_\textsc{phen}$)}: Its domain of inquiry is a subset of language (e.g., phonetics studies speech sounds).
    \item \textbf{Method-part ($\leq_\textsc{meth}$)}: Its tools are a subset of the field's toolkit (e.g., corpus linguistics uses distributional analysis).
    \item \textbf{Theory-part ($\leq_\textsc{thy}$)}: Its explanatory commitments inherit from a broader framework (e.g., Minimalism is a theoretical part of Generative Grammar).
\end{itemize}

Once we distinguish these types, the \enquote{boundary disputes} resolve into structural features.

Take \term{Computational Linguistics}. Is it part of linguistics? Methodologically, yes: it contributes formal and algorithmic tools. Theoretically, often no: its goals (engineering performance) diverge from the core explanatory project (cognitive realism). It is a method-part that is not always a theory-part.

Take \term{Construction Grammar}. It is a theoretical framework ($\leq_\textsc{thy}$) that claims all of morphosyntax as its phenomenon-domain ($\leq_\textsc{phen}$). This puts it in direct competition with Minimalism for the same territory.

Take the \term{syntax--semantics interface}. In the tree view, this is a border. In the typed-parthood view, it is a zone of \term{principled overlap}. Many linguistic phenomena cluster here because the mechanisms are bidirectional. Syntactic structure cues semantic interpretation (comprehension); semantic intent licenses syntactic choice (production). The two systems maintain each other. The overlap is not a messy intermediate zone to be purified; it is the engine of the system.

\subsection{Consequences for practice}

This re-framing has concrete consequences for how we work.

First, it explains \textbf{peer review friction}. When a paper on \enquote{experimental syntax} is reviewed, it often faces a double bind. Syntacticians (judging by $\leq_\textsc{thy}$) may find the theoretical contribution thin. Psycholinguists (judging by $\leq_\textsc{meth}$) may find the experimental design naive. The paper is attempting a fusion of two bundles. The friction isn't just grumpiness; it's a clash of validation criteria. Recognizing typed parthood allows editors to assign reviewers who are competent in the specific intersection being claimed.

Second, it validates \textbf{methodological pluralism}. If subfields are valid HPCs, then \enquote{pure} linguistics is just one bundle among many~-- usually the bundle that privileges introspective data and structural economy. Other bundles (corpus linguistics, sociolinguistics) privilege different stabilizers (usage data, social variation). These aren't failed attempts at pure linguistics; they are different cuts through the same multidimensional reality.

Finally, it recovers \textbf{intensional mereology}. Just as Newton asked what makes a planet a single object (internal forces cancelling out), we can ask what makes a subfield a genuine whole. Why does \term{Sociophonetics} exist as a stable cluster, while \term{Generative Phonetics} never quite cohered? Because the mechanisms aligned. Social variation turns out to be deeply entangled with phonetic detail (as we saw in Chapter~\ref{ch:register}). The phenomena cluster naturally.

The HPC framework doesn't just categorize our data. It categorizes us. And it suggests that the messy, overlapping map of modern linguistics is not a sign of immaturity, but an accurate reflection of a system where everything is braided.

\section{No level privilege}
\label{sec:15:no-level-privilege}

I don't assume that linguistic kinds must be grounded at a uniquely fundamental level of description. On a homeostatic property cluster view, the standing of a category is not conferred by its position in a hierarchy~-- phonetic, phonological, morphosyntactic, semantic, discourse~-- but by whether it supports projectible generalizations and whether there is a plausible account of the stabilizers that keep its properties clustered. Different explanatory projects may legitimately treat different grains as locally foundational, and those choices can be evaluated empirically.

This pluralism is constrained. Some proposed categories correspond to robust clusters sustained by recognizable mechanisms; others are weak, local, or artefactual. The point is not that any category is as good as any other, but that no level is privileged in advance. The \term{meta-Occam} principle from Chapter~\ref{ch:kinds-without-essences} applies here: we should expect parsimony in the stabilizers, not in the categories. Linguists can stop expecting tidy category definitions and start expecting tidy mechanism inventories.

\subsection{What earns foundational status}

A category is a better candidate for kindhood~-- and a better foundation for a local explanatory project~-- when it shows more of the following:

\begin{enumerate}
    \item \textbf{Cluster stability across contexts.} The cluster persists across speakers, registers, tasks, and modest perturbations in methodology.
    \item \textbf{Projectibility.} It supports reliable generalizations (including predictable failure modes) beyond the dataset that suggested it.
    \item \textbf{Mechanistic anchoring.} You can point to stabilizers~-- learning biases, articulatory or perceptual constraints, communicative pressures, institutional norms, processing limitations, interactional routines~-- that make the cluster non-accidental.
    \item \textbf{Cross-level consilience.} It's compatible with adjacent-level regularities without being reducible to them; tensions are diagnostically useful rather than merely inconsistent.
    \item \textbf{Intervention sensitivity.} Changing relevant conditions predictably shifts the cluster (even if the intervention is only observational or quasi-experimental).
    \item \textbf{Typological and diachronic tractability.} It supports comparative work without collapsing into stipulation.
\end{enumerate}

These criteria are graded, not binary. A category can score high on some dimensions and low on others. The framework doesn't issue licenses; it calibrates confidence.

\section{Conclusion: The zipper at scale}
\label{sec:15:zipper-at-scale}

We ended Chapter~\ref{ch:the-category-zipper} with the image of grammatical categories as zippers~-- mechanisms that couple distinct feature systems into functional alignments. The form-side teeth (morphosyntax) lock into the meaning-side teeth (semantics) not because they are perfectly identical, but because the coupling is tight enough to hold.

Disciplinary unity works the same way.

The "syntax--semantics interface" is not a line on a map. It is a zipper. Syntax involves one cluster of mechanisms (combinatorial efficiency, structural parsing); semantics involves another (compositionality, inference). They are autonomous systems, maintained by different pressures. But they are coupled. Communication forces them into alignment. The subfields that study them~-- and the theories we build~-- are attempts to describe that coupling.

If we treat \term{Syntax} and \term{Semantics} as essentialist territories with fixed borders, the mismatch will always look like a failure of theory. If we treat them as homeostatic property clusters, coupled by functional necessity, the mismatch is exactly what we expect. This integration relies on \term{compression}. As \textcite{nefdt2023} argues, following \textcite{dennett1991}, linguistic structures are \term{real patterns} because they are efficient compressions of data. Syntax doesn't need to access the full causal density of a semantic category; it only needs the compressed schema~-- the \term{value}. The zipper works because the teeth are simplified encodings, not raw complexity. The field of linguistics holds together for the same reason language does: not because it has a single essence, but because its parts are zipped together by the work they% The chapter ends here.