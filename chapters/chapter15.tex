\chapter{The Social Stabilization of Kinds}
\label{ch:social-stabilization}

The previous chapters have treated the \term{stabilizer}~-- the mechanism that keeps the cluster tight~-- as an internal property of the speaker. The detector clicks; the coupling holds; the category persists.

But this individualist picture misses the most obvious fact about human language: it is shared. If I equate \term{grammaticality} with a coupling in \emph{my} head, and you equate it with a coupling in \emph{yours}, what guarantee do we have that we are talking about the same thing? If mechanisms are internal, why are categories public?

Nothing in HPC theory requires that the stabilizing mechanism be wholly internal. Boyd's original account explicitly allows that homeostatic causation can include \emph{external} or environmental factors. This opens the door to social and cultural processes as homeostatic mechanisms~-- an idea that is HPC-compatible even if the biological examples didn't emphasize it. The question isn't whether social stabilization is possible, but how it works.

The answer lies in the messy reality of variation. In traditional formal theory, variation is often treated as noise~-- an irritating deviation from the pure signal of Competence. In the HPC framework, variation isn't noise. It is the signature of a mechanism working at a different scale.

\section{The signal in the noise}
\label{sec:15:signal-in-noise}

Consider the \term{double copula} construction, a staple of spontaneous Anglo-American speech:

\begin{exe}
\ex\label{ex:double-copula-narrative}
\begin{xlist}
\ex \mention{The thing is is that we need to leave.}
\ex \mention{The problem is is the money.}
\end{xlist}
\end{exe}

If you measure this construction against the \mention{Standard English} cluster~-- the one enforced by copy-editors and schoolteachers~-- it looks like an error. It appears to be a stutter, a planning failure, or a chaotic blending of two syntactic frames. Under this lens, it degrades the category of the copula. It is entropy.

But if you measure it in a corpus of spontaneous speech, especially in planning-heavy contexts, it looks like a robust, high-frequency attractor. It has stable properties: it occurs predominantly with \mention{reason}, \mention{problem}, and \mention{thing}; it projects (forms like \mention{The catch is is...} are predictable); and it is maintained by communicative efficiency, signaling a focus structure or a restart.

So is it \term{grammatical}?

The question is ill-posed because it treats \mention{English} as a single, flat population. If you pool all speakers and all contexts into one bucket, the double copula looks like a blurred edge~-- a 10\% probability event attached to the copula category. But the event isn't random. It is \term{conditioned}. In the context of spontaneous planning, its probability is high and stable. In the context of edited prose, its probability is near zero.

This suggests that the \term{kind} isn't \mention{English sentences}. The kind is \mention{English sentences in context X}. The social variable isn't an external \mention{influence} on the grammar; it is a parameter of the grammar itself.

\citet{Sperber1996} calls such stable equilibria \term{cultural attractors}. The puzzle he addresses is this: if each transmission of a cultural item (a story, a pronunciation, a grammatical pattern) involves imperfect imitation or deliberate modification, why doesn't the item drift beyond recognition? The answer is that not all variations are equally likely.

Shared cognitive biases systematically favor certain variants over others; over many transmission events, these biases cause variations to cancel out or converge rather than accumulate randomly. \emph{Little Red Riding Hood} has been retold with countless small deviations, yet it remains the same recognizable story: most tellers, facing an audience of children, avoid variants where everyone dies horribly and favor a moral resolution. The selective bias is a social-cognitive mechanism that attracts the tale toward a stable form.

Linguistic conventions work the same way. Pronunciation varies, but biases (ease of articulation, prestige, communicative clarity) pull variants toward a common attractor. Extreme deviations don't spread because they get corrected or fail to communicate. The double copula persists in spontaneous speech not because speakers are copying it perfectly, but because the communicative pressures in that context keep regenerating it.

\section{The Bolt Factory}
\label{sec:15:bolt-factory}

To understand why this conditioning matters for the metaphysics of categories, we need a statistical metaphor.

Imagine two factories making bolts. Factory A makes 5mm bolts. Factory B makes 10mm bolts. Both factories are highly precise ($SD = 0.1mm$). If you walk into Factory A and measure the output, you see a tight, homeostatic cluster. If you walk into Factory B, you see a similarly tight cluster.

But suppose someone takes the bins from both factories, dumps them into a single pile, and asks you to measure the \mention{Bolt} category.

You measure the pile. You calculate the mean: 7.5mm. You calculate the standard deviation: massive. You look at the distribution and see a wide, messy bimodal spread. You conclude that bolt manufacturing is a high-variance process and that the \mention{Bolt} category is loose and ill-defined.

You would be wrong. The process is low-variance; you just failed to condition on the source.

Linguistic variation is a latent mixture problem. When we say that subject--verb agreement is variable in \mention{English} because we see \mention{There's two men} alongside \mention{There are two men}, we are looking at the mixed bin.
Under standard formal conditions ($V_{standard}$), agreement is categorical.
Under casual spoken conditions ($V_{casual}$), \mention{there's} acts as an invariant presentative particle.

Both systems are tight. Both systems are homeostatic. The apparent looseness of the category comes from summing them together.

The \term{unconditioned probability} is the messy sum. The \term{conditioned probability} is the tight coupling. Mechanisms of homeostasis operate at the conditioned level. We align to \emph{the variety}, not the language. \citet{haspelmath2018} makes a parallel point in typology: treating comparative labels as if they were language-particular categories commits the \term{general category fallacy} and erases the very structure we are trying to explain; cross-linguistic claims should target phenomena, not whole incommensurable systems.

In Bayesian terms, varieties aren't labels but conditioning structure. The object we actually learn is $P(\text{Form} \mid D, R, C)$, where dialect $D$ is a durable speaker-linked parameter, register $R$ is a situational reweighting, and discourse community $C$ is a latent mixture component. The apparent looseness lives in the marginal $P(\text{Form})$, the distribution you get only after averaging over those conditioning variables.

\section{Three mechanisms of social stabilization}
\label{sec:15:three-mechanisms}

If the \term{mixed bin} is the wrong place to look for homeostasis, how do speakers unmix the bin? They don't have labels on the bolts. They have to infer the source.

This inference isn't magic; it relies on three distinct mechanisms of social stabilization. In the technical literature, these often go by the names \term{Dialect}, \term{Register}, and \term{Discourse Community}. In the HPC framework, we can see them for what they are: stabilizer mechanisms that protect the cluster from drift. This mirrors \citet{khalidi2015three}'s taxonomy of social kinds, specifically his second kind: kinds that depend on attitudes toward the kind itself, but not toward each individual instance.

\citet{mallon2003} makes the philosophical case explicit. A human category~-- a gender, an occupation, a speech community~-- can become \term{projectable} (supporting reliable generalizations) when social mechanisms stabilize and entrench that category. The category comes to correspond to a homeostatic cluster not by coincidence, but because the social role provides a \emph{causal infrastructure} that makes members more similar to each other than to outsiders.

Think of professional violinists: they share certain hand callouses, a repertoire of Italian musical terms, typical hearing profiles, and cultural experiences like conservatory training. No single property is essential, but the cluster is real~-- maintained by auditions, training regimens, peer expectations. If someone lacks all these properties, they either won't be recognized as a violinist or will acquire them through socialization. The variation among violinists (one plays jazz, another only classical; one has perfect pitch, another doesn't) doesn't negate the kind; it's variation around a socially maintained cluster.

\subsection{Dialect: The factory setting}
\label{sec:15:dialect}

In sociolinguistics, a \term{dialect} is traditionally defined as \mention{variety according to the user}~-- the way a person speaks because of who they are, not what they're doing \citep{halliday1978}. Dialects are tied to region, ethnicity, class, or network history. They travel with the speaker across situations.

Labov's foundational insight was that dialect differences aren't random noise but \term{orderly heterogeneity}: systematic, structured variation that can be described with variable rules \citep{labov1972}. African American English, for instance, isn't \mention{broken Standard English} but a rule-governed system with its own phonological and morphosyntactic constraints. This was a major advance. But it left a puzzle: if dialects are systems, what makes them cohere? What keeps AAVE from drifting into something unrecognizable across generations?

The standard sociolinguistic answer invokes geography, social networks, and identity. But these are descriptions, not mechanisms. The HPC framework offers something stronger: dialects are \term{maintained kinds}. They cohere because transmission within communities, alignment during interaction, and identity-marking pressures keep regenerating the cluster. The dialect boundary isn't where the rules change; it's where the \emph{maintenance regime} changes.

Consider the dialect continuum problem. Dutch shades into German across the Rhine; Scandinavian varieties form a chain of mutual intelligibility. Where does one dialect end and another begin? Sociolinguists often conclude that boundaries are political or ideological~-- \enquote{a language is a dialect with an army and a navy.} HPC says something different: the boundaries are \emph{ontologically sharp} but \emph{epistemically inaccessible}. The mechanisms that maintain Dutch and German are genuinely distinct (different national media, different school systems, different prestige norms), so there is a real boundary. But because the boundary is maintained by distributed social processes rather than marked by a single diagnostic feature, we can't point to it with precision. The continuum is epistemic, not ontological.

Think of dialect as the \mention{Factory Setting}. When you meet a speaker from a specific region or network, you load a set of priors. You assume that their vowel space, their lexical inventory, and their basic morphosyntactic constraints will remain relatively stable across the interaction. This stabilization is crucial because it allows us to attribute variation to the \emph{conditioned source}~-- whether the \emph{person} or the \emph{situation}~-- rather than the \emph{system}. If a speaker consistently uses \mention{needs washed} (the Midland US pattern), I don't conclude that English syntax is collapsing; I conclude that this speaker has a different parameter setting. The construction remains stable because I have conditioned on the dialect.

\subsection{Register: The mode switch}
\label{sec:15:register}

Where dialect is \mention{variety according to user}, \term{register} is traditionally defined as \mention{variety according to use} \citep{halliday1978}. The same speaker shifts registers across situations: casual at breakfast, formal in a job interview, technical in a lab report. Halliday's framework analyzed this situational variation along three dimensions: \term{field} (what is being talked about), \term{tenor} (the social relationship between participants), and \term{mode} (whether communication is spoken, written, or mixed). Each dimension shapes the probabilities of linguistic choices.

Subsequent work confirmed that these situational shifts aren't random. \citet{biber1988} applied multidimensional analysis to large corpora and showed that texts cluster into recognizable register types along interpretable dimensions (\eg\ involved vs.\ informational, narrative vs.\ non-narrative). The clusters are real: academic prose genuinely differs from casual conversation in systematic, measurable ways. But what \emph{maintains} those clusters? Biber's methodology describes the patterns; it doesn't explain their stability.

The HPC framework offers an answer: registers are attractor basins maintained by social mechanisms. Think of register as the \mention{Mode Switch}. We don't change our underlying grammar when we walk into a lecture hall or log onto Twitter; we reweight the options. Formal writing is a \emph{stance}~-- a mode where we actively inhibit certain convenient forms (like the double copula or the \mention{get}-passive) and boost others (nominalizations). The inhibition isn't random; it's enforced by gatekeeping (editors, teachers, peer reviewers) and rewarded by social outcomes (credibility, publication, grades).

The \term{Academic Register} discussed in Chapter 13 is the prime example. It isn't a natural dialect acquired in childhood; it is a learned, inhibited stance maintained by explicit institutional mechanisms. When a student submits a paper with too many contractions, the red ink flows. When a job candidate gives a colloquial research talk, eyebrows rise. These responses are the homeostatic forces that keep academic prose tight around its centre. \citet{pullum2019-normativity} argues that grammar is a normative domain~-- grammars make claims about correctness and incorrectness~-- and that such normativity can be made precise without implying obligations or duties. They say: \enquote{In this room, we only use the nominalized subset.}

What about boundaries? A sceptic might say: \enquote{Registers blend into each other. There's no sharp line between 'formal' and 'informal'.} The HPC reply mirrors the dialect case. Variation exists \emph{within} each register basin~-- no two academic papers are identical. But the basins themselves are ontologically distinct because they are maintained by different mechanisms. Academic gatekeeping doesn't operate on Twitter; Twitter's norms of brevity and informality don't govern journal submissions. The boundary is where one maintenance regime ends and another begins. If that boundary is blurry to introspection, it's because we lack epistemic access to the full causal structure~-- not because the structure is absent.

\subsection{Discourse Community: The source attribution}
\label{sec:15:community}

In applied linguistics and rhetoric, a \term{discourse community} is a group whose members share communicative conventions: specialized vocabulary, preferred genres, implicit standards of argument. \citet{swales1990} identified discourse communities by their shared goals, participatory mechanisms, genres, specialized lexis, and threshold membership. Academic disciplines are paradigm cases: physicists, linguists, and literary critics each write differently, and those differences aren't random~-- they're maintained by disciplinary training, peer review, and citation practices.

Standard accounts describe these conventions as products of socialization. Newcomers learn what counts as a good argument in philosophy or a publishable paper in biology. But description isn't explanation. Why do discourse communities maintain coherence over decades, given constant turnover in membership and constant pressure from individual innovation? Socialization names the phenomenon; it doesn't identify the mechanism.

The HPC framework identifies the mechanism: discourse communities are the \emph{latent mixture components} in the statistical model of variation. They correspond to distinct clusters of speakers whose linguistic behavior is conditioned on shared standards. Membership isn't arbitrary; it's maintained by gatekeeping (journal acceptance, degree conferral, conference invitations) and by looping effects (successful participants internalize the norms and perpetuate them). The community is a coordination equilibrium: once established, deviation is costly and conformity is rewarded.

This acts as the category's immune system. When a new or weird form appears, the community doesn't necessarily widen the category to accommodate it. Instead, we often \emph{partition} the input. We say, \enquote{Oh, that's tech-speak,} or \enquote{That's a Reddit thing.}

By assigning the deviant form to a specific sub-community, we protect the general category from drift. The bolt doesn't ruin our definition of typical bolts because we mentally tag it as \mention{Factory B}. This source attribution allows English to host massive internal diversity without losing its structural coherence. The clusters stay tight because we keep the bins separate effectively.

Nunberg's history of \term{slurs} shows how such boundaries can be socially enforced into existence: the noun \mention{slur} as a label for derogative words only becomes widespread in Anglophone public discourse in the 1960s, tied to a changing civic-moral framework. He also stresses that \mention{slur} is a \term{thick term}, mixing categorization and attitude, so the label itself encodes stance. That is source attribution at the lexical level, and it makes the category real by making it policed by the community. \citep{nunberg2016slurs}

This is closely related to what \citet{hacking1999} calls \term{looping kinds}. Hacking observed that when people are classified in a certain way~-- diagnosed with a disorder, labeled as a type~-- the classification loops back to affect how they behave, how others treat them, and even how they self-identify. The label provides an intentional framework under which individuals start to act. This is a homeostatic feedback loop: the category causes conformity, and conformity reinforces the category.

For linguistic kinds, the loop is constant. When speakers are told (explicitly by teachers, implicitly by reactions) that a form is \mention{incorrect} or \mention{slangy} or \mention{formal}, they adjust. The category doesn't passively describe; it actively shapes. The messy variation that might exist initially gets channelled and pruned by the social response, producing what Hacking calls an \term{interactive kind}~-- in HPC terms, an extrinsically homeostatic cluster.

Recent work by \citet{Floyd2025} provides empirical backing for this specialized tracking. They find that \mention{Social Conventions}~-- including the processing of irony, indirect requests, and conversational implicatures~-- form a distinct cognitive factor. This aligns with \citet{oconnor2021-conventionality}'s information-theoretic measure of conventionality, which treats conventions as degrees of arbitrariness and is representation-dependent: communities stabilize specific arbitrary choices not because they are optimal, but because coordination problems demand shared solutions. The same machinery can also stabilize inequity; when bargaining games include social categories, discriminatory norms can be stable outcomes rather than deviations. \citep{oconnor2022contracts} In her summary, inequity emerges robustly once agents (i) recognize social categories, (ii) condition their behavior on them, and (iii) learn self-benefiting actions~-- minimal preconditions that are almost always met. \citep[14]{oconnor2022contracts}

\subsection{The conditioning structure}
\label{sec:15:conditioning-structure}

These three mechanisms aren't independent; they form a unified conditioning structure that unmixes the bin.

In probabilistic terms, each mechanism contributes a different kind of parameter to the prediction of linguistic form. First, dialect is a persistent parameterization~-- a speaker-level setting that remains stable across situations. When I condition on a speaker using \mention{needs washed}, I update my priors for that speaker's entire output. Second, register is situational conditioning~-- a context-level factor that reweights probabilities within a single interaction. When I condition on an academic lecture, I expect nominalizations and passive constructions. Third, discourse community is a mixture component~-- a latent variable that identifies the source. When I recognize a text as coming from medical professionals, I interpret technical jargon as standard rather than deviant.

The three operate at different timescales and different levels of abstraction, but they work together. A single utterance is conditioned simultaneously by who is speaking (dialect), what they're doing (register), and what community they're participating in (discourse community). The apparent chaos of \mention{English} resolves into an orderly set of conditioned probabilities~-- not one messy distribution, but many tight ones layered atop each other.

\subsection{Developmental grounding}
\label{sec:15:development}

This conditioning structure isn't just an analytical convenience for linguists; it is the ground truth of acquisition. \citet{wiese2023grammatical} argues that children don't begin by learning languages like German or Turkish. They learn \term{Communicative Situations} (\term{com-sits}).

In Wiese's \mention{free-range language} framework, the primary unit of acquisition is the com-sit: interactions with specific people (\eg\ \mention{talking to Dad}) in specific contexts (\eg\ \mention{breakfast table}). A child might learn \mention{doggie} in one com-sit and \mention{Wauwau} in another. Initially, these are just situation-bound variants. Over time, the child detects that certain bundles of features co-occur across specific sets of situations. The \mention{German} cluster and the \mention{English} cluster emerge from the bottom up, not because the child has a \term{Language Acquisition Device} dealing in named languages, but because the input is naturally partitioned by social routines.

This reverses the standard picture. We don't start with a language and then learn to vary it. We start with specific, highly conditioned systems (com-sits) and later project the abstract category of language as a social index over them. The structural coherence comes from the specific situations; the borders come from the ideology. This explains why the no privileged scale conclusion holds: the cognitive machinery is built to track \term{conditioned systems} at any scale where they stabilize, whether that's a dyadic routine between siblings or a formal register shared by millions. The mechanisms of homeostasis are scale-invariant because acquisition is com-sit dependent.

This is why the mixed bin problem from Section~\ref{sec:15:bolt-factory} doesn't doom us to noise. Speakers are remarkably good at inferring the conditioning structure. We hear an utterance and rapidly compute: \emph{who is this person? what situation are we in? what community norms apply?} That inference unmixes the bin, allowing the tight coupling of the conditioned category to emerge.

\section{Indexicality as the inverse function}
\label{sec:15:indexicality}

This framework gives us a rigorous definition of social meaning, or \term{indexicality}.

If \term{grammar} is the function that predicts form given a social context ($P(\text{Form} \mid \text{Social})$), then \term{social meaning} is simply the inverse function: predicting the social context given the form ($P(\text{Social} \mid \text{Form})$).

When you hear \mention{The thing is is...}, your brain runs the inverse calculation. It asks under which condition this form is probable. The answer comes back: \mention{Spontaneous planning}. You index the speaker as being in a specific cognitive state.

When you hear \mention{It is posited that...}, the inverse function points to \mention{Academic Stance}. You index the speaker as performing a specific social identity.

We are, as Clifford Geertz said, \enquote{meaning-seeking animals}. But as Chapter 13 argued, we seek different kinds of meaning at different levels. The social stabilization of kinds works because we are constantly running this inverse function, attributing every token to a source, a stance, or a setting.

\section{The dunes are real}
\label{sec:15:conclusion}

The Nominalist might look at this conditioned landscape and say: \enquote{See? There are no fixed categories, only shifting social sands. 'English' is a fiction; 'Grammaticality' is just a probability distribution that changes with the wind.}

The Realist replies: \enquote{No. The sand shifts, but the dunes are real.} As \citet{bach2016-social-kinds} argues, social categories like gender~-- and, we contend, grammar~-- are \term{natural kinds} even if they aren't mind-independent \term{objective types}. Their reality consists in their causal power and their projectibility within the social system.

By acknowledging that categories are parameterized by varieties, we don't make them less real. We make them \emph{more precise}. A biological species is defined relative to an ecosystem. A polar bear is a robust kind in the Arctic; it is a failed kind in the Sahara. \citet{khalidi2024historical} argues that many social kinds are historically individuated: membership depends on origin or trajectory (token or type), and some are \term{copied kinds}. Historical individuation can also enable inference to common causes and explanation of synchronic causal properties \citep{khalidi2024historical}. That gives us a natural way to treat varieties as historically stabilized kinds, not just synchronic bundles.

The biological analogy is deliberate: HPC theory was forged to make sense of kinds that are stable without essences, and species were the proving ground. \citep{boyd1991,boyd1999} Dialects are species-like in their stabilization dynamics, not in their individuation. Both are population-level clusters maintained by feedback processes, but dialects are socially policed, rapidly reweighted, and open to horizontal transfer; they don't track reproductive lineages. The analogy buys us vocabulary~-- drift, selection, niches, hybrid zones~-- while keeping mechanism talk separate from taxonomy talk.

Nominalism was a necessary corrective to essentialist overreach. But it threw out the baby with the bathwater. We can admit that categories are constructed~-- built by history, maintained by interaction, variable at the margins~-- without admitting they are arbitrary. \citet{oconnor2019games} shows with similarity-maximizing models that linguistic categories can be conventional even when they appear to respect property structure, and that when payoffs depend on other dimensions, terms needn't track pre-existing clusters. On this view, similarity is defined in payoff space, so the properties that count as natural are those relevant to the community's action needs \citep{oconnor2019games}. That is exactly the space in which \term{maintained kinds} live.

And because they are maintained by the necessity of coordination, they are real. Still, as \citet{oconnor2019methods} stresses, models constrain explanations but don't by themselves identify the psychological machinery; they need empirical triangulation.

Crucially, this implies there's no single correct scale of analysis. If the social game requires broad coordination (\eg\ \mention{International English} for trade), the \mention{Language} scale stabilizes. If it requires fine-grained identity signaling (\eg\ \mention{Academic Register}), the \mention{Stance} scale stabilizes. Realism isn't about finding the one true atomic level; it is about finding the level where the coordination game is actually being played.

A linguistic category is likewise defined relative to a social ecology. The \term{double copula} is a robust kind in spontaneous speech; it is an error in the pages of \emph{Science}. To study linguistic kinds without studying social varieties is like studying biology without ecology. You might understand the anatomy, but you will never understand why the animal survives.
