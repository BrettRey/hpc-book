\chapter{The Social Stabilization of Kinds}
\label{ch:social-stabilization}

Chapter~14 concluded with the observation that grammaticality functions as the immune system of the grammar. But an immune system isn't calibrated in a vacuum; it is calibrated to a community.

A student writes \mention{The thing is is that we can't afford it} in an essay. The instructor's red pen strikes. \enquote{Repetition,} says the margin note. \enquote{Delete one \mention{is}.} The student complies, loses a point, and moves on. The red pen is the immune system operating at the social scale.

But wait. In corpora of spontaneous speech, this \term{double copula} construction is robust. It isn't a stutter or a planning failure; it's predictable, frequent, and stable across millions of speakers. It clusters with focus-marking contexts and certain subject nouns (\mention{thing}, \mention{problem}, \mention{reason}). Speakers produce it; hearers understand it without difficulty; and yet~-- red ink.

Here's the puzzle. The same form is produced, understood, transmitted, and punished. Where's the stabilizer?

Chapter~7 introduced the mechanisms that keep categories tight: acquisition, entrenchment, interactive alignment, iterated transmission. Those mechanisms live in the speaker's head. They explain why \emph{you} maintain the cluster. But they don't explain why the cluster stays aligned across heads~-- why my \mention{noun} and your \mention{noun} pick out the same thing, why we both recognize the double copula even as one of us corrects it.

The answer is that stabilizers aren't only internal. They're also external: editors, teachers, peer groups, prestige norms, register ecologies. These social mechanisms don't replace the cognitive ones from Chapter~7; they supervise them. The red pen isn't inside the student's head, but it shapes what stays there.

This chapter widens the lens. Parts II and III showed how countability, definiteness, and lexical categories cohere despite variation. What we didn't yet explain is how they stay coherent \emph{across} speakers~-- why the categories remain public. The answer is social stabilization: a layer of homeostatic machinery that operates between minds, not just within them.

\section{The signal in the noise}
\label{sec:15:signal-in-noise}

In traditional formal theory, variation is often treated as noise~-- an irritating deviation from the pure signal of Competence. In the HPC framework, variation isn't noise. It is the signature of a mechanism working at a different scale.

Return to the double copula. If you measure it against the \mention{Standard English} cluster, it looks like an error:

\begin{exe}
\ex\label{ex:double-copula-narrative}
\begin{xlist}
\ex \mention{The thing is is that we need to leave.}
\ex \mention{The problem is is the money.}
\end{xlist}
\end{exe}

It appears to be a stutter, a planning failure, or a chaotic blending of two syntactic frames. Under this lens, it degrades the category of the copula. It is entropy.

But if you measure it in a corpus of spontaneous speech, especially in planning-heavy contexts, it looks like a robust, high-frequency attractor. It has stable properties: it occurs predominantly with \mention{reason}, \mention{problem}, and \mention{thing}; it projects (forms like \mention{The catch is is...} are predictable); and it is maintained by communicative efficiency, signaling a focus structure or a restart.

The construction isn't a slip. \textcite{coppock2018double} argues that \mention{The thing is, is...} isn't disfluency but a focus-marking device, with the second \mention{is} introducing a specificational clause. The Yale Grammatical Diversity Project documents its distribution across North American varieties: stable, predictable, and grammaticalized in specific discourse contexts.\footnote{See \url{https://ygdp.yale.edu/phenomena/double-is} for an overview.}

So is it \term{grammatical}?

The question is ill-posed because it treats \mention{English} as a single, flat population. If you pool all speakers and all contexts into one bucket, the double copula looks like a blurred edge~-- a 10\% probability event attached to the copula category. But the event isn't random. It is \term{conditioned}. In the context of spontaneous planning, its probability is high and stable. In the context of edited prose, its probability is near zero.

This suggests that the projectable regularities don't live at the level of \mention{English sentences} simpliciter. They live at the level of \mention{English sentences in context X}: conditioned systems whose distributions are stable because the same social mechanisms keep regenerating them. The social variable isn't an external \mention{influence} on the grammar; it is a parameter of the grammar itself.

\citet{sperber1996} calls such stable equilibria \term{cultural attractors}. The puzzle he addresses is this: if each transmission of a cultural item (a story, a pronunciation, a grammatical pattern) involves imperfect imitation or deliberate modification, why doesn't the item drift beyond recognition? The answer is that not all variations are equally likely.

Shared cognitive biases systematically favor certain variants over others; over many transmission events, these biases cause variations to cancel out or converge rather than accumulate randomly. \emph{Little Red Riding Hood} has been retold with countless small deviations, yet it remains the same recognizable story: most tellers, facing an audience of children, avoid variants where everyone dies horribly and favor a moral resolution. The selective bias is a social-cognitive mechanism that attracts the tale toward a stable form.

Linguistic conventions work the same way. Pronunciation varies, but biases (ease of articulation, prestige, communicative clarity) pull variants toward a common attractor. Extreme deviations don't spread because they get corrected or fail to communicate. The double copula persists in spontaneous speech not because speakers are copying it perfectly, but because the communicative pressures in that context keep regenerating it.

\section{The mixed bin problem}
\label{sec:15:mixed-bin}

To see why the double copula is puzzling, we need a statistical metaphor.

Imagine two factories making bolts. Factory A makes 5mm bolts. Factory B makes 10mm bolts. Both factories are highly precise ($SD = 0.1mm$). If you walk into Factory A and measure the output, you see a tight, homeostatic cluster. If you walk into Factory B, you see a similarly tight cluster.

But suppose someone takes the bins from both factories, dumps them into a single pile, and asks you to measure the \mention{Bolt} category.

You measure the pile. You calculate the mean: 7.5mm. You calculate the standard deviation: massive. You look at the distribution and see a wide, messy bimodal spread. You conclude that bolt manufacturing is a high-variance process and that the \mention{Bolt} category is loose and ill-defined.

You would be wrong. The process is low-variance; you just failed to condition on the source.

Linguistic variation is a latent mixture problem. When we say that subject--verb agreement is variable in \mention{English} because we see \mention{There's two men} alongside \mention{There are two men}, we are looking at the mixed bin.
Under standard formal conditions ($V_{standard}$), agreement is categorical.
Under casual spoken conditions ($V_{casual}$), \mention{there's} acts as an invariant presentative particle.
\textcite{krejciHilton2022} identify three variants in existential constructions: agreeing \mention{there are}, invariant \mention{there's}, and levelled \mention{there is}. The invariant \mention{there's} isn't agreement failure; it's a presentational particle with its own distributional profile~-- stable under casual spoken conditions, near-absent in edited prose.

Both systems are tight. Both systems are homeostatic. The apparent looseness of the category comes from summing them together.

The \term{unconditioned probability} is the messy sum. The \term{conditioned probability} is the tight coupling. Mechanisms of homeostasis operate at the conditioned level. We align to \emph{the variety}, not the language. \citet{haspelmath2018} makes a parallel point in typology: treating comparative labels as if they were language-particular categories commits the \term{general category fallacy} and erases the very structure we are trying to explain; cross-linguistic claims should target phenomena, not whole incommensurable systems.

In Bayesian terms, varieties aren't labels but conditioning structure. The object we actually learn is $P(\text{Form} \mid D, R, C)$, where dialect $D$ is a durable speaker-linked parameter, register $R$ is a situational reweighting, and discourse community $C$ is a latent mixture component. The apparent looseness lives in the marginal $P(\text{Form})$, the distribution you get only after averaging over those conditioning variables.

This is field-relative projectibility (Chapter~\ref{ch:projectibility}) made concrete. The conditioning variables~-- dialect, register, discourse community~-- are what make projectibility field-relative in practice. A form that projects reliably in casual speech may project unreliably if you pool formal and informal contexts. The field isn't a prior; it's a conditioning structure.

\section{Three mechanisms of social stabilization}
\label{sec:15:three-mechanisms}

If the \term{mixed bin} is the wrong place to look for homeostasis, how do speakers unmix the bin? (Section~\ref{sec:15:mixed-bin} explains the metaphor.) They don't have labels on the bolts. They have to infer the source.

This inference isn't magic; it relies on three distinct mechanisms of social stabilization. In the technical literature, these often go by the names \term{Dialect}, \term{Register}, and \term{Discourse Community}. In the HPC framework, we can see them for what they are: stabilizer mechanisms that protect the cluster from drift. This mirrors \citet{khalidi2015three}'s taxonomy of social kinds, specifically his second kind: kinds that depend on attitudes toward the kind itself, but not toward each individual instance.

\citet{mallon2003} makes the philosophical case explicit. A human category~-- a gender, an occupation, a speech community~-- can become \term{projectable} (supporting reliable generalizations) when social mechanisms stabilize and entrench that category. The category comes to correspond to a homeostatic cluster not by coincidence, but because the social role provides a \emph{causal infrastructure} that makes members more similar to each other than to outsiders.

Think of professional violinists: they share certain hand callouses, a repertoire of Italian musical terms, typical hearing profiles, and cultural experiences like conservatory training. No single property is essential, but the cluster is real~-- maintained by auditions, training regimens, peer expectations. If someone lacks all these properties, they either won't be recognized as a violinist or will acquire them through socialization. The variation among violinists (one plays jazz, another only classical; one has perfect pitch, another doesn't) doesn't negate the kind; it's variation around a socially maintained cluster.

\subsection{Dialect: The factory setting}
\label{sec:15:dialect}

In sociolinguistics, a \term{dialect} is traditionally defined as \mention{variety according to the user}~-- the way a person speaks because of who they are, not what they're doing \citep{halliday1978}. Dialects are tied to region, ethnicity, class, or network history. They travel with the speaker across situations.

Labov's foundational insight was that dialect differences aren't random noise but \term{orderly heterogeneity}: systematic, structured variation that can be described with variable rules \citep{labov1972}. African American English, for instance, isn't \mention{broken Standard English} but a rule-governed system with its own phonological and morphosyntactic constraints. This was a major advance. But it left a puzzle: if dialects are systems, what makes them cohere? What keeps AAVE from drifting into something unrecognizable across generations?

The standard sociolinguistic answer invokes geography, social networks, and identity. But these are descriptions, not mechanisms. Mechanisms are what HPC provides: dialects are \term{maintained kinds}. They cohere because transmission within communities, alignment during interaction, and identity-marking pressures keep regenerating the cluster. The dialect boundary isn't where the rules change; it's where the \emph{maintenance regime} changes.

Consider the dialect continuum problem. Dutch shades into German across the Rhine; Scandinavian varieties form a chain of mutual intelligibility. Where does one dialect end and another begin? Sociolinguists often conclude that boundaries are political or ideological~-- \enquote{a language is a dialect with an army and a navy.}

HPC says something different: maintenance regimes can be discrete even when diagnostics are gradient. The mechanisms that maintain Dutch and German are genuinely distinct (different national media, different school systems, different prestige norms), so there is a real boundary. But because the boundary is maintained by distributed social processes rather than marked by a single diagnostic feature, we can't point to it with precision. The continuum is epistemic, not ontological.

Think of dialect as a factory setting~-- a set of priors you load when you meet a speaker from a specific region. If a speaker consistently uses \mention{needs washed} (the Midland US pattern), I don't conclude that English syntax is collapsing; I conclude that this speaker has a different parameter setting. The construction remains stable because I have conditioned on the dialect.

The feedback loop: a speaker produces a dialectal variant; interlocutors either accommodate (reinforcing the variant) or fail to understand (penalizing it); network density and media exposure re-inject community norms; over months to years, the speaker's inventory shifts toward the network mode. The timescale is developmental: what you hear in formative years anchors what you produce as an adult.

\subsection{Register: The mode switch}
\label{sec:15:register}

Where dialect is \mention{variety according to user}, \term{register} is traditionally defined as \mention{variety according to use} \citep{halliday1978}. The same speaker shifts registers across situations: casual at breakfast, formal in a job interview, technical in a lab report. Halliday's framework analyzed this situational variation along three dimensions: \term{field} (what is being talked about), \term{tenor} (the social relationship between participants), and \term{mode} (whether communication is spoken, written, or mixed). Each dimension shapes the probabilities of linguistic choices.

Subsequent work confirmed that these situational shifts aren't random. \citet{biber1988} applied multidimensional analysis to large corpora and showed that texts cluster into recognizable register types along interpretable dimensions (\eg\ involved vs.\ informational, narrative vs.\ non-narrative). The clusters are real: academic prose genuinely differs from casual conversation in systematic, measurable ways. But what \emph{maintains} those clusters? Biber's methodology describes the patterns; it doesn't explain their stability.

The HPC framework offers an answer: registers are attractor basins maintained by social mechanisms. Register is a mode switch~-- we don't change our underlying grammar when we walk into a lecture hall; we reweight the options. Formal writing is a \emph{stance}~-- a mode where we actively inhibit certain convenient forms (like the double copula or the \mention{get}-passive) and boost others (nominalizations). The inhibition isn't random; it's enforced by gatekeeping (editors, teachers, peer reviewers) and rewarded by social outcomes (credibility, publication, grades).

The \term{Academic Register} discussed in Chapter 13 is the prime example. It isn't a natural dialect acquired in childhood; it is a learned, inhibited stance maintained by explicit institutional mechanisms. When a student submits a paper with too many contractions, the red ink flows. When a job candidate gives a colloquial research talk, eyebrows rise. These responses are the homeostatic forces that keep academic prose tight around its centre.

\citet{pullum2019-normativity} argues that grammar is a normative domain~-- grammars make claims about correctness and incorrectness~-- and that such normativity can be made precise without implying obligations or duties. They say: \enquote{In this room, we only use the nominalized subset.}

What about boundaries? A sceptic might say: \enquote{Registers blend into each other. There's no sharp line between 'formal' and 'informal'.} The HPC reply mirrors the dialect case. Variation exists \emph{within} each register basin~-- no two academic papers are identical. But the basins themselves are ontologically distinct because they are maintained by different mechanisms.

Academic gatekeeping doesn't operate on Twitter; Twitter's norms of brevity and informality don't govern journal submissions. The boundary is where one maintenance regime ends and another begins. If that boundary is blurry to introspection, it's because we lack epistemic access to the full causal structure~-- not because the structure is absent.

The feedback loop: a writer uses a casual form in formal prose; an editor marks it; the writer self-monitors on future attempts; genre models provide templates that reweight expectations. Over submission cycles and revision rounds, the register basin tightens. The timescale is institutional: weeks to months per cycle, cumulating over a career.

\subsection{Discourse Community: The source attribution}
\label{sec:15:community}

In applied linguistics and rhetoric, a \term{discourse community} is a group whose members share communicative conventions: specialized vocabulary, preferred genres, implicit standards of argument. \citet{swales1990} identified discourse communities by their shared goals, participatory mechanisms, genres, specialized lexis, and threshold membership. Academic disciplines are paradigm cases: physicists, linguists, and literary critics each write differently, and those differences aren't random~-- they're maintained by disciplinary training, peer review, and citation practices.

Standard accounts describe these conventions as products of socialization. Newcomers learn what counts as a good argument in philosophy or a publishable paper in biology. But description isn't explanation; explanation requires mechanism. Why do discourse communities maintain coherence over decades, given constant turnover in membership and constant pressure from individual innovation? Socialization names the phenomenon; it doesn't identify the mechanism.

The HPC framework identifies the mechanism: discourse communities are the \emph{latent mixture components} in the statistical model of variation. They correspond to distinct clusters of speakers whose linguistic behavior is conditioned on shared standards. Membership isn't arbitrary; it's maintained by gatekeeping (journal acceptance, degree conferral, conference invitations) and by looping effects (successful participants internalize the norms and perpetuate them). The community is a coordination equilibrium: once established, deviation is costly and conformity is rewarded.

The feedback loop: a newcomer uses unfamiliar argumentation; reviewers reject or request revision; apprenticeship provides explicit models; citation practices reward conformity. Over years of professionalization, the newcomer's rhetorical inventory converges on community norms. The timescale is generational for the community, years for the individual.

This acts as a category's immune system: when a weird form appears, we partition rather than widen, saying \enquote{That's tech-speak} or \enquote{That's a Reddit thing.}

By assigning the deviant form to a specific sub-community, we protect the general category from drift. The bolt doesn't ruin our definition of typical bolts because we mentally tag it as \mention{Factory B}. This source attribution allows English to host massive internal diversity without losing its structural coherence. The clusters stay tight because we keep the bins separate effectively.

Nunberg's history of \term{slurs} shows how such boundaries can be socially enforced into existence: the noun \mention{slur} as a label for derogative words only becomes widespread in Anglophone public discourse in the 1960s, tied to a changing civic-moral framework. He also stresses that \mention{slur} is a \term{thick term}, mixing categorization and attitude, so the label itself encodes stance. That is source attribution at the lexical level, and it makes the category real by making it policed by the community. \citep{nunberg2016slurs}

This is closely related to what \citet{hacking1999} calls \term{looping kinds}. Hacking observed that when people are classified in a certain way~-- diagnosed with a disorder, labeled as a type~-- the classification loops back to affect how they behave, how others treat them, and even how they self-identify. The label provides an intentional framework under which individuals start to act. This is a homeostatic feedback loop: the category causes conformity, and conformity reinforces the category.

For linguistic kinds, the loop is constant. When speakers are told (explicitly by teachers, implicitly by reactions) that a form is \mention{incorrect} or \mention{slangy} or \mention{formal}, they adjust. The category doesn't passively describe; it actively shapes. The messy variation that might exist initially gets channelled and pruned by the social response, producing what Hacking calls an \term{interactive kind}~-- in HPC terms, an extrinsically homeostatic cluster.

Recent work by \citet{Floyd2025} provides empirical backing for this specialized tracking. They find that \mention{Social Conventions}~-- including the processing of irony, indirect requests, and conversational implicatures~-- form a distinct cognitive factor. This aligns with \citet{oconnor2021-conventionality}'s information-theoretic measure of conventionality, which treats conventions as degrees of arbitrariness and is representation-dependent: communities stabilize specific arbitrary choices not because they are optimal, but because coordination problems demand shared solutions.

The same machinery can also stabilize inequity; when bargaining games include social categories, discriminatory norms can be stable outcomes rather than deviations. \citep{oconnor2022contracts} In her summary, inequity emerges robustly once agents (i) recognize social categories, (ii) condition their behavior on them, and (iii) learn self-benefiting actions~-- minimal preconditions that are almost always met. \citep[14]{oconnor2022contracts}

\section{Acquisition as source inference}
\label{sec:15:acquisition}

The three mechanisms we've outlined~-- dialect, register, and discourse community~-- aren't independent. They form a unified conditioning structure that unmixes the bin.

In probabilistic terms, each mechanism contributes a different kind of parameter to the prediction of linguistic form. Dialect is a persistent parameterization~-- a speaker-level setting that remains stable across situations. When I condition on a speaker using \mention{needs washed}, I update my priors for that speaker's entire output. Register is situational conditioning~-- a context-level factor that reweights probabilities within a single interaction. When I condition on an academic lecture, I expect nominalizations and passive constructions. Discourse community is a mixture component~-- a latent variable that identifies the source. When I recognize a text as coming from medical professionals, I interpret technical jargon as standard rather than deviant.

The three operate at different timescales and different levels of abstraction, but they work together. A single utterance is conditioned simultaneously by who is speaking (dialect), what they're doing (register), and what community they're participating in (discourse community). The apparent chaos of \mention{English} resolves into an orderly set of conditioned probabilities~-- not one messy distribution, but many tight ones layered atop each other.

This conditioning structure isn't just an analytical convenience for linguists; it is the ground truth of acquisition. \citet{wiese2023} argues that children don't begin by learning languages like German or Turkish. They learn \term{Communicative Situations} (\term{com-sits}).

In Wiese's \mention{free-range language} framework, the primary unit of acquisition is the com-sit: interactions with specific people (\eg\ \mention{talking to Dad}) in specific contexts (\eg\ \mention{breakfast table}). A child might learn \mention{doggie} in one com-sit and \mention{Wauwau} in another. Initially, these are just situation-bound variants.

Over time, the child detects that certain bundles of features co-occur across specific sets of situations. The \mention{German} cluster and the \mention{English} cluster emerge from the bottom up, not because the child has a \term{Language Acquisition Device} dealing in named languages, but because the input is naturally partitioned by social routines.

This reverses the standard picture of language acquisition. The standard picture holds that children learn a language and then learn to vary it. The com-sits picture says that children learn to track conditioning structure and only later project \mention{language} as a social index over it. To return to our earlier metaphor from Section~\ref{sec:15:mixed-bin}: children aren't unmixing a bin they were given mixed. They're building separate bins from the start, and only later learn that adults call the collection \mention{English}.

Because acquisition is com-sit dependent, we arrive directly at a core methodological conclusion of the maintenance view: there is no privileged scale of analysis. If the cognitive machinery is built to track \term{conditioned systems}, it tracks them at any scale where they stabilize~-- whether that's a dyadic routine between siblings, a specialised internet subculture, or a formal register shared by millions. The mechanisms of homeostasis are scale-invariant. You don't need a different kind of cognitive apparatus to learn a sociolect than you do to learn a \enquote{language}.

This is why the mixed bin problem doesn't doom us to noise. Speakers are remarkably good at inferring the conditioning structure because inference is what they have been doing since infancy. We hear an utterance and rapidly compute: \emph{who is this person? what situation are we in? what community norms apply?} That inference unmixes the bin, allowing the tight coupling of the conditioned category to emerge.

\section{Indexicality as the inverse function}
\label{sec:15:indexicality}

Indexical meaning is just the inverse of that conditioning structure. If \term{grammar} is the function that predicts form given a social context ($P(\text{Form} \mid \text{Social})$), then \term{social meaning} is simply the inverse function: predicting the social context given the form~-- $P(\text{Social} \mid \text{Form})$.

This framework gives us a rigorous definition of social meaning, or \term{indexicality}. And to see that this formalization has teeth, we need only return to the double copula.

The grammar~-- the forward function~-- predicts the form securely. Conditioned on a state of \mention{spontaneous planning in an informal register}, the probability of the double copula ($P(\text{double copula} \mid \text{spontaneous planning})$) is phenomenologically high. Conditioned on \mention{edited academic prose}, that probability approaches zero.

The inverse function runs this in reverse. When you hear \mention{The thing is is...}, your brain's Bayesian machinery calculates the inverse probability. It asks under which condition this form was most likely produced. The answer comes back: \mention{Spontaneous planning}. You index the speaker as being in a specific cognitive and social state. The inference is fast, automatic, and consequential.

In fact, the red pen from the beginning of this chapter is exactly an inverse-function computation gone awry. The instructor detects the form, runs the inverse function, and gets an indexical hit for \mention{informal spontaneous speech}. But because the essay is supposed to be \mention{formal edited prose}, the instructor diagnoses a mismatch. The error isn't that the student generated an ungrammatical string; the error is that the instructor evaluates a form generated under one set of conditions as if it were produced under another.

This inverse computation connects directly to what \citet{eckert2008indexical} calls the \term{indexical field}. The inverse function rarely returns a single point estimate. When you hear \mention{It is posited that...}, the inverse function points to \mention{Academic Stance}. But that stance itself carries a distribution of potential social meanings that the community history sustains: authority, pedantry, objectivity, or distance. The listener infers a distribution over these meanings, and the community maintains it dynamically.

We are, as Clifford Geertz said, \enquote{meaning-seeking animals}. But as Chapter~\ref{ch:the-category-zipper} argued, we seek different kinds of meaning at different levels. The social stabilization of kinds works because we are constantly running this inverse function, attributing every token to a source, a stance, or a setting.

\section{The nominalist challenge, socially grounded}
\label{sec:15:nominalism-social}

This returns us to the challenge from Chapter~\ref{ch:what-we-havent-been-asking}. Haspelmath and Croft argued that because categories lack definitions, they can't be cross-linguistically real (Haspelmath) or even language-internally global (Croft). We are now in a position to see what they got right, and where the HPC framework allows us to go further.

They were right about the failure of definitions. If \textsc{noun} requires a set of necessary and sufficient conditions that holds across all languages, then \textsc{noun} doesn't exist. If \textsc{subject} requires a definition that covers every construction in English without exception, then \textsc{subject} doesn't exist.

But they were wrong to conclude that the only alternative is stipulation. 

To Haspelmath, we can now say: \term{Comparative concept}s aren't merely yardsticks we invent. They are \term{basin recognition}. When we find that the concept \textsc{adjective} is useful for describing unrelated languages, it isn't because we have forced the data into a box, but because the functional pressure to modify referents creates a recurring stabilizer basin. Languages slide into this basin repeatedly. We can now add that the \emph{social} mechanisms of this chapter~-- community norms, register ecologies, and the necessity of coordination~-- are part of what makes those attractors cross-linguistically recurrent. The \enquote{comparative concept} tracks an attractor in the design space that is maintained by the community's action needs.

To Croft, we can say: \term{Construction}s are indeed the primary units of form-meaning pairing, but they aren't islands. They are braided together by economy and alignment. \textsc{subject} is real in English not because it's a Platonic essence instantiated in every clause, but because the same social coordination that maintains dialect boundaries also maintains constructional coherence across speakers. The properties cluster because the mechanisms of learning and production favour reuse, and the community polices that reuse. The \enquote{generalization} isn't a fiction of the analyst; it's the causal adhesive of the grammar.

To both, we can say: The complexity of sociolinguistic variation isn't evidence against categories, but evidence of \term{mixture}. Speakers don't condition on a single homogeneous grammar but on latent variables~-- \term{discourse communities} and \term{registers}~-- that shift the parameters. The fact that \mention{I done it} is grammatical in one community and ungrammatical in another doesn't mean the category \textsc{Perfect} is a fiction; it means it's parameterised by a conditioning structure. Structure and social meaning are the same math read in opposite directions.

\citet{oconnor2019games} shows with similarity-maximizing models that linguistic categories can be conventional even when they appear to respect property structure. When payoffs depend on coordination, terms needn't track pre-existing essences; similarity is defined in payoff space, so the properties that count as natural are those relevant to the community's action needs. That is exactly the space in which \term{maintained kinds} live.

The nominalist is right that categories lack essences. What the nominalist declines to explain is why certain essenceless categories support induction, resist perturbation, and recur across unrelated languages. The social mechanisms of this chapter are part of the answer. By acknowledging that categories are parameterized by varieties, we don't make them less real. We make them \emph{more precise}.

This isn't realism-lite~-- categories as \enquote{useful fictions} or \enquote{helpful patterns.} It's realism proper. The mechanisms that maintain a category are as real as the mechanisms that maintain a species or a chemical bond. The habits that keep \textsc{noun} stable across English speakers aren't conventions we could abolish by fiat; they're embedded in acquisition, production, and the inferential economy of communication. Peirce called these \term{real generals}~-- laws of habit that exist not in particular tokens but in the regularity of their effects. That's what HPC categories are: generals stabilized by causal work, sustaining would-bes that earn their reality by the inferences they support.


\textsc{Definiteness thread.} Chapter~\ref{ch:definiteness-and-deitality} established that definiteness splits into two coupled HPCs~-- semantic identifiability and formal deitality~-- each maintained by distinct cognitive mechanisms. This chapter adds the social layer: those mechanisms don't operate in isolation. Register gatekeeping, discourse-community norms, and cross-dialectal accommodation keep the two clusters aligned \emph{across} speakers. The definite article's distribution is stable not because speakers share an innate parameter but because the same social pressures regenerate the same conditioning structure in each learner. Definiteness, like the double copula, is a coordination equilibrium: maintained because deviation is costly and conformity is rewarded.
