% Chapter 1: Words That Won't Hold Still

\chapter{Words That Won't Hold Still}

On March 2, 2008, at 3:13 in the morning, I got an email from Rodney Huddleston about a word he didn't understand.

The time stamp is misleading~-- Huddleston was writing from Australia, where it was a reasonable hour. But there's something fitting about the image of a linguist awake in the dark, wrestling with a single word. The word was \term{otherwise}. I had asked him whether there were grounds for treating it as a preposition rather than an adverb. His reply:

\begin{quote}
I'm not proud of the adverb analysis, or confident about it, and don't intend it to cover all uses. Its classification is quite a puzzle. Dictionaries have it as adverb and adj (\term{the truth is quite otherwise}) and some also as conjunction. There is something to be said for a prep analysis, which might cover adjunct and predicative complement uses. But I don't know how to handle \term{this suggests otherwise} or \term{the correctness or otherwise of the proposal}.
\end{quote}

Huddleston was not a careless analyst. He was the lead author of \textit{The Cambridge Grammar of the English Language}~-- 1,860 pages, seventeen years in the making, the most comprehensive descriptive grammar of English ever produced. If anyone knew what \term{otherwise} was, it should have been him.

But he didn't. Not because he had missed something. Because the word wouldn't hold still.

\section*{~}

This book is about what that puzzle reveals.

The natural reading is that \term{otherwise} is simply a hard case~-- an outlier, a word with an unusual history, a problem for specialists. Every grammar has its edge cases. You note them, flag the uncertainty, and move on.

But \term{otherwise} is not alone. Each of the following behaves as if it were built from a different blueprint than the one our categories assume:

\term{Cattle} takes plural agreement (\term{the cattle are grazing}) but has no singular. You can say \term{many cattle} or \term{three cattle}, but not \term{a cattle} or \term{one cattle}. The usual singular--plural paradigm simply doesn't apply. And it has been this way for centuries~-- not drifting toward regularity, not acquiring a singular, just sitting there being strange.

\term{The} marks definiteness~-- or so the textbooks say. But \term{go to the hospital} doesn't pick out a particular hospital. \term{The tiger is endangered} doesn't refer to any individual tiger. These aren't rare or marginal uses; they're ordinary English, and they're well-known problem cases precisely because the standard definition of definiteness~-- that the referent is identifiable to the hearer~-- fails to cover them. Grammars describe these uses; they don't explain how they fit the category.

Reciprocals~-- \term{each other} and \term{one another}~-- pass some pronoun diagnostics and fail others. They function as objects (\term{they saw each other}), but their possessive forms are restricted: \term{each other's} exists but is limited in distribution; \term{one another's} is rarer still. Are they pronouns? Something else? \textit{The Cambridge Grammar} classifies them as pronouns, but the fit is imperfect, and the imperfection is systematic.

These are not obscure examples chosen to embarrass a theory. They are common words, central constructions, patterns that learners must acquire and speakers must process every day. The categories we use to describe them~-- noun, article, pronoun, adverb~-- are the basic machinery of grammatical analysis. And the machinery keeps encountering parts it wasn't built for.

\section*{~}

Two broad responses recur in the literature, and both fail.

The first says: categories are defined by necessary and sufficient conditions. A noun is whatever satisfies the necessary and sufficient conditions for nounhood; an adverb is whatever satisfies the conditions for adverbhood. Membership is binary. Boundaries are sharp. If a word doesn't fit, either the criteria are wrong or the word is exceptional. Refine the criteria, explain away the exceptions, and the system will be clean.

This is the essentialist view. It has the virtue of clarity: either something is an X or it isn't. It has the vice of never quite working. Every set of criteria produces counterexamples. The counterexamples get handled by stipulation, by subclasses, by \enquote{special} readings that multiply until the exceptions rival the rules. Huddleston's email is the essentialist view confronting its limits: here is a word, here are the criteria, and no combination of criteria delivers a stable answer.

The second response says: categories are not defined by conditions at all. They are prototypes~-- clusters of typical features, with central members and peripheral members and fuzzy boundaries all the way down. A robin is a better example of \term{bird} than a penguin is, but both are birds. \term{Run} is a better example of \term{verb} than \term{beware} is, but both are verbs. Stop expecting sharp edges. Gradience is the nature of the beast.

This is the prototype view. It captures something real~-- the empirical fact of gradience, the persistent failure of neat definitions. But as it is usually deployed in linguistics, it purchases descriptive adequacy at the cost of explanation. If \term{cattle} is a \enquote{less central} noun, why does its non-centrality take exactly the form it takes~-- no singular, plural agreement, full compatibility with numerals above one? Why has it been stable for five hundred years instead of drifting toward the core or out of the category entirely? Why don't categories dissolve into chaos if their boundaries are genuinely fuzzy? Prototype descriptions record the gradience. They don't, by themselves, explain why the gradience holds still.

\section*{~}

The essentialist sees sharp boundaries that don't exist. The prototype theorist sees fuzzy boundaries and stops there. Both share an assumption so deep it's almost invisible: that these are the only options. Either categories have essences, or they are looser groupings~-- useful for description, perhaps, but not the kind of thing that could bear explanatory weight.

There is a third possibility.

What if categories are real, stable, and explanatorily powerful~-- but not because they have essences? What if their boundaries are genuinely fuzzy~-- but not because they are arbitrary? What if the stability and the fuzziness are both consequences of something else, something neither tradition has squarely addressed?

That something is \textit{mechanism}: the causal processes that hold category properties together. The forces that keep the clustering clustered. This idea will need to be made precise~-- \enquote{mechanism} can't be a black box with explanatory magic attributed to it. Specifying the mechanisms for grammatical categories is where the real work lies, and the next chapter begins that work. But the core insight can be stated now: categories hold together not because they have essences but because something is actively holding them together.

The framework has a name~-- homeostatic property cluster kinds~-- and a history. It was developed in philosophy of biology, where it offered a way through the species problem: the long failure to define \term{species} by necessary and sufficient conditions. A species, on this view, is not a type fixed by morphology or genetics or interbreeding capacity. It is a population whose properties cluster because gene flow, development, ecology, and selection maintain the clustering. The extension to grammar is not a loose analogy. Grammatical categories are populations too~-- distributed across speakers, transmitted across generations, stabilized by processes that can be identified and tested. A grammatical category is a linguistic population whose properties cluster because acquisition, entrenchment, interactive alignment, and iterated transmission maintain the clustering over time.

But before the solution can do its work, the problem needs to be felt more precisely. The next sections sharpen the critique: what essentialism actually claims and where exactly it breaks; what prototype theory offers and what it leaves unexplained; what it would take for a view of categories to do better. Only then will the third option look like what it is~-- not a compromise, but a different kind of answer to a different kind of question.

Huddleston's email sits in my files, seventeen years old now. \textit{Its classification is quite a puzzle.} The puzzle was never just \term{otherwise}. It was what kind of thing a grammatical category must be for that sentence to be exactly the right thing to say.


\section{Essentialism examined}

Chomsky, in his 1965 \textit{Aspects of the Theory of Syntax}, crystallised a picture that many linguists had already been working with. The grammar of a language consists of a finite stock of categories~-- each with sharp boundaries~-- and rules for combining them. A lexical item belongs to exactly one major category (or to several, but then as several distinct lexical entries), and the job of theory is to state the defining conditions on each category so that every item can, in principle, be placed once and for all. This picture brought order: it made syntactic theory look like a matter of discovery rather than stipulation, and for decades that is how linguistics proceeded.

This assumption~-- that categories like Noun and Verb are fixed and universal~-- embodies what philosophers call essentialism, and it's been linguistics' default mode for millennia. Essentialism is the natural view. It's where you start if no one has told you there's a problem.

The intuition runs deep. When a child learns that \term{dog} picks out the class of dogs, we're tempted to think that there's some inner dogness~-- some cluster of necessary properties~-- that all and only dogs share. Borderline cases exist, of course, but we think of them as fringe matters. The real work of the concept is done by its core, and the core is fixed by an essence. The same picture quietly informs how linguists talk about \term{phonemes}, about \term{nouns} and \term{adjectives}, about \term{definiteness}, about \term{subject} and \term{object}. There are different kinds of sounds, different kinds of words, different kinds of grammatical functions, and the grammar cares about the difference.

The philosophical roots go back to Aristotle. A category, on the classical view, is defined by a set of properties that are individually necessary and jointly sufficient. To be a triangle is to be a closed plane figure with three straight sides. Anything that satisfies those conditions is a triangle; anything that doesn't, isn't. There are no borderline triangles, no gradient membership, no triangles that are more triangular than others. The definition carves nature at its joints.

That framework remained remarkably stable across two millennia. Dionysius Thrax codified Greek grammar into eight parts of speech; Pāṇini imposed rigid categorial distinctions on Sanskrit; medieval Modistae treated nouns and verbs as reflections of ontological types (\textit{modi essendi}); Port-Royal grammarians mapped word classes onto universal thought structures. When Bloomfield formalized structural linguistics in 1933, he was inheriting, not inventing, the assumption that grammar consists of sharply bounded types.

The structuralist inheritance brought this view into modern linguistics. Leonard Bloomfield characterized the phoneme as \enquote{a minimum unit of distinctive sound-feature{\ldots} The speaker has been trained to make sound-producing movements in such a way that the phoneme features will be present in the sound waves, and he has been trained to respond only to these features} \citep[79]{bloomfield1933}. The phoneme, on this view, is a real unit with identifiable acoustic properties~-- discrete, bounded, essential. Bloomfield treated morphemes and word classes the same way: each a fixed category with clear membership conditions. His program made linguistic analysis a matter of identifying and taxonomizing these essential units.

Applied to language, this yields a picture that most working linguists absorb without being taught it explicitly. A noun is whatever satisfies the criteria for nounhood~-- it inflects for number, it heads noun phrases, it can be modified by adjectives. A verb is whatever satisfies the criteria for verbhood~-- it inflects for tense, it takes arguments, it heads verb phrases. The criteria might be debated; the assumption that there \textit{are} criteria, that categories have definitions, is rarely questioned. Even linguists who reject essentialism in principle often write as if it were true, because the alternative is hard to operationalize.

The essentialist picture has genuine explanatory power. The great descriptive grammars~-- Jespersen, Quirk, the Cambridge Grammar itself~-- proceed category by category, laying out the membership criteria, cataloguing the members, noting the exceptions. The exceptions are always there, but they're handled as exceptions: marginal cases, historical residue, items in transition. The core of each category is secure. Textbooks are organized around this architecture. Pedagogical grammars depend on it. Parsers are built on it. The infrastructure of linguistic analysis presupposes that categories have boundaries and that the boundaries can, in principle, be found.

The strongest modern version of essentialism isn't naive. Generative grammar, in particular, has developed sophisticated accounts of category structure. Categories are defined by bundles of features~-- [+N], [−V], and so on~-- and the features are claimed to be universal, part of the innate language faculty. On this view, the category system isn't learned; it's given in advance. What children acquire is which words go in which slots, not the slots themselves. The boundaries are sharp because they're hardwired.

Mark Baker's \textit{Lexical Categories} (2003) provides a recent, explicit defense of this position. Baker argues that Noun, Verb, and Adjective are not arbitrary or language-specific groupings but natural categories with essences rooted in syntactic roles. His approach is, in his words, \enquote{formal, syntax-oriented, and universal, as opposed to the functionalist, semantic, and relativist approaches} \citep[ix]{baker2003}. Nouns, on his account, bear referential indices; verbs license specifiers; adjectives predicate properties. These defining properties hold across all languages. Surface variation gets explained via parameterized syntax, but the underlying category types remain constant.

The same essentialist logic extended to semantics. Jerrold Katz, working in the 1960s and 70s, defended the classical theory of lexical concepts: each word's meaning is defined by a set of necessary and sufficient semantic features. The standard example was \term{bachelor}, analyzed as [+adult] [+male] [+unmarried]. Anything satisfying those features is a bachelor; anything lacking one isn't. Katz treated such decompositions as cognitively real~-- part of what speakers know when they know a word's meaning. The category \textsc{bachelor} has an essence: unmarried adult maleness. The definition carves the concept at its joints.

This is a powerful picture. It explains why categories are universal~-- every language has nouns and verbs~-- and why children converge on the same system despite variable input. It makes predictions: if categories are feature bundles, then certain combinations should be impossible, and certain patterns of acquisition should be observed. The predictions have been tested, debated, refined. Whatever its ultimate fate, generative essentialism is a serious research program, not a folk prejudice dressed up in jargon.

And yet.

The problem with essentialism isn't that it's wrong about everything. It's that it keeps encountering cases where the machinery doesn't fit~-- and the response is always the same. Add a stipulation. Create a subclass. Mark the item as exceptional. The exceptions accumulate, and at some point you have to ask whether the machinery is doing explanatory work or merely recording the failures of a prior commitment.

Return to \term{otherwise}. Huddleston's puzzlement wasn't a lapse. It was a report from the front lines. The word functions as an adverb (\term{think otherwise}), as an adjective (\term{the truth is quite otherwise}), as something conjunction-like (\term{do it or otherwise face the consequences}), and in constructions that resist any standard label (\term{the correctness or otherwise of the proposal}). What are the necessary and sufficient conditions for adverbhood that include \term{otherwise} in \term{think otherwise} and exclude it in \term{the correctness or otherwise}? There aren't any. Every definition either lets in too much or leaves out too much.

The essentialist response is predictable: \term{otherwise} is polysemous, multiply listed, historically complex. Each use gets its own entry, its own feature specification, its own subcategorization frame. The grammar doesn't fail; it proliferates. But this is bookkeeping, not explanation. We wanted to know what makes something an adverb. We got a list of contexts where \term{otherwise} behaves adverbially, and another list where it doesn't, and no principled account of why the word straddles the boundary instead of sitting on one side or the other.

Consider the broader pattern. In English, the major categories~-- noun, verb, adjective, adverb, preposition~-- are supposed to be definable by distributional and morphological criteria. But every category has items that meet some criteria and fail others:

\term{Fun} looks like a noun (\term{we had fun}) but increasingly takes degree modification like an adjective (\term{that was very fun}). The change is ongoing, and speakers differ. What is \term{fun}~-- noun, adjective, or in transit between them? The essentialist must say it's one or the other, perhaps with a secondary listing for the emerging use. But the very fact of the transition shows that the boundary isn't sharp. The word is moving across a space that essentialism says doesn't exist.

\term{Near} takes objects without a preceding \term{to} (\term{near the house}), which makes it look like a preposition. But it also has comparative and superlative forms (\term{nearer}, \term{nearest}), which makes it look like an adjective. Some grammars call it a preposition; some call it an adjective; some, including CGEL, call it both~-- a word that belongs to two categories. But if categories are defined by necessary and sufficient conditions, how can a single word satisfy two incompatible definitions? The answer is that the definitions weren't necessary and sufficient after all. They were rough guides, and \term{near} falls in the gap between them.

\term{Like} has migrated from verb (\term{I like it}) to preposition (\term{people like you}) to conjunction (\term{like I said}), with each use attracting different degrees of acceptance depending on register and generation. The prescriptive tradition treats conjunction \term{like} as an error; the descriptive tradition documents its spread; neither can say what \term{like} is, because \term{like} is not one thing. It's a word with multiple category memberships, or a word in category flux, or a word that exposes the fiction of stable category boundaries. What it's not is a tidy example of how essentialism works.

The deeper problem is that the failures aren't random. They cluster in predictable places.

High-frequency items with broad distribution are especially likely to have mixed category profiles. This makes sense if category membership is reinforced by consistent behavior in consistent contexts: a word that shows up everywhere accumulates conflicting evidence about what it is. But essentialism has no account of this. Frequency isn't a feature. Distribution is a symptom, not a cause. The theory lacks the resources to explain why some words misbehave and others don't.

The intersection zones between major categories are chronic trouble spots. The adjective--adverb boundary, the preposition--conjunction boundary, the noun--adjective boundary~-- these are not marginal curiosities. They're systematic sites of instability, not because linguists are careless but because the boundaries themselves are under constant pressure. Essentialism predicts sharp lines; the evidence shows gradients. The response is always the same: posit more subcategories, more features, more exceptions. But a theory that can accommodate any pattern by adding epicycles is a theory that predicts nothing.

Items undergoing historical change present a special problem. If categories have essences, then change should be abrupt: a word is one thing, then it's another. But the actual pattern is gradual. \term{While} was a noun (meaning 'period of time'), became an adverb, became a conjunction. Each stage left traces in the next. Essentialist grammars must treat each stage as a separate entry, missing the continuity that drives the change. The alternative~-- acknowledging that category membership is something a word can have more or less of~-- is precisely what essentialism forbids.

None of this means essentialism is useless. For most words, most of the time, the category labels work. \term{Dog} is a noun; \term{run} is a verb; \term{quickly} is an adverb. The labels support generalization. They organize grammars. They enable pedagogy. If every word were like \term{otherwise}, linguistic analysis would be impossible.

But not every word is like \term{dog}. And the question is what to make of the words that aren't. The essentialist answer~-- treat them as exceptions, lexical idiosyncrasies, marginal cases~-- works as long as the exceptions stay marginal. When the exceptions multiply, when the boundaries fray systematically, when the core cases start looking like special instances of a more general pattern of variation, then the answer stops working. The framework designed to handle the clear cases has nothing to say about the unclear ones, except that they're problems to be solved later, with better definitions.

This is where essentialism stands. It handles the core. It fumbles the periphery. And it has no account of why the periphery exists at all~-- why categories should have fuzzy edges if they're defined by sharp conditions.

\subsection*{Essentialism beyond formalism}

The impulse isn't confined to formalist or generative traditions. Even frameworks that foreground gradience and usage often smuggle essences back in. Cognitive grammarians like Langacker reject autonomous syntax and embrace prototypes, yet treat the noun/verb opposition as a universal, notional distinction grounded in essential cognitive abilities for conceptualising things versus processes. Functional grammarians in Dik's tradition allow that not every language has a \term{subject}, but hold that once a language does, the function comes with universal properties and obeys cross-linguistic hierarchies. Role and Reference Grammar, explicitly typological and semantically driven, posits exactly two macroroles~-- Actor and Undergoer~-- as a \enquote{fundamental opposition} underlying clause structure everywhere. The details differ, but the metaphysical shape is familiar: a small, privileged stock of basic categories whose natures are fixed in advance and which languages are assumed to instantiate.

Essentialism survives across theoretical divides. It continues to guide practice even where explicit talk of necessary and sufficient conditions has faded. It's still the default assumption: that categories, if they are to be real and explanatory, must have sharp boundaries and that fuzziness, where it appears, is a defect in our descriptions.

But the pragmatic compromise runs deeper than most practitioners acknowledge. Working linguists proceed \textit{as if} categories have sharp boundaries even when they privately doubt that the boundaries are real. This isn't intellectual dishonesty; it's a rational response to the demands of description and analysis. Grammars need to be written. Students need to be taught. Parsers need to be built. All of these tasks become vastly simpler if you can assume that \term{fun} is either a noun or an adjective, that \term{near} is either a preposition or an adjective, that \term{otherwise} has a stable category assignment. The alternative~-- treating every item as occupying a unique position in continuous category space~-- would make grammatical description intractable.

The compromise shows up in how grammarians talk about their own methods. They'll note that a particular classification is \enquote{not entirely satisfactory} or that \enquote{the boundaries are somewhat fluid}, then proceed to use the classification anyway because the architecture of the grammar requires it. They'll acknowledge gradient membership in one paragraph and then, two pages later, write rules that presuppose binary distinctions. This isn't sloppiness. It's the recognition that some idealizations are necessary for the work to proceed, even when the idealizations are known to be false.

Some grammarians make this tension explicit.\footnote{For example, Geoffrey K.\ Pullum (personal communication) writes: \enquote{I am well aware that I am (for pragmatic reasons) presupposing an essentialist mindset that assumes `Categories exist; items either belong or don't; our job is to discover the boundaries', and that such essentialism might well be wrong. I have thought that for quite a long time. The main pragmatic reason for maintaining the implausible essentialism in question is just that it's useful to be able to talk to linguists in a language they understand.}} They distinguish the metaphysical implausibility of sharp essences from the practical convenience of talking as if such essences existed. Essentialism becomes a shared fiction~-- useful for communication, indispensable for pedagogy, but not to be confused with a claim about what grammatical categories actually are.

The question this raises is whether there's an alternative. Can we build a picture of categories that accommodates the gradience, the flux, the systematic misbehavior of high-frequency items and boundary cases~-- but still manages to be tractable, explanatory, and usable in grammatical description? Or are we stuck with the choice between an essentialist picture we know to be false and a gradient picture too unwieldy to operationalize?

The prototype tradition arose precisely to address this gap. Its answer: categories don't have sharp conditions. They have central tendencies, typical members, gradient structure. The fuzziness is real, and the theory should accommodate it rather than explain it away.

That answer has its own problems, which the next section addresses. But the prototype theorists saw something the essentialists missed: the boundaries weren't going to sharpen up with better definitions. The boundaries were evidence of something the definitions couldn't capture.
