% SLOGANS: 
% - Chapter-specific: \"dynamic discreteness\" and \"real gradience\" (twin themes)
% - Book-wide: \"categories are real because they're maintained\" appears in conclusion
% See notes/book-slogans.md for the slogan strategy.

\chapter{Discrete from continuous}
\label{ch:dynamic-discreteness}

% Chapter 4 introduces HPC framework; this chapter applies it to the discreteness problem.

Thursday, 4 December 2025 was one of the most intellectually exciting days of my life.

I had conceived of this book~-- or perhaps I'd say the conceptual process had come to a head~-- the previous Saturday, when I created a project folder on my computer called \texttt{HPC book} and sent a synopsis to some friends with the subject line \enquote{Does linguistics need this book?} I've learned to treat the act of naming a folder as the weakest possible form of commitment, but it was enough to start the chain reaction.

Geoff Pullum said yes. Ryan Nefdt said yes~-- and added something I hadn't thought of:

\begin{quote}
I personally favour a phase transition approach in which discreteness really is a feature of a system under a particular kind of measurement and context and continuity under a different one (like water moving from liquid to gas takes on different properties). So borrowing more from physics than biology.
\end{quote}

I woke very early the next morning thinking about phase transitions~-- and about a paper I'd written two months earlier and then set aside. That paper had formalised Sorites tolerance using hyperreal numbers, inspired by work by Toby Ord. It had been \enquote{sitting in a drawer}, waiting for a use. Now it had one.

By bedtime that evening, I had the first draft of this chapter.

Here is the puzzle I hadn't known I needed to solve.

\section{The gradience problem}
\label{sec:5:gradience-problem}

Grammatical categories look discrete: a word is a noun or it isn't, a sentence is grammatical or it isn't. But the properties underlying those categories are continuous: frequency distributions shade smoothly from one pattern to another, phonetic realisations vary along continuous dimensions, semantic features don't come with sharp edges. How do discrete categories emerge from continuous substrates?

Two familiar answers, each half right. The essentialist gets discreteness but loses reality~-- definitions that don't exist can't ground categories that do. The prototype theorist accepts the reality but loses the discreteness~-- why don't categories blur into each other over time? We need a third option: an account of how mechanisms operating on continuous substrates produce categorical outcomes. Call this \term{dynamic discreteness}~-- discreteness that is achieved, not given.

The answer lies in scale. A change that's negligible when you're far from a boundary becomes appreciable when you're close. That's why tolerance intuitions coexist with boundary intuitions~-- both are correct, just in different regimes. And the mechanisms that maintain categories~-- acquisition, entrenchment, alignment, transmission~-- are what keep the boundary where it is.

\subsection{The phase-transition intuition}
\label{subsec:5:phase-intuition}

Chapter~\ref{ch:what-we-havent-been-asking} introduced a physical analogy. Water has no essence of liquidity. The same H\textsubscript{2}O molecules constitute ice, liquid water, and steam. What differs is the collective behaviour: at certain temperatures and pressures, molecules settle into configurations we recognize as distinct phases. The boundaries between phases are real~-- you can skate on ice, swim in water, be shrouded by steam~-- and they are sharp enough for practical purposes. But they are not maintained by essences. They are maintained by dynamics.

Phase transitions produce discreteness from continuous substrates. Temperature is continuous. Pressure is continuous. Molecular motion is continuous. Yet the macroscopic outcome~-- solid, liquid, gas~-- is categorical. The categories are real; they support predictions; they figure in explanations. And no essence is required.

This is suggestive. If physical systems can exhibit discrete, stable structure without essences, perhaps grammatical categories can too. But the analogy needs to be more than suggestive. We need to say precisely what it is about the dynamics that produces discreteness, and we need to show that something analogous operates in language.

The key insight from physics is that phase boundaries are \emph{scale-sensitive}. Near a phase boundary, small changes in temperature or pressure can flip the system from one phase to another. Far from the boundary, the same small changes have no categorial effect~-- the system remains solidly in one phase. The boundary is sharp, but its sharpness is a feature of the system at a particular scale of observation. Zoom in far enough and you find molecules fluctuating; zoom out and you find stable ice or stable water. The discreteness is real, but it emerges at the macroscopic level from continuous microscopic variation.

The same logic applies to vague predicates generally. Consider a heap of sand. Ten thousand grains is clearly a heap. One grain is clearly not. Somewhere between them lies a boundary. The boundary seems sharp~-- at some point, removing one more grain tips the collection from heap to non-heap. But we can't identify where the boundary falls. Every attempt to specify a cutoff seems arbitrary. This is the Sorites paradox, and it has troubled philosophers for millennia.

The standard responses mirror those for grammatical categories. The epistemicist says the boundary is sharp but unknowable~-- there really is a precise number of grains where heaphood ends, we just can't find it. The degree theorist says heaphood comes in degrees~-- there's no sharp boundary, just a gradient from clearly-heap to clearly-not-heap. The supervaluationist says multiple precisifications are equally acceptable~-- the boundary is indeterminate, not because it's fuzzy, but because language doesn't fix a unique cutoff.

None of these quite captures the phase-transition intuition. The intuition says: the boundary is sharp (at the right scale), it is located in a determinate region (even if we can't specify it), and what maintains it is not an essence but a dynamic. The heap doesn't have an essence of heaphood; it has a configuration that, under the relevant conditions, clusters enough properties to count as a heap. Change the conditions enough and the clustering dissolves.

\subsection{Relative tolerance}
\label{subsec:5:relative-tolerance}

To make this precise, consider what \enquote{one grain doesn't matter} really means. It can't mean that removing one grain \emph{never} makes a difference~-- that would entail that even a single grain is a heap. What it means is that removing one grain doesn't matter \emph{when you have many grains}. Ten thousand minus one is still clearly a heap. Fifty minus one might not be.

The difference is relative, not absolute. Removing one grain from ten thousand is a 0.01\% change. Removing one grain from fifty is a 2\% change. The same absolute change has different significance depending on the scale.

This suggests a principle:

\begin{quote}
\textbf{Relative Tolerance:} Changes that are negligibly small \emph{relative to the current scale} preserve category membership. Changes that are appreciable relative to the current scale may not.
\end{quote}

The principle is intuitive. When you're dealing with a clear heap~-- thousands of grains~-- removing one is negligible. When you're down to borderline cases~-- dozens of grains~-- removing one is appreciable. Tolerance holds in the first regime and fails in the second. The boundary lies where negligible changes accumulate into appreciable ones.

But \enquote{negligible} and \enquote{appreciable} are themselves vague. To make the principle precise, we need a framework that can handle the distinction rigorously. This is where nonstandard analysis helps.

The next section is therefore not a philosophical aside. If tolerance is genuinely scale-sensitive, then the boundary region should have an identifiable empirical signature: judgments should become unstable as items approach it, and that instability should be selectively amplifiable by repetition and task framing. The formalism that follows is a way of stating those commitments precisely enough to be wrong.

\section{A formal solution}
\label{sec:5:formal-solution}

\begin{framed}
\noindent\textbf{Skip path.} The next subsection develops a formal model using hyperreal numbers. Readers who prefer to skip the mathematics can take away three points and rejoin at §\ref{sec:5:geometry-to-mechanism}:
\begin{enumerate}
    \item Tolerance is scale-sensitive: small changes are tolerated, but they accumulate.
    \item Sharp boundaries exist at thresholds we can't precisely locate.
    \item Discrete categories and gradient intuitions are compatible~-- the discreteness is in the structure, the gradience is in our access to it.
\end{enumerate}
\end{framed}

\subsection{The hyperreal formalization}
\label{subsec:5:hyperreal}

The hyperreal numbers extend the real numbers to include \emph{infinitesimals}~-- quantities greater than zero but smaller than any positive real number \citep{Goldblatt1998LecturesHyperreals}. This might sound impossible, but the construction is mathematically rigorous; Abraham Robinson developed it in the 1960s. For our purposes, infinitesimals provide a way to make \enquote{negligibly small} precise without committing to any particular finite threshold. The application to vague predicates summarized here is developed fully in \citet{reynolds2025sorites}.

Nonstandard analysis sounds exotic, but it is just the rigorous way of doing what physicists and engineers have always done with infinitesimals. If your first instinct is to raise an eyebrow at the jargon, that is the correct emotional response; the mathematics is here to discipline an intuition you already have. I'm not claiming that speakers perform hyperfinite reasoning or that grammars contain infinite objects~-- any more than calculus claims that real trajectories contain infinitely many points. The hyperreals are a modelling tool: they let us talk cleanly about \enquote{negligible relative to current scale} across all finite scales at once, without arbitrary cutoffs. Readers who find the formalism intimidating can take away the core insight~-- tolerance is scale-sensitive, and scale-sensitivity produces sharp boundaries at inaccessible thresholds~-- and trust that the mathematics underwrites it.

Here is how the framework applies to vague predicates. Let $P$ be a predicate like \textit{is a heap}, and let $\mu(x)$ be a measure of the relevant property~-- say, grain count. For successive cases $c_i$ and $c_{i+1}$ in a series, let $\Delta\mu_i = \mu(c_{i+1}) - \mu(c_i)$ be the change from one case to the next.

The Relative Tolerance principle becomes:

\begin{quote}
If $|\Delta\mu_i|/\mu(c_i)$ is infinitesimal, then $P(c_i) \leftrightarrow P(c_{i+1})$.
\end{quote}

In plain language: if the fractional change is infinitesimally small, the predicate's truth value is preserved. This captures the intuition that proportionally tiny changes don't matter, while allowing that accumulated changes can.

Now model the entire Sorites series~-- from clear heap to clear non-heap~-- as a \emph{hyperfinite chain} of $\omega$ steps, where $\omega$ is an infinite hypernatural number (Figure~\ref{fig:hyperfinite-chain}). This is a standard construction in nonstandard analysis: a sequence indexed by hypernaturals, behaving internally like a finite sequence but containing infinitely many elements from the external perspective.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/5.hyperfinite-chain.png}
\caption{The hyperfinite Sorites chain. The predicate $P$ (e.g., \emph{is a heap}) holds at every standard natural index: tolerance preserves truth across any finite portion of the series. The cutoff $K$ lies at a hypernatural index beyond all standard naturals~-- determinate within the model but not finitely specifiable.}
\label{fig:hyperfinite-chain}
\end{figure}

Within this hyperfinite chain, by the transfer principle of nonstandard analysis, any monotone sequence from $P$-true to $P$-false has a least index $K$ at which $P$ flips. This $K$ is a hypernatural~-- not a standard natural number, but a number in the extended system. And crucially, because we've stipulated that $P$ holds at all standard indices (encoding the intuition that tolerance holds throughout any finite portion of the series), $K$ must be nonstandard: infinitely large compared to any standard natural.

The picture that emerges:

\begin{itemize}
\item \textbf{Far from $K$}: The fractional change at each step is infinitesimal. Relative Tolerance applies. The predicate is preserved.
\item \textbf{Near $K$}: The fractional change is appreciable (no longer infinitesimal at this scale). Relative Tolerance is silent. The predicate can flip.
\item \textbf{At $K$}: The boundary. Sharp, determinate, located at a specific hypernatural index~-- but epistemically inaccessible because we can't finitely specify which hypernatural.
\end{itemize}

This is why the Sorites induction fails. The inductive premise~-- \enquote{if $P(c_i)$ then $P(c_{i+1})$}~-- holds only where Relative Tolerance applies, which is far from the boundary. Near the boundary, the premise is false. The chain breaks not because tolerance is non-transitive, but because tolerance doesn't apply uniformly across the entire series.

\subsection{Sharp boundaries, fuzzy appearances}
\label{subsec:5:sharp-fuzzy}

The hyperreal model maintains classical bivalent logic throughout. At every index~-- including hypernatural indices~-- the predicate is either true or false. There are no degrees of heaphood, no fuzzy membership values. The boundary is sharp.

But the boundary is also inaccessible. It lies at a hypernatural index that we can't specify using finite means. From our finite observational standpoint, we see clear heaps, clear non-heaps, and a region of uncertainty in between. The uncertainty is epistemic, not semantic: there is a fact of the matter about where the boundary falls; we just can't determine what it is.

The hyperreal view differs from epistemicism~-- Timothy Williamson's view that vague predicates have sharp boundaries, unknowable because of the limits of our discriminatory capacities \citep{williamson1994}. Both views say: sharp boundary, epistemic inaccessibility.

But the sources differ. For the epistemicist, the boundary is a brute metaphysical fact~-- fixed by the meaning of the word. For the hyperreal view, the boundary's existence is a consequence of the model's structure: the transfer principle plus monotonicity entail a cutoff at some hypernatural index. The sharpness falls out of the mathematics. The inaccessibility is built into what kind of thing a hypernatural index is. You can't finitely specify a nonstandard number any more than you can count to infinity.

More importantly, the explanatory consequences differ. Epistemicism treats the boundary as static: fixed by meaning, unchanging unless the word itself changes. The maintenance view predicts that boundaries can \emph{shift} when the mechanisms holding them in place are perturbed~-- and that the direction of shift is partially predictable from knowing which mechanisms are under pressure. Change acquisition patterns, change register distributions, change alignment pressures, and the basin structure should respond in characteristic ways. This is a research programme claim, not just a philosophical flourish. The maintenance view also \emph{entails} heightened judgment variance near boundaries (as we'll see in §\ref{subsec:5:gradient-discrete}): distance-to-boundary should predict judgment spread, with a characteristic functional form. Epistemicism is \emph{compatible} with such patterns but requires extra auxiliary assumptions to generate them. The hyperreal machinery, in short, is not ornamental: it is the formal face of a framework that predicts structured drift and variance signatures where epistemicism predicts only static inaccessibility.

This explains why tolerance intuitions and boundary intuitions can coexist. When you have a clear heap, you're right that one grain doesn't matter~-- the fractional change is negligible at that scale. When you're near the boundary, you're right that the situation is unclear~-- you're in the region where tolerance breaks down. Both intuitions are correct; they just apply to different regimes.

The model also explains why boundaries are stable. The boundary isn't an arbitrary stipulation imposed by speakers or analysts. It emerges from the structure of the hyperreal model, determined by the interplay between the tolerance principle and the monotonicity of the series. Different choices of nonstandard model (technically: different choices of ultrafilter in the construction) yield different specific values of $K$, but the structural features~-- sharp boundary, epistemic inaccessibility, scale-dependent tolerance~-- are invariant.

A different response to the discreteness problem is available. Khalidi, extending his critique of Boyd's homeostatic requirement, argues that natural kinds can have genuinely \emph{fuzzy} boundaries~-- not sharp-but-inaccessible, just fuzzy \citep[63--69]{khalidi2013}. Chemical isomers shade into each other as bond angles vary continuously. Biological species intergrade where populations overlap. If these paradigmatic natural kinds tolerate fuzziness, why should grammatical categories require sharp edges?

The answer lies in the structure of tolerance intuitions themselves. The Sorites reasoning pattern~-- one grain doesn't make a difference, so no number of grains makes a difference~-- is compelling precisely because we feel the tolerance premise is true. Khalidi-style fuzzy realism dissolves the puzzle by denying that there's a boundary at all: categories just shade into each other, and the puzzle evaporates. But this misses the explanatory target. Speakers don't behave as if categories shade continuously. They exhibit \emph{scale-dependent} tolerance: small changes are tolerated, large changes flip categorisation, and the transition is experienced as sudden even when the underlying change is gradual. The hyperreal model captures this phenomenology. Fuzzy realism doesn't.

This isn't to say Khalidi's \enquote{fuzzy kinds} are never apt. For weak grammatical categories~-- the thin ones from §\ref{sec:4:heterogeneity}~-- genuine fuzziness may be the right description. But for robust categories, where speakers make categorical judgments, exhibit abrupt transitions, and show heightened variance specifically near boundaries, the hyperreal model's sharp-but-inaccessible picture fits the data better. The choice between models is empirical, not metaphysical: does the category exhibit gradient shading (Khalidi) or scale-dependent tolerance (hyperreals)?

\section{From geometry to mechanism}
\label{sec:5:geometry-to-mechanism}

\subsection{From heaps to categories}
\label{subsec:5:heaps-to-categories}

Grammatical categories aren't heaps. But they face the same discreteness problem. If the underlying properties~-- frequency, phonetic realization, semantic features, distributional patterns~-- vary continuously, how do discrete categories emerge?

The hyperreal framework extends naturally to this multi-dimensional case. The single dimension of grain count becomes a multi-dimensional feature space; the single predicate $P$ becomes a family of predicates $P_1, \ldots, P_n$ corresponding to different categories; and the linear Sorites series becomes a space of possible items, each located at some point in feature space.

The Relative Tolerance principle generalizes:

\begin{quote}
If $d(\mathbf{x}, \mathbf{x} + \Delta\mathbf{x}) / d(\mathbf{x}, \partial R_i)$ is infinitesimal, then $P_i(\mathbf{x}) \leftrightarrow P_i(\mathbf{x} + \Delta\mathbf{x})$.
\end{quote}

Here $\mathbf{x}$ is a point in feature space, $\Delta\mathbf{x}$ is a perturbation, $d$ is a metric on the space, and $d(\mathbf{x}, \partial R_i)$ is the distance from $\mathbf{x}$ to the boundary of category $i$'s region. In plain language: perturbations that are infinitesimally small relative to the distance to the nearest boundary preserve categorization.

\paragraph{The metric question.}
The notation $d(\cdot, \cdot)$ raises an immediate question: what metric? Grammatical features are heterogeneous. Some are ratio-scaled (frequency, duration). Some are ordinal (degree of acceptability). Some are binary (takes plural marking: yes/no). No single off-the-shelf metric applies to all.

The answer is that we don't need a unique privileged metric. What we need is a \emph{family of admissible metrics} that all induce the same small-change structure~-- the same notion of when a perturbation counts as negligible. Two metrics $d_1$ and $d_2$ are equivalent for our purposes if they are locally Lipschitz-equivalent in the region of interest: there exist positive constants $a, b$ such that $a \cdot d_1(\mathbf{x}, \mathbf{y}) \le d_2(\mathbf{x}, \mathbf{y}) \le b \cdot d_1(\mathbf{x}, \mathbf{y})$ whenever $\mathbf{x}$ and $\mathbf{y}$ are in the relevant portion of feature space. Metrics in the same equivalence class generate the same neighbourhoods of zero~-- the same judgments about which changes are infinitesimally small. Any member of that class suffices for the Relative Tolerance condition.

There's a principled way to construct such metrics for heterogeneous feature spaces. The strategy, familiar from statistical ecology and clustering analysis, proceeds in two steps \citep[cf.][]{gower1971}.

First, embed each feature into a dimensionless numerical scale:
\begin{itemize}
\item \textbf{Ratio and interval features} (frequency, duration, formant values) are standardized~-- divided by range or standard deviation~-- so that a unit change in one dimension is comparable to a unit change in another.
\item \textbf{Ordinal features} (ranked acceptability, degree of grammaticalization) are mapped to their rank position, rescaled to $[0,1]$.
\item \textbf{Binary features} (presence/absence of inflection, compatibility with a construction) are left as $\{0,1\}$.
\end{itemize}

After embedding, all coordinates are dimensionless and live in a common numerical space.

Second, place a weighted norm on the embedded vector:
\[
d(\mathbf{x}, \mathbf{y}) = \left( \sum_k w_k \left| \phi_k(\mathbf{x}) - \phi_k(\mathbf{y}) \right|^p \right)^{1/p}
\]
where $\phi_k$ is the embedding function for feature $k$, $w_k$ is a weight reflecting that feature's contribution to the homeostatic cluster, and $p = 1$ or $p = 2$. The weights can be estimated empirically (from regression coefficients, factor loadings, or feature-importance measures in classification tasks) or treated as parameters to vary in thought experiments.

The embedding is a heuristic, not a discovery procedure for Platonic dimensions. We don't expect factor analysis or multidimensional scaling to reveal the true axes of grammatical space~-- there may be no unique decomposition. What we expect is that any adequate embedding will recover dimensions that are interpretable: dimensions that correlate with independently motivated grammatical or semantic scales. If a derived dimension tracks degree-modification compatibility, and degree-modification compatibility tracks the conceptual scale of gradability, then the embedding is doing its job.

This construction is a normed cousin of Gower distance, widely used for mixed-type data \citep{gower1971}, and it aligns with Gärdenfors's \emph{conceptual spaces}: multiple quality dimensions grouped into domains, each with its own metric, combined via weighted aggregation across domains \citep{gardenfors2000}. The connection to HPC is direct: the weights $w_k$ encode which properties play a stronger homeostatic role. Properties central to the cluster get large weights; free riders get small ones.

The point isn't that this construction is uniquely correct. The point is that a class of admissible metrics exists, all yielding the same small-change structure. The Relative Tolerance condition holds for all of them, not just one distinguished choice. Figure~\ref{fig:basin-visualization} illustrates the resulting landscape.

A clarification about dimensions. Some are conceptual in Gärdenfors's sense~-- grounded in perception and cognition, interpretable as quality scales that structure thought independently of any particular language. Eventivity, animacy, degree-scale structure: these show up in acquisition patterns, constrain semantic extension, have correlates in non-linguistic cognition.

Others are grammatical diagnostics: compatibility with plural morphology, occurrence in comparative constructions, position relative to the head. These may not correspond to conceptual dimensions at all. They're distributional reflexes of how a language happens to mark the underlying distinctions.

This means the feature space is hybrid, combining conceptual geometry with grammatical symptomology. The HPC claim is that homeostatic mechanisms operate on both~-- that acquisition pressures and functional demands keep conceptual and grammatical dimensions aligned. Where they pull apart, the basin may shift or split. The geometry isn't pure cognition and it isn't pure grammar; it's the joint product of both.

English evidentiality offers a worked example. English has no grammaticalised evidential marking~-- no obligatory morphology indicating whether information is firsthand, reported, or inferred. But the conceptual dimension exists: speakers make evidential distinctions using lexical means (\mention{apparently}, \mention{reportedly}, \mention{I saw that}).

In languages with grammaticalised evidentiality~-- Tibetan, Turkish, many Amazonian languages~-- the grammatical and conceptual dimensions are aligned; the basin structure has both. In English, the conceptual dimension floats free of any grammatical basin. English speakers should show no clustering on evidential diagnostics, even though they make evidential distinctions conceptually. This is alignment failure without semantic loss. The concept persists; the grammatical basin doesn't form.

\paragraph{Visualizing the landscape.}
The three-dimensional surface we have been imagining is a visualization: the potential function $V$ is defined over the full feature space $X \subseteq \mathbb{R}^n$, with category cores corresponding to local minima and boundaries to the stable manifolds of saddle points. Any particular ``valley and ridge'' picture is the graph of $V$ over a low-dimensional slice of $X$ on which the relevant contrasts are visible. Choosing the metric~-- the weights $w_k$ and the embedding functions $\phi_k$~-- is, in effect, choosing a geometry for the energy landscape: which directions are steep, which are shallow, which features matter most for which basin. Relative Tolerance then says: stay within one valley when perturbations are small in that geometry.\footnote{The physics metaphor is heuristic, not literal. Potential minima correspond to prototypes (geometrical centres of categories); basins of attraction correspond to convex regions where similarity to the prototype dominates; \enquote{forces} are learning and update processes~-- acquisition, entrenchment, alignment~-- that shift representations toward or away from category centres. The metaphor earns its keep by unifying these phenomena under a single image; it doesn't commit us to actual energy functions computed by the brain.}

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/5.three-basins.png}
\caption{A two-dimensional slice through grammatical feature space, with the potential function $V(\mathbf{x})$ plotted vertically. Category cores correspond to local minima; boundaries to ridges and saddle points. The noun and verb basins are separated by a high ridge (disjoint categories); the noun and adjective basins share a low saddle (porous boundary, overlapping membership possible). The trajectory of \emph{fun} illustrates diachronic movement from deep in the noun basin toward the noun--adjective boundary.}
\label{fig:basin-visualization}
\end{figure}

\paragraph{Basin structure.}
With a metric in hand, the multi-dimensional picture comes into focus. Each category occupies a region in feature space~-- a basin of attraction. Items deep in a basin are stably categorised: small perturbations don't move them out. Items near the boundary are unstable: the same perturbation that would be negligible elsewhere can flip the categorization.

The basins are typically convex. If two items are stably categorised as nouns, items intermediate between them in feature space should generally be too. This is Gärdenfors's criterion for natural categories, and it holds here because the mechanisms maintaining the basin~-- entrenchment, analogy, transmission~-- operate by similarity. Items near the prototype pull their neighbours toward the same categorisation. Apparent non-convexity, where it arises, may indicate that the relevant metric isn't the one assumed, that multiple overlapping basins are being conflated, or that the category has a multi-peaked structure~-- each of these is informative about the mechanisms at work.

The potential-well metaphor captures this: a convex basin corresponds to a single local minimum, and gradient descent from anywhere in the basin leads to the same attractor.

The boundary cases that trouble essentialism~-- \mention{fun}, \mention{near}, \mention{otherwise}~-- are items near basin boundaries. They exhibit properties of multiple categories because they're in the region where tolerance breaks down. Their instability isn't noise; it's evidence about where the boundaries lie.

The categories remain discrete. At any point in feature space, an item is either in category $i$ or not. The boundaries are sharp, located at hyperreal distances that we can't finitely specify. But the \emph{appearance} of gradience arises from epistemic limitations: we observe items at various distances from boundaries, and we can't determine exactly where the boundaries fall. This is \term{real gradience}~-- gradient structure that is genuine evidence about category organisation, not noise to be explained away.

\subsection{Mechanisms and basins}
\label{subsec:5:mechanisms-basins}

The hyperreal model describes the geometry of category boundaries. The HPC framework describes what maintains that geometry. The two are complementary.

Two stories, then. The first is representational: grammatical categories are regions in a feature space, with prototypes at centres and boundaries where similarity to multiple prototypes is balanced. This is the geometry.

The second is causal: the geometry persists because mechanisms~-- acquisition, entrenchment, alignment, transmission, functional pressure~-- exert forces that keep items clustered and boundaries stable. Conceptual-space approaches develop the first story in rich detail \citep{gardenfors2000,gardenfors2014}; HPC frameworks develop the second \citep{boyd1991,boyd1999}.

Both are needed. The geometry tells you what shape the categories have. The mechanisms tell you why they hold that shape.

Rosch's insight that categories have graded internal structure organised around prototypes is preserved~-- but repurposed. What's graded is typicality, not membership. An item is in the category or out; that boundary is sharp. But items inside the category vary in how typical they are, how central to the basin, how far from competing boundaries. HPC retains the gradience while relocating it: not degrees of membership but degrees of stability. A typical noun is one that sits deep in the noun basin, far from competition; an atypical noun sits nearer a boundary, more vulnerable to drift, eruptions, erosion. The mechanisms~-- entrenchment, alignment, transmission~-- are the forces that maintain this structure.

The hyperreal formalisation adds precision about boundaries: sharp but located at unreachable distances, exactly as tolerance intuitions suggest. Prototype theory describes the shape; HPC explains the stability; hyperreals explain the sharpness.

Recall the spinning top from Chapter~\ref{ch:kinds-without-essences}: the stability is dynamic, maintained by active forces rather than static rigidity. Consider what it takes for a basin structure to persist. The boundaries don't maintain themselves. Something has to keep items clustered in their basins; something has to resist drift toward the edges; something has to stabilise the overall configuration. Without such forces, categories would dissolve: items would wander through feature space, edges would shift randomly, the discrete structure would blur into noise.

The homeostatic mechanisms are exactly these stabilising forces. They don't define the boundaries~-- that would be essentialism. They maintain them by exerting pressure on items in feature space.

\textbf{Acquisition} shapes the initial configuration. Children don't learn categories by being told definitions; they induce structure from input. The input reflects the existing basin structure~-- speakers produce items clustered in basins, with boundaries marked by distributional discontinuities. Learners who are sensitive to these patterns acquire the same basin structure, perpetuating it.

\textbf{Entrenchment} deepens the basins. High-frequency items are processed more automatically, stored with greater strength, and more resistant to analogical pressure. They anchor the category, providing stable reference points that other items cluster around. The more entrenched the central members, the steeper the basin walls, the more stable the category.

\textbf{Interactive alignment} maintains consistency across speakers. In conversation, speakers accommodate to each other's usage, converging on shared norms. This creates pressure toward uniformity within a speech community, keeping items in their basins even when individual variation would otherwise push them toward boundaries.

Here is alignment in action. Suppose you say \mention{that was very fun}~-- using the degree modifier that signals adjectival status. Your interlocutor might accommodate, adopting \mention{very fun} in their next turn. Or they might resist, persisting with \mention{a lot of fun}. Either way, the micro-choice shifts local expectations: accommodation reinforces the adjectival pattern; resistance maintains the nominal one. Scale this up across thousands of conversations, and you get the macro-pattern: a word drifting toward or away from a category boundary depending on the balance of accommodation and resistance across the speech community. The mechanism is invisible in any single exchange. The effect accumulates. A two-turn micro-exchange captures the scale: \enquote{That was fun.} \enquote{Yeah, very fun.} Nothing dramatic happens, but a tiny accommodation has occurred~-- and those are the nudges that accumulate into a changed basin.

\textbf{Iterated transmission} filters variation across generations. Not all variants survive transmission; some are easier to learn, easier to produce, easier to understand. The variants that survive tend to be those well inside basins~-- clear exemplars, not boundary cases. Over generations, this filtering effect stabilizes the basin structure.

Consider a thought experiment. You invent a small language~-- twelve signals for twelve meanings~-- and teach it to someone. That person teaches what they learned to a third, who teaches a fourth, and so on. Each link in the chain introduces errors: the learner doesn't quite reproduce what they heard. You might expect the language to degrade into noise. But the opposite happens. Structure emerges.

This isn't speculation. \citet{kirby2008} ran exactly this experiment in the laboratory. Participants learned an artificial language associating nonsense words with coloured shapes, then reproduced it for the next \enquote{generation}. The initial language was random: no pattern connected form to meaning. By generation ten, the language had become compositional~-- word parts systematically encoded colour, shape, and motion. Transmission error dropped from roughly 50\% in the first generation to under 10\%. The key point: no individual designed this structure. As Kirby and colleagues put it, this \enquote{cultural evolution is an `invisible hand' process leading to phenomena that are the result of human action but are not intentional artifacts} \citep[10681]{kirby2008}. The linguistic variants are \term{replicators} in Hull's sense~-- entities that pass copies of themselves through the bottleneck of learning~-- and the communicative episodes are \term{interactors}, where selection pressure applies.\footnote{Hull's replicator/interactor distinction differs from Millikan's copied-kind framework used in Chapter~\ref{ch:kinds-without-essences}. Both capture differential reproduction, but Hull emphasises selection dynamics while Millikan emphasises lineage and proper function. The two frameworks are compatible but not identical.}

Figure~\ref{fig:iterated-learning} shows the pattern. As transmission error decreases (left panel), compositionality increases (right panel). The languages that survive iterated transmission are those easiest to learn~-- and learnable languages, it turns out, are structured languages. The bottleneck of imperfect learning acts as a filter, selecting for systematicity.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/5.iterated-learning.png}
\caption{Structure from transmission. Left: transmission error drops across generations as the language becomes more learnable. Right: compositionality index rises as systematic form--meaning mappings emerge. Data approximated from \citet{kirby2008} and \citet{kirby2015compression}. The pattern shows that iterated cultural transmission, not intentional design, produces linguistic structure.}
\label{fig:iterated-learning}
\end{figure}

The relevance to grammatical categories is direct. If category structure is transmitted imperfectly across generations~-- as it must be, since every child learns anew~-- then the structures that persist are those that survive the transmission bottleneck. Clear exemplars transmit better than boundary cases. Systematic patterns transmit better than exceptions. The basin structure we observe today is the residue of this filtering process, iterated across thousands of generations.

\textbf{Functional pressure} shapes the landscape. Categories exist because they serve functions: nouns package referents for tracking across discourse; verbs encode events and their participants; definiteness signals identifiability. Where functional pressure is strong, basins are deep and boundaries are sharp. Where it weakens, categories erode or restructure.

These mechanisms interact. Functional pressure determines which properties matter; acquisition determines how learners identify them; entrenchment determines which items anchor the category; alignment maintains community-level coherence; transmission filters what persists diachronically. The basin structure at any moment reflects the balance of all these forces.

The hyperreal model tells us what the basin structure looks like: true if irregular boundaries at hyperreal distances, tolerance holding within basins, instability near boundaries. The HPC framework tells us why the structure persists: mechanisms operating across multiple timescales, maintaining the clustering without defining it.

\subsection{Multi-category spaces}
\label{subsec:5:multi-category}

Linguistic categories don't exist in isolation. Nouns compete with verbs; adjectives shade into adverbs; prepositions overlap with both. The basin structure is not a set of independent regions but a tessellation of feature space, with categories abutting each other along shared boundaries. Khalidi calls such overlapping classification schemes \term{crosscutting kinds} and argues they pose no threat to realism: the same entity can be a node in multiple causal networks simultaneously \citep[69--72, 92--97]{khalidi2013}.
This creates additional structure. An item near the noun-verb boundary might be stably a noun (because it's deep in the noun basin) or unstably categorized (because small changes could push it into the verb basin). The same item might be far from the noun-adjective boundary, so its nominal vs.\ adjectival status is never in doubt. Category membership is determinate in multiple dimensions simultaneously, with different degrees of stability in different directions.

Consider the data for \mention{fun}~-- for fun, as it were. In the frequency-of-degree-modification dimension, it has drifted toward adjectives: \mention{very fun} is increasingly acceptable. In the takes-a-determiner dimension, it remains nominal: \mention{the fun we had} is unremarkable. The item sits near the noun-adjective boundary in one dimension, deep in noun territory in another. Its apparent mixed status reflects its location in a multi-dimensional space, not indeterminacy about what categories are.

American historical corpus evidence makes the trajectory visible \citep{davies2010coha}. Degree-modified \mention{fun}~-- \mention{really fun}, \mention{so fun}, \mention{rather fun}, \mention{very fun}~-- is rare through most of the 19th and early 20th centuries but rises sharply from the late 20th century onward: the combined frequency roughly triples between the 1980s and the 2010s. The pattern is consistent with a shift from marginal adjectival-like uses to robust productivity. \mention{Rather fun} shows early isolated footholds (British epistolary usage appears as early as 1827); the American surge comes later. The diagnostics co-move: this is not one fragile string but a bundle of degree environments converging in the same direction.

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=0.9\textwidth,
    height=4.5cm,
    view={0}{90},
    colorbar horizontal,
    colorbar style={
        at={(0.5,-0.25)},
        anchor=north,
        width=0.6\textwidth,
        height=0.3cm,
        xlabel={Tokens},
        xlabel style={yshift=0.2cm},
        xticklabel style={font=\footnotesize},
    },
    colormap={WhiteBlue}{rgb255(0cm)=(255,255,255); rgb255(0.02cm)=(200,220,255); rgb255(1cm)=(8,48,107)},
    colormap name=WhiteBlue,
    point meta min=0,
    point meta max=55,
    xtick={1,2,3,4,5,6,7,8,9,10,11,12},
    xticklabels={1870,1900,1910,1920,1930,1940,1950,1960,1970,1980,1990,2000s},
    xticklabel style={font=\footnotesize, rotate=45, anchor=east},
    ytick={1,2,3,4},
    yticklabels={\textit{very fun},\textit{so fun},\textit{really fun},\textit{rather fun}},
    yticklabel style={font=\small},
    xlabel={},
    ylabel={},
    enlargelimits=false,
]
% Data matrix: rows = modifiers (bottom to top: very, so, really, rather)
% columns = decades (1870, 1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000s)
% 2000s combines 2000 and 2010 since those are the surge years
\addplot[matrix plot*, mesh/cols=12, point meta=explicit] coordinates {
    % very fun: 1870=1, gaps, 1940=1, 1960=1, 1980=2, 1990=3, 2000s=18
    (1,1) [1]  (2,1) [0]  (3,1) [0]  (4,1) [0]  (5,1) [0]  (6,1) [1]  (7,1) [0]  (8,1) [1]  (9,1) [0]  (10,1) [2]  (11,1) [3]  (12,1) [18]
    % so fun: late surge
    (1,2) [0]  (2,2) [0]  (3,2) [0]  (4,2) [0]  (5,2) [0]  (6,2) [0]  (7,2) [0]  (8,2) [0]  (9,2) [1]  (10,2) [0]  (11,2) [8]  (12,2) [45]
    % really fun: late surge
    (1,3) [0]  (2,3) [0]  (3,3) [0]  (4,3) [0]  (5,3) [0]  (6,3) [1]  (7,3) [2]  (8,3) [3]  (9,3) [9]  (10,3) [6]  (11,3) [25]  (12,3) [52]
    % rather fun: steady early, declines (now row 4 = top)
    (1,4) [0]  (2,4) [3]  (3,4) [5]  (4,4) [2]  (5,4) [7]  (6,4) [5]  (7,4) [7]  (8,4) [7]  (9,4) [3]  (10,4) [1]  (11,4) [2]  (12,4) [2]
};
\end{axis}
\end{tikzpicture}
\caption{Degree-modified \mention{fun} in the Corpus of Historical American English. Darker cells indicate higher frequency. \mention{Rather fun} shows scattered early attestations; \mention{really fun} and \mention{so fun} surge in the 1990s--2000s. The late-century darkening across multiple rows shows convergent adjectivalisation.}
\label{fig:fun-coha}
\end{figure}

\paragraph{Boundary phenomena: operationalising distance.}
The \mention{fun} example shows diachronic drift toward a boundary. But how do we study items that sit \emph{at} a boundary synchronically~-- stably intermediate, not in transition but genuinely in between?

English reciprocals~-- \mention{each other}, \mention{one another}~-- offer a detailed case study \citep{reynolds2025reciprocals}. Standard reference grammars classify them as pronouns, and their distribution supports that much: they head NP-like phrases in the same slots as core pronouns, as verb and preposition complements. But they are non-canonical pronouns. Unlike core personal pronouns, they lack a person/gender/case paradigm and are distributionally defective in the most salient way: they resist ordinary free subject use (cf. \mention{Somebody left} vs. \mention{*Each other left}). 

On the other side, reciprocals are built from determinative material (\mention{each}/\mention{one}) and impose a determinative-like semantic constraint: they are obligatorily anaphoric and require plural antecedents. Yet they are not determiners: they cannot occur in determiner function (*\mention{each other friends} in the intended sense). The point is not to force a verdict from a handful of tests, but to motivate the boundary question: reciprocals sit where the pronoun and determinative profiles exert opposed pulls, making them an ideal probe for how to operationalise stable in-betweenness.

How stable is this in-betweenness? The methodology in \citet{reynolds2025reciprocals} addresses exactly this question by applying three diagnostics:

\begin{enumerate}
\item \textbf{Invariance under analytic perturbation.} Does the boundary position shift when you change the distance metric, the comparison set, or the feature weights? Across multiple correspondence analysis ordinations, Jaccard distances, and specification curves, reciprocals sit midway between pronoun and determinative anchors~-- a stable intermediate, not an artefact of a particular operationalisation.

\item \textbf{Cross-dimensional tension.} Do different feature families pull in different directions? Morphology pulls reciprocals toward determinatives (they lack the case paradigm of core pronouns). Semantics pulls them toward pronouns (they denote referents, not quantities). Syntax and phonology contribute little. The signature of a boundary item is exactly this: opposed pulls from mechanisms that maintain different basins.
\item \textbf{Clear anchors behave cleanly.} Do items unambiguously inside each basin show the expected clustering? Core pronouns like \mention{he} and \mention{herself} sit deep in the pronoun basin; determinatives like \mention{every} and \mention{either} sit deep in the determinative basin. The methodology confirms what intuition expects: the anchors cluster where they should. The interesting finding is that reciprocals don't.
\end{enumerate}

A caveat about statistical significance. Whether the boundary position looks \enquote{statistically significant} depends on which comparison items you choose. Pick one reasonable set and the result is extreme; vary the comparison set and the result wobbles. But the \emph{qualitative} finding~-- reciprocals sit between pronoun and determinative anchors~-- is stable across all choices. Under an HPC reading, this is exactly what genuine boundaries should look like: the diagnosis persists, but decisive p-values elude us because the item really is intermediate.

This operationalises the chapter's theoretical picture. \enquote{Distance to boundary} is not metaphorical; it can be measured. \enquote{Stability of ambiguity} is not vague intuition; it is invariance under analytic perturbation~-- the in-betweenness persists when you change your measurement instrument. Reciprocals sit in the overlap region of Figure~\ref{fig:disjoint-overlap}, not because the analysis went wrong but because the basin structure itself has overlap.

The same analysis applies to the existential debates about feature systems raised in §\ref{sec:3:lexeme-obsession}. Is \textsc{gender} a natural kind, or is it reducible to \textsc{number}? The question becomes: do gender and number occupy distinct basins maintained by distinct mechanisms, or is gender a sub-region of a larger number basin, maintained by the same mechanisms at a finer grain?

If the mechanisms maintaining gender systems~-- agreement patterns, acquisition pathways, functional pressures toward nominal classification~-- are distinct from those maintaining number systems, then gender is a genuine natural kind: a separate basin in the space of grammatical features. If the mechanisms overlap substantially~-- if gender turns out to be number's machinery for individuation, as some theorists have argued~-- then the basins merge, and \textsc{gender} as a cross-linguistic category dissolves into \textsc{number}.

This is an empirical question, not a definitional one. The hyperreal model tells us what to look for: distinct basins with true if irregular boundaries and scale-dependent tolerance. The HPC framework tells us how to investigate: identify the mechanisms, trace their operation, determine whether they cluster distinct properties or the same properties at different scales.

Consider the Shilluk number system introduced in §\ref{sec:3:lexeme-obsession}. English marks number with a suffix: \mention{cat}, \mention{cats}. Shilluk marks it through stem-internal changes in tone, vowel length, and voice quality~-- a singular/plural pair might differ only in whether the vowel is short, long, or overlong, or in whether the tone is Low or Fall. The exponence is lexicalised: each pair must be learned; no productive rule derives one from the other.

What does the HPC + hyperreal view say about this? Both English and Shilluk number systems occupy basins in a feature space whose dimensions include obligatoriness (number must be specified), agreement scope (specifier, verb, other constituents), and semantic function (individuation of entities). The exponence dimension~-- where English is affixal and Shilluk is fusional/suprasegmental~-- places them in different regions of morphophonological space but the same region of morphosyntactic and semantic space. They're in overlapping basins, not identical ones~-- independent systems shaped by similar pressures, not one system shared across languages.

The mechanisms maintaining the two systems are partially shared (acquisition of obligatory contrast, iterated transmission of the singular/non-singular distinction) and partially distinct (rule-based productivity in English, lexical storage in Shilluk). The prediction is that where the mechanisms align~-- the functional pressure to distinguish singular from plural~-- the categories should pattern similarly across languages. Where they diverge~-- the phonological substance of the exponence~-- we should expect variation. That's exactly what we find. The basin boundary for \textsc{number} is sharp in both languages (a noun is singular or plural, not in between), but the basin's location in the full feature space differs, because different mechanisms weight different dimensions. The same analysis extends to noun-class systems like Swahili's, to Mandarin classifiers, to German gender. Wherever there's grammatical categorisation, there should be basin structure maintained by mechanisms.

A question arises: is the cross-linguistic similarity \emph{convergence} or \emph{homology}? In biology, convergence means similar phenotypes arising independently (the wings of bats and birds), while homology means similarity inherited from a common ancestor (the forelimbs of all mammals). For \textsc{number}, the answer is probably convergence: the functional pressure to distinguish singular from plural exists in any language with nominal reference, and different languages have evolved different morphological solutions. The mechanisms overlap not because of common ancestry but because of common function. For \textsc{noun} and \textsc{verb}, the answer might be different: if these categories reflect universal constraints on predication and reference, they may be homologous~-- inherited from whatever cognitive architecture makes human language possible. The framework should distinguish these cases, and the mechanistic analysis provides the tools: shared mechanisms suggest homology; parallel mechanisms suggest convergence.

If the HPC analysis just confirms that English number and Shilluk number are both \textsc{number}, have we learned anything? Yes: the framework explains \emph{why} they deserve the same label~-- not definitional fiat, but overlapping mechanisms maintaining overlapping basins. And it makes predictions: if we found a putative \enquote{number} category maintained by entirely different mechanisms with no overlap in the functional dimension, the framework would say it's not the same kind~-- same label, different basin.

\section{Empirical consequences}
\label{sec:5:empirical-consequences}

\subsection{Gradient judgments, discrete categories}
\label{subsec:5:gradient-discrete}

The discreteness problem has a close cousin: the gradient-judgment problem. Speakers' acceptability judgments are notoriously gradient. Asked to rate sentences on a 1--7 scale, they distribute across the range, with clear acceptability at the extremes and uncertainty in the middle. If categories are discrete, why are judgments gradient?

The hyperreal framework suggests an answer. Distinguish the category from its measurement, the ontology from its epistemology.

\textbf{Grammaticality}~-- the property of conforming to the grammar~-- is discrete. A sentence either satisfies the constraints or it doesn't. The boundary between grammatical and ungrammatical is sharp, located at a hyperreal threshold in some underlying dimension (perhaps accumulated constraint violation, or distance from prototype, or processing cost).

\textbf{Acceptability}~-- the measured response in judgment tasks~-- reflects grammaticality filtered through noise. Processing difficulty, frequency effects, task demands, individual variation, and measurement error all intervene between the discrete category and the gradient response.

One reason this distinction is so easy to lose sight of is that the phenomenology is asymmetric. When a sentence is well-formed, nothing in particular announces itself: processing just runs. The felt data are mostly negative~-- the \enquote{catch} of a violation, the extra effort, the moment of repair. There is, in that sense, no positive feeling of grammaticality, just as there is no feeling of having sufficient oxygen; the experiences that demand attention are the absences. This is why boundary detection is cognitively natural: violations have a signature, conformity is typically silent.

A second reason is methodological. Temperature is not something you see; it is a theoretical magnitude inferred from what thermometers do. Likewise grammaticality is a latent property that we access only through instruments~-- judgments, reaction times, eye movements, priming, production choices~-- each of which can misread the target in both directions, yielding false comfort (illusory well-formedness) or false alarm (grammatical but degraded). The point is clearest in \emph{satiation} effects: repeated exposure can raise acceptability for initially degraded strings, even though it is implausible that the grammar itself has been rewritten over the course of a short experimental session. Satiation is therefore not a nuisance; it is direct evidence that the mapping from grammaticality to felt acceptability is plastic, and that we should not identify the construct with any one measurement channel.

Not all of these factors work the same way. Random fluctuations~-- trial-to-trial variation in attention, incidental processing load, measurement noise~-- blur the signal without shifting it. But frequency effects and task demands can systematically shift the mapping from grammaticality to acceptability. A borderline construction may be judged more acceptable if it's high-frequency than if it's low-frequency, even when both are equally grammatical. Instructions emphasising naturalness may locate the boundary differently than instructions emphasising correctness. These are biases, not noise. The two-layer model accommodates them: different contexts may activate different effective basin configurations, shifting where tolerance breaks down.

This predicts:
\begin{itemize}
\item Clear cases cluster at scale extremes (1s and 7s), because items deep in the grammatical or ungrammatical basin are stably categorized and noise doesn't flip them.
\item Borderline cases distribute across the middle, because items near the boundary are sensitive to noise~-- small processing fluctuations can shift judgments in either direction.
\item The gradient spread is widest near the boundary, because that's where the signal-to-noise ratio is lowest (see Figures~\ref{fig:two-layer-model} and~\ref{fig:judgment-variance}).
\end{itemize}

This is a testable prediction. If the two-layer model is right, items independently rated as near category boundaries should show higher variance in acceptability judgments than items rated as central. The prediction isn't that boundary items get middling ratings~-- that's compatible with gradient grammaticality~-- but that their ratings should be more variable across trials and subjects. The pattern should show up as a hyperbolic relationship between distance-to-boundary and judgment variance. Existing work on gradient acceptability already points in this direction: \citet{sprouse2013} found that judgment variability correlates with distance from prototypical (un)grammaticality, and \citet{dabrowska2010} showed systematic individual differences in judgments of complex constructions~-- exactly what you'd expect if different speakers have slightly different basin structures.

This last point deserves emphasis. The basin structure isn't uniform across speakers. Speakers with different input histories~-- more or less exposure to written registers, different dialectal backgrounds, varying levels of literacy~-- will have basins with different shapes and different boundary locations. The hyperreal model accommodates this naturally: each speaker's grammar is a different model, with a different boundary index $K$. What varies across speakers isn't just noise; it's the underlying geometry. The framework predicts that variability in judgments should be highest for constructions that fall near the boundary \emph{for most speakers}~-- and that speakers who agree on central cases may nonetheless diverge on peripheral ones, because their boundaries are located at different hyperreal indices.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/5.two-layer-grammaticality.png}
\caption{The two-layer model. Discrete grammaticality (binary: grammatical or ungrammatical) is filtered through processing and measurement noise to produce gradient acceptability judgments. Items deep in basins produce stable judgments at scale extremes; items near boundaries produce variable judgments distributed across the middle of the scale.}
\label{fig:two-layer-model}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/5.judgment-variance.png}
\caption{Predicted relationship between distance to category boundary and acceptability judgment variance. Items deep in basins (e.g., \mention{dog}) show stable judgments with low variance; items near boundaries (e.g., \mention{near}) show high variability. The hyperbolic relationship reflects the signal-to-noise ratio degrading as items approach the boundary.}
\label{fig:judgment-variance}
\end{figure}

This is not a new observation~-- the distinction between competence and performance has been with us since Chomsky. But the hyperreal framework adds precision. The boundary is not merely \enquote{somewhere in the grammar}; it's located at a specific (if unspecifiable) hyperreal threshold. The gradience is not merely \enquote{performance noise}; it's the predictable consequence of epistemic limitations near a sharp boundary.

Chapter~\ref{ch:grammaticality-itself} develops this picture fully, arguing that grammaticality itself is a homeostatic property cluster~-- a category maintained by mechanisms, with a sharp boundary at hyperreal distance. For now, the point is that gradient judgments and discrete categories are compatible. The discreteness is in the structure; the gradience is in our access to it.

\subsection{Dual membership}
\label{subsec:5:dual-membership}

One puzzle remains. If categories are discrete~-- if at every point in feature space an item is either in or out~-- how can there be genuine dual membership? How can \mention{near} be both a preposition and an adjective, not contextually selected, not in transition, but stably both? English is comfortable with a kind of dual citizenship here: items like \mention{near} can hold both passports without being in transit.

The answer is that discreteness holds predicate-by-predicate, not across predicates. The predicates \textit{is a preposition} and \textit{is an adjective} are each bivalent: at any point, each is either true or false. But they're not mutually exclusive. The basins can overlap.

This follows naturally from the HPC framework. Categories are maintained by mechanisms, not defined by partition. The mechanisms maintaining prepositionhood~-- patterns of complementation, head-of-PP status, lack of degree modification~-- are partially independent of those maintaining adjectivehood~-- gradability, predicative use, comparative morphology. An item can fall within the tolerance threshold for both, satisfying the clustering criteria for each.

Where the mechanisms align, the basins are disjoint: nouns and verbs occupy separate regions because the mechanisms that maintain them pull in different directions (Figure~\ref{fig:disjoint-overlap}, left). Where the mechanisms cross, the basins overlap: adjectives and prepositions share enough properties~-- predicative function, modification of nominals~-- that some items cluster with both (Figure~\ref{fig:disjoint-overlap}, right).

The regions of overlap are not arbitrary. They're located where the property clusters themselves overlap~-- where an item can satisfy the tolerance criteria for both categories simultaneously. These are exactly the cases that trouble essentialism: items that have \enquote{mixed} status because they satisfy criteria for multiple categories. On the HPC + hyperreal view, mixed status is not indeterminacy but dual location: the item is in two basins at once, stably, because the mechanisms that maintain each basin both apply.

The reciprocals case from §\ref{subsec:5:multi-category} exemplifies this cleanly. Morphology maintains the determinative basin; semantics maintains the pronoun basin. Reciprocals satisfy both. The mixture weights from the blend model~-- 0.534 pronoun-like for \mention{each other}, 0.487 for \mention{one another}~-- are not noise or measurement error. They are the empirical signature of genuine dual location: stable membership in overlapping basins, maintained by partially independent mechanisms.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\textwidth]{figures/5.disjoint-overlap.png}
\caption{Disjoint vs.\ overlapping category basins. Left: noun--verb mechanisms pull in opposite directions, creating mutual exclusivity. Right: noun--adjective mechanisms are partially independent, permitting overlap. Items like \mention{fun} and \mention{near} occupy the overlap region, satisfying the clustering criteria for both categories simultaneously.}
\label{fig:disjoint-overlap}
\end{figure}

\subsection{Summary: discreteness without essence}
\label{subsec:5:discreteness-summary}

The discreteness problem asked: if underlying properties are continuous, how do discrete categories emerge?

The answer combines two components:

\textbf{Structure (hyperreal model):} Discrete boundaries arise from scale-dependent tolerance. Changes that are negligible at the current scale preserve categorization; changes that are appreciable can flip it. The boundary is sharp, located at a hyperreal threshold, epistemically inaccessible but structurally determinate. Dynamic discreteness is like a snow-covered streetcar right-of-way: the snowfall is continuous, but the rails impose two stubbornly sharp tracks that exist only because of the system underneath them.

\textbf{Maintenance (HPC framework):} The basin structure persists because mechanisms~-- acquisition, entrenchment, alignment, transmission, functional pressure~-- hold properties together. Without these mechanisms, categories would dissolve. With them, the clustering is stable even though boundaries are fuzzy at the edges.

Together, these explain how categories can be:
\begin{itemize}
\item \textbf{Real}: maintained by causal mechanisms, not merely stipulated.
\item \textbf{Discrete}: with sharp boundaries, not gradient membership.
\item \textbf{Fuzzy at the edges}: because tolerance fails near boundaries, producing instability and apparent gradience.
\item \textbf{Stable}: because mechanisms maintain the basin structure across time.
\item \textbf{Capable of change}: because mechanisms can shift, basins can migrate, boundaries can move.
\end{itemize}

This is what the essentialist wanted~-- real structure, sharp boundaries, determinacy~-- without what the essentialist thought was required: definitions, necessary and sufficient conditions, essences. The categories are real because they're maintained, not because they're defined. The boundaries are sharp because tolerance is scale-dependent, not because some property is binary. The determinacy is causal-structural, not definitional.

The nominalist was right that essences don't exist. The nominalist was wrong to conclude that determinacy fails. The HPC framework, formalised via hyperreal tolerance, shows how to have both: natural kinds without essences, discrete categories from continuous substrates, sharp boundaries that we can't quite see. Dynamic discreteness and real gradience~-- the chapter's twin themes~-- turn out to be two sides of one insight.
