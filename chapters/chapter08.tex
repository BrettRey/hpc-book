% SLOGAN: How to know when you don't have a kind.
% Sticky sentence: "Not every stable pattern is a homeostatic cluster."

\chapter{Failure modes}
\label{ch:failure-modes}

\epigraph{To classify things is to name the games we play with them. To explain them is to ask why the game works.}{Cailin O'Connor (paraphrased), \textit{Games and Kinds}}

The danger of a good framework is that you start seeing it everywhere.

Once you learn to spot homeostatic property clusters~-- once you see how feedback loops can maintain stable patterns without essences~-- it becomes tempting to diagnose homeostasis in every corner of the grammar. \term{Noun}? HPC. \term{Subject}? HPC. \term{Voice}? HPC. \term{Non-finite}? Surely an HPC too. The world fills up with spinning tops.

This is the \term{inflation problem}. If the criteria for being a mechanism-maintained kind are too loose, the framework explains nothing because it excludes nothing. If every stable pattern counts as an HPC, then \enquote{HPC} just means \enquote{pattern we have a name for.}

Realism requires rejection. To claim that some categories are real because they are maintained by causal mechanisms is to imply that others are \emph{not}. For the maintenance view to do any work, we need to know what a failure looks like. We need criteria for when a category is a mere label, a transient accident, or a fiction.

\section{The inflation problem}
\label{sec:8:inflation}

Why do we have so many categories that fail the test? Why does linguistics~-- and cognition generally~-- fill up with "kinds" that turn out to be illusions?

The answer lies in the difference between the games we play and the world we play them in.

Cailin O'Connor's work on evolutionary signalling games identifies the trap precisely \autocite{oconnor2019games}. In her \enquote{sim-max} games, agents evolve signals to coordinate action. They learn to categorize the world in ways that maximise their payoff. The critical insight is that these evolved categories track \emph{payoff similarity}, not necessarily \emph{property similarity}.

Agents will group things together if treating them the same way yields a reward. They will treat distinct things differently if that matters for payoff.
\begin{enumerate}
    \item \textbf{Convergence:} Sometimes, payoff tracks property. We treat tigers alike because they share causal properties (teeth, claws, predation) that matter for our survival. Here, the category in our head maps to a causal cluster in the world.
    \item \textbf{Divergence:} Often, payoff ignores property. We treat "weeds" alike not because they share a biology (they don't), but because they share a relationship to our goals (we want them gone). \term{Weed} is a payoff category, not a biological kind.
\end{enumerate}

The inflation problem arises when we mistake a payoff category for a property category. We see that \term{adverb} is a useful label (it tells us \enquote{this modifies a verb}), and we assume there must be a deep, unified mechanism maintaining all adverbs as a natural kind. We confuse the utility of the label with the reality of the cluster.

Diagnosing failures, then, is about spotting this divergence. It is about identifying when the stability of a category comes only from the rules of the game (the grammar's checking requirements, the analyst's convenience), not from a shared causal maintenance loop.

\section{Thin clustering: The smoke ring}
\label{sec:8:thin}

The first failure mode is the category that barely exists.

A \term{thin} category is one where the signal exists~-- speakers recognize it, linguists name it~-- but the property cluster is ghostly. There is no robust homeostatic loop maintaining it. It is a \enquote{smoke ring}: it has structure, but that structure is ephemeral, typically generated by a transient convergence of other mechanisms rather than a dedicated feedback loop of its own.

\subsection{Diagnostics of thinness}
How do you know when a category is thin?
\begin{itemize}
    \item \textbf{High Arbitrariness:} Following O'Connor, thin categories act like pure conventions. They settle on an equilibrium that could easily have been otherwise.
    \item \textbf{Low Projectibility:} Knowing that an item belongs to the category tells you almost nothing else about it.
    \item \textbf{Vulnerability:} The category collapses under minor perturbation (e.g., a slight shift in register or frequency).
\end{itemize}

\subsection{Case study: \textit{Whose}-stranding}
Recall the \textit{whose}-stranding examples from Chapter~\ref{ch:the-stabilisers} (\mention{the guy whose I borrowed car}). This pattern exists. It is attested. It is reproducible. But it is thin.

It fails the homeostasis test. No child hears enough tokens (1 per 100 million words) to acquire a robust rule. There is no social indexing loop (it signals nothing social). There is no functional niche that isn't already better served by pied-piping or paraphrase.

It exists only as a transient \enquote{spillover} effect from the \term{wh-question} mechanism and the \term{stranding} parameter. It is a side-effect, not a substance. If English lost \textit{whose}-stranding tomorrow, no feedback loop would kick in to restore it. It persists only because the mechanisms that generate it (general extraction constraints) haven't changed. It is a category without an immune system.

\section{Fat clustering: The wastebasket}
\label{sec:8:fat}

The second failure mode is the opposite: too much mechanism.

A \term{fat} category is a label that lumps distinct causal clusters into a single bin. The payoff game treats them as one (usually for \enquote{wastebasket} reasons: \enquote{everything that isn't X goes here}), but the mechanisms maintaining them are radically different.

\subsection{The illusion of unity}
Fat categories create an illusion of understanding. We have a label, so we think we have a kind. But seeing the label prevents us from seeing the mechanics.

\subsection{Case study: The \term{Adverb}}
The traditional category \term{adverb} is the paradigmatic fat category. As noted in introductory textbooks, it is the dustbin of the parts of speech. It contains:
\begin{itemize}
    \item \textbf{Manner adverbs} (\mention{quickly}): deeply integrated into the verb phrase, open class, maintained by productive morphology (\textit{-ly}).
    \item \textbf{Degree words} (\mention{very}): modifiers of adjectives/adverbs, closed class, maintained by high-frequency entrenchment.
    \item \textbf{Sentence adverbs} (\mention{unfortunately}): scope over propositions, prosodically detached, maintained by discourse-level alignment.
\end{itemize}

Why group them? Because they share a negative property: \emph{they are modifiers that are not adjectives.} The payoff game of traditional grammar needs a bin for \enquote{non-adjectival modifier.}

But if you try to treat \term{adverb} as a single HPC, you fail. Projectibility breaks down. Learning the distribution of \mention{quickly} (it can appear sentence-finally) generates false predictions for \mention{very} (*\textit{It was good very}). Intervening on the \textit{-ly} mechanism (e.g., teaching children to drop it) affects manner adverbs but leaves degree words untouched.

The category \term{adverb} is not an HPC. It is a taxonomy. The actual HPCs are the sub-clusters: \term{Manner Adverb} is a kind; \term{Degree Word} is a kind. The lump is a fiction.

\section{Negative diagnosis}
\label{sec:8:negative}

A \term{negative} category is defined by what it is not. It is a complement class: \mention{everything that isn't X.} 

In set theory, complement classes are well-behaved. In causal mechanism terms, they are incoherent. There is no such thing as a \enquote{homeostatic property cluster of absence.} The absence of a property is typically not a property, and certainly not one that mechanisms can stabilize.

\subsection{The illusion of the non-finite}

Consider the category \term{non-finite verb}. In traditional grammar, it groups infinitives (\mention{to eat}), bare stems (\mention{eat}), past participles (\mention{eaten}), and gerund-participles (\mention{eating}). 

What do they share? 
\begin{itemize}
    \item They lack tense marking.
    \item They lack agreement marking (mostly).
    \item They cannot head a main clause (mostly).
\end{itemize}

They share failures. But causally, they are radically different animals. The infinitive is stabilized by specific selectional patterns (control verbs, purposeful adjuncts). The past participle is stabilized by the perfect auxiliary system and passive voice constructions. The gerund-participle is maintained by progressive aspect and nominalization strategies.

There is no \enquote{non-finite mechanism} that maintains them all. There is no feedback loop that keeps infinitives and participles similar to each other. They are grouped only by their failure to be finite. 

If you try to project from the class \term{non-finite}, you fail. Learning the distribution of infinitives tells you nothing about the distribution of past participles. They are not a natural kind; they are a bucket.

\section{The grain of analysis}
\label{sec:8:grain}

This brings us to the most difficult failure mode: the problem of grain. Even when we have genuine HPCs, we often group them into larger systems labeled \enquote{Grammar} or \enquote{Language.} 

Is \term{English Morphosyntax} an HPC kind?

The temptation is to say yes. It's stable, it's learned, it's maintained. But this is the \term{Madagascar fallacy}.

\subsection{The Madagascar analogy}

Biological species are the paradigmatic HPC kinds. \textit{Lemur catta} (the ring-tailed lemur) is a homeostatic cluster maintained by interbreeding, shared ecology, and genetic transmission.

But consider \enquote{the fauna of Madagascar.} It is a stable collection of animals. It is distinct from the fauna of Australia. It has a causal history (isolation). But \enquote{the fauna of Madagascar} is not itself a species. It is an \emph{ecosystem} of species. There is no single mechanism that maintains the \enquote{fauna} as a unit; there are thousands of mechanisms maintaining the individual populations that comprise it.

\subsection{Constructions are the kinds}

Language is an ecosystem. The HPCs are the \emph{components}, not the whole.

The phoneme /\textipa{T}/ is an HPC (stabilized by articulatory targets and contrasts). The word \term{\textit{dog}} is an HPC (stabilized by lexical entrenchment). The construction \term{\textit{let alone}} is an HPC (stabilized by a cue bundle of string, syntax, and scalar semantics).

But \enquote{English Grammar} is just the island where they all live. It is the \term{constructicon}~-- the population of constructional species inhabiting a speech community. 

This gives us the answer to the grain question. The level of analysis where HPC theory applies is the level of the \term{individual form-meaning pairing}. 
\begin{itemize}
    \item \term{Noun} is likely a fat category (an umbrella for many specific nominal constructions).
    \item \term{Count Noun} is a plausible HPC (a specific sub-family of constructions with shared maintenance).
    \item \term{The construction \textit{let alone}} is definitely an HPC.
    \item \term{English} is not an HPC; it's the fauna.
\end{itemize}

The discipline of the framework forces us to go local. We find the mechanisms where they live: in the specific transmission of specific patterns. When we lump them too high, we lose the causal thread.

\section{Principled criteria: The two-diagnostic test}
\label{sec:8:criteria}

How do we prevent the HPC framework from sliding into the inflation problem? We replace intuition with diagnostics.

In this book, and in recent work extending the framework \autocite{Reynolds2025Stack}, we have developed two primary diagnostics. They correspond to the two faces of the HPC definition: the epistemic pay-off (induction) and the causal source (maintenance). To warrant the claim that a linguistic category is an HPC kind, it must pass both.

\begin{enumerate}
    \item \textbf{The Projectibility Diagnostic (Epistemic)}
    \emph{Can we predict new data from old?}
    
    If a category is a kind, learning its properties from a finite sample should allow you to predict unobserved instances. 
    \begin{itemize}
        \item \textbf{Test:} Train a model on one dataset (e.g., a corpus, a time period, a set of languages). Test it on a held-out set. 
        \item \textbf{Success:} Performance significantly exceeds baseline (e.g., shuffled labels). The pattern travels.
        \item \textbf{Failure:} The model overfits or collapses on new data. The pattern is local, accidental, or illusory.
    \end{itemize}

    \item \textbf{The Homeostasis Diagnostic (Ontological)}
    \emph{Can we name the stabilizers?}
    
    If a category is a kind, its stability must be causal, not accidental. We must be able to identify the specific feedback loops maintaining it.
    \begin{itemize}
        \item \textbf{Test:} Identify the mechanisms (acquisition, frequency entrenchment, social indexing, functional pressure). 
        \item \textbf{Success:} We can trace the causal chain. We can predict what would happen if a stabilizer were removed (perturbation).
        \item \textbf{Failure:} We can only name the pattern, not its cause. Or the mechanisms we find are heterogeneous (the \term{fat} failure) or absent (the \term{thin} failure).
    \end{itemize}
\end{enumerate}

\subsection{The Discipline of the Stack}

This test imposes discipline. 

\term{Whose}-stranding fails the Homeostasis diagnostic (no stable transmission). \term{Adverb} fails the Projectibility diagnostic (inference from manner adverbs to degree words fails). \term{Non-finite} fails both.

But \term{phoneme inventories}, \term{lexical items} (even under drift), and \term{constructions} like \textit{let alone} pass. They project, and we can name their stabilizers.

Realism is not about claiming that \emph{everything} is real. It is about having a standard for what counts. The maintenance view provides that standard. It allows us to be realists about the construction and anti-realists about the "grammar." It allows us to distinguish the signal from the noise, not by fiat, but by checking the feedback loops. 

\section{Implications}
\label{sec:8:implications}

If we adopt this discipline, the landscape of linguistics looks different.

We stop fighting about whether \term{Subject} is a universal category. Instead, we ask: is the category \term{Subject} in Language L maintained by the same mechanisms as in Language M? Usually, the answer is no. They are distinct HPCs that happen to look alike (convergent evolution), not the same kind.

We stop worrying about whether \term{grammaticality} is a sharp line. Instead, we ask: is the boundary maintained by a sharp phase transition (an active stabilizer), or is it just a region of uncertainty?

And we stop looking for essences. We start looking for the spinning tops. Because in the end, the only things that persist in a dynamic system are the things that know how to keep themselves spinning.
