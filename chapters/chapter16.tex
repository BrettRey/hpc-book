\chapter{What changes}
\label{ch:what-changes}
\epigraph{At the quantum scale there are no cats; at scales appropriate for astrophysics there are no mountains.}{â€” James Ladyman \& Don Ross, \textit{Every Thing Must Go} (2007)}

% NOTE: Chapter 6 (sec:6:what-framework-offers) references this chapter for agent-based modelling
%       as a methodological consequence of the maintenance view. See notes/chapter-feedback-deferred.md
%       for planned ABM content: testing basin dynamics, boundary cases, convergence vs homology, etc.

\section{The status of the framework}

What \emph{kind} of kind is an \term{HPC category}? it's a \term{second-order explanatory kind}: a kind whose members are themselves kinds, unified not by shared first-order properties but by a shared explanatory role and stabilization pattern. Its members are things like \term{species}, \term{chemical elements}, \term{phonemes}, and \term{constructions}~-- categories that support projectible generalizations, are stabilized by multiple partially independent mechanisms, and tolerate property variation without collapse.

What makes \term{HPC category} itself a kind is that these properties cluster reliably across domains. It isn't a natural kind in the same sense as \term{gold}, nor a merely stipulative classificatory label, nor a purely philosophical artifact. it's a meta-level homeostatic cluster over explanatory practices.

This recursion is \emph{typed}, not flat. Compare \term{gene} (a kind whose instances are molecular entities) with \term{functional gene} (a kind whose instances are ways of carving interactions for explanation), or \term{model organism} (a kind whose instances are organisms plus institutional practices). None of these collapses because they occupy different explanatory grains. Likewise, \term{noun} is an HPC kind in English; \term{lexical category} is an HPC kind across languages; and \term{HPC category} is an HPC kind across scientific taxonomies.

Crucially, the higher-order kind isn't stabilized by the same mechanisms as its members. HPC categories \emph{in the world} are stabilized by causal mechanisms, developmental pathways, and communicative coordination. The \term{HPC category} kind is stabilized by repeated success of explanation, methodological convergence across sciences, robustness under theory change, and the survival of the concept under critical scrutiny (against eliminativism or essentialism). Its homeostasis is epistemic and methodological rather than biological or physical.

This framing distinguishes the theory from a mere framework or model. Frameworks can be swapped without residue; HPC categories resist that. Once the pattern is identified~-- why strict definitions fail, why exceptions cluster~-- we can make reliable predictions about where gradience will appear, where sharp boundaries will re-emerge, and where eliminativist arguments will systematically overreach. That predictive success is exactly what licenses kindhood.

A natural objection is that if \term{HPC category} is itself an HPC kind, the theory is self-validating or circular. This would be true only if HPC theory claimed \emph{a priori} necessity. It doesn't. It makes a fallible empirical claim about the structure of successful scientific kinds. If HPC categories stopped supporting prediction, coordination, or explanation, the kind would dissolve~-- by its own lights. That isn't circularity; it's reflexive risk.

\section{Overlap as principled}
\label{sec:15:overlap}

When physicists calculate the motion of a planet, they face a choice. They can treat the planet as a single coherent object, or as a vast aggregate of $10^{50}$ atoms. For the theory to be consistent, the answer must be the same either way.

Newton worried about this. As \textcite{barandes2024} notes, this mereological consistency constraint~-- the requirement that the laws work equally well for parts and for wholes~-- acts as a theoretical guardrail. It forces the inclusion of Newton's Third Law: for every internal action between particles, there must be an equal and opposite reaction. Without this, the internal forces wouldn't cancel out, and a composite object could accelerate itself just by the interaction of its parts. Mereology constrains physics.

Linguistics has no such guardrail.

Instead, we have a loose federalism. We divide the field into sub-disciplines~-- phonetics, phonology, morphology, syntax, semantics, pragmatics~-- and treat them as distinct territories. We talk about \enquote{interfaces} (the syntax--semantics interface, the phonology--morphology interface) as if they were national borders to be policed rather than contact zones to be mapped. We assume a tree-like structure: a phenomenon belongs to syntax \emph{or} semantics, but ideally not both.

The HPC framework suggests this picture is wrong.

If linguistic categories are homeostatic property clusters, then disciplinary subfields are too. \term{Psycholinguistics}, \term{Construction Grammar}, and \term{Experimental Semantics} aren't mutually exclusive territories. They are bundles of phenomena, methods, and theoretical commitments, stabilized by institutional mechanisms (journals, hiring lines, training pipelines). And crucially, these bundles overlap.

\subsection{Typed parthood}

The mistake is assuming that parthood is a single relation. In standard hierarchies, if $A$ is part of $B$, it's part of $B$ generally. But in scientific practice, parthood is \term{typed}.

A subfield can be part of linguistics in different ways:
\begin{itemize}
    \item \textbf{Phenomenon-part ($\leq_\textsc{phen}$)}: Its domain of inquiry is a subset of language (e.g., phonetics studies speech sounds).
    \item \textbf{Method-part ($\leq_\textsc{meth}$)}: Its tools are a subset of the field's toolkit (e.g., corpus linguistics uses distributional analysis).
    \item \textbf{Theory-part ($\leq_\textsc{thy}$)}: Its explanatory commitments inherit from a broader framework (e.g., Minimalism is a theoretical part of Generative Grammar).
\end{itemize}

Once we distinguish these types, the \enquote{boundary disputes} resolve into structural features.

Take \term{Computational Linguistics}. Is it part of linguistics? Methodologically, yes: it contributes formal and algorithmic tools. Theoretically, often no: its goals (engineering performance) diverge from the core explanatory project (cognitive realism). it's a method-part that isn't always a theory-part.

Take \term{Construction Grammar}. it's a theoretical framework ($\leq_\textsc{thy}$) that claims all of morphosyntax as its phenomenon-domain ($\leq_\textsc{phen}$). This puts it in direct competition with Minimalism for the same territory.

Take the \term{syntax--semantics interface}. In the tree view, this is a border. In the typed-parthood view, it's a zone of \term{principled overlap}. Many linguistic phenomena cluster here because the mechanisms are bidirectional. Syntactic structure cues semantic interpretation (comprehension); semantic intent licenses syntactic choice (production). The two systems maintain each other. The overlap isn't a messy intermediate zone to be purified; it's the engine of the system.

The verbless clause puzzle from Chapter~\ref{ch:essentialism} is a textbook instance of typed overlap. What looked like \textit{CGEL}'s global inconsistency~-- defining clauses by VP-headedness but then identifying verbless clauses by subject--predicate structure~-- turns out to be the diagnostic signature of a kind whose markers come from two braided mechanisms. VP-headedness is one stabilizer; predication structure is another. At the core they converge; at the margins they decouple. The inconsistency was the evidence.

\subsection{Consequences for practice}

This re-framing has concrete consequences for how we work.

First, it explains \textbf{peer review friction}. When a paper on \enquote{experimental syntax} is reviewed, it often faces a double bind. Syntacticians (judging by $\leq_\textsc{thy}$) may find the theoretical contribution thin. Psycholinguists (judging by $\leq_\textsc{meth}$) may find the experimental design naive. The paper is attempting a fusion of two bundles. The friction isn't just grumpiness; it's a clash of validation criteria. Recognizing typed parthood allows editors to assign reviewers who are competent in the specific intersection being claimed.

Second, it validates \textbf{methodological pluralism}. If subfields are valid HPCs, then \enquote{pure} linguistics is just one bundle among many~-- usually the bundle that privileges introspective data and structural economy. Other bundles (corpus linguistics, sociolinguistics) privilege different stabilizers (usage data, social variation). These aren't failed attempts at pure linguistics; they are different cuts through the same multidimensional reality. This answers \textcite{nefdt2023}'s worry that methodological pluralism leads to ontological chaos. It doesn't~-- as long as the methods all map to valid homeostatic clusters. Unity lies in shared criteria for kindhood, not in a shared ontology of objects.

Finally, it recovers \textbf{intensional mereology}. Just as Newton asked what makes a planet a single object (internal forces cancelling out), we can ask what makes a subfield a genuine whole. Why does \term{Sociophonetics} exist as a stable cluster, while \term{Generative Phonetics} never quite cohered? Because the mechanisms aligned. Social variation turns out to be deeply entangled with phonetic detail. The phenomena cluster naturally.

The HPC framework doesn't just categorize our data. It categorizes us. And it suggests that the messy, overlapping map of modern linguistics isn't a sign of immaturity, but an accurate reflection of a system where everything is braided.

\subsection{Formalisms as overlapping HPCs}
\label{sec:16:formalisms}

The same logic applies to rival syntactic formalisms~-- not just rival subfields.

For decades, generative syntax has argued about the right representational primitives. Phrase-structure grammars represent sentences as hierarchies of constituents: noun phrases, verb phrases, clauses nested inside each other. Dependency grammars, like \textcite{gibson2025}'s, represent sentences as networks of relations: subject-of, object-of, modifier-of~-- linking words directly without intermediate phrasal nodes.

The rivalry is old. Phrase-structure adherents emphasise that constituents pass substitution, coordination, and movement tests~-- you can replace \mention{the cat} with \mention{it}, front it, conjoin it with \mention{the dog}. Dependency adherents emphasise that dependencies predict processing difficulty~-- longer dependencies impose memory costs, and languages minimise them \citep{gibson2000,futrell2020}. Each side treats its primitives as foundational; each finds the other's primitives derivative or epiphenomenal.

The HPC framework dissolves this debate.

Both formalisms are tracking real structure, maintained by mechanisms, projectible for different purposes. Neither is epiphenomenal; neither is foundational. They are overlapping HPCs~-- different cuts through the same multidimensional linguistic reality.

\term{Constituency} is a mechanism-maintained cluster. What keeps it together? Substitution, movement, and coordination~-- the facts that expressions pronominalize together, front together, and conjoin together. These aren't just tests for constituency; they are the mechanisms that maintain the clustering. Knowing that \mention{the cat} behaves as a unit lets you predict it will replace with \mention{it}, move to topic position, and coordinate with \mention{the dog}. The category projects for questions about \emph{what groups with what}.

\term{Dependency} is a different mechanism-maintained cluster. What keeps it together? Processing constraints~-- working memory tracks incomplete dependencies, and integration cost increases with distance \citep{gibson2000}. Knowing that a verb governs a distant subject lets you predict processing slowdown, reading-time inflation, and comprehension difficulty. The category projects for questions about \emph{what costs what}.

The extensions overlap completely~-- every sentence can be described both ways~-- but the HPCs are distinct. A syntactician asking \enquote{what can move together?} needs constituency. A psycholinguist asking \enquote{why is this sentence hard?} needs dependency. A typologist asking \enquote{why do languages prefer short dependencies?} needs the dependency formalism to state the generalisation \citep{futrell2020}. A constructionist asking \enquote{what licenses this extraction?} needs constituency to identify the relevant structural configuration.

This is precisely the typed-parthood pattern from the previous subsection. The two formalisms are method-parts of syntax ($\leq_\textsc{meth}$) with different theoretical commitments ($\leq_\textsc{thy}$) about which primitives are explanatorily primary. But the HPC framework reveals that neither is primary. Both are real. The question isn't \enquote{which is the true grammar?} but \enquote{which cluster do you need for the question you're asking?}

The practical consequence: stop fighting the wrong war. Phrase-structure grammarians and dependency grammarians aren't competitors for a single prize; they're cartographers of different terrains that happen to share a geography. A complete account of syntax will need both~-- constituency for distributional generalisations, dependency for processing predictions~-- just as a complete account of linguistic categories needs both semantic and morphosyntactic HPCs (Chapter~\ref{ch:countability}).

This is what the maintenance view offers: not a verdict, but a dissolution. The debate was undecided not because the evidence was insufficient but because the question was malformed. Once you ask \enquote{what maintains the cluster?} rather than \enquote{what is the essence?}, the rivalry evaporates.

\subsection{Agent-based modeling}

If linguistic categories are homeostatic property clusters, then agent-based modeling is their natural computational laboratory.

The logic is straightforward. HPCs are maintained by mechanisms operating across agents and timescales: acquisition shapes the priors of new learners; entrenchment solidifies frequently encountered patterns; alignment coordinates interlocutors in real time; transmission filters for learnability across generations; institutional norms impose top-down pressure. These mechanisms interact. Their interactions can be subtle. And the system-level outcomes~-- stable categories, graded boundaries, drift when stabilizers weaken~-- are exactly what emergent dynamics in agent-based models are designed to capture.

Consider countability (Chapter~\ref{ch:countability}). The count cluster is a bundle of morphosyntactic properties~-- article selection, numeral compatibility, \mention{many}/\mention{few}, agreement~-- that cohere in an implicational hierarchy. The stabilizers are explicit: bidirectional inference couples count marking to individuation; entrenchment anchors core cases; functional alternatives (singulatives, measure constructions) absorb pressure at the margins. This is precisely the architecture an agent-based model can operationalise. Give agents lexicons with individuation confidence parameters; let constructions act as locks with tolerance thresholds; have agents update their parameters based on observed usage. Run the simulation.

What emerges isn't merely a replication of what we already knew. The model makes predictions. Weaken the singulative anchor (let \mention{datum} die) and the system drifts: agreement loosens first (\mention{this data is}), tight properties like low numerals remain unavailable, and the noun settles into quasi-count equilibrium~-- exactly the pattern we observe. Introduce prescriptive pressure (editors penalising \mention{three Legos}) and the model shows why such campaigns typically fail: the cognitive economy of count packaging for discrete objects overwhelms institutional feedback unless that feedback is implausibly strong and omnipresent. The LEGO Company, on this account, is fighting against the stabilizers rather than with them.

More generally, agent-based models provide three kinds of value for the HPC framework:

\begin{enumerate}
    \item \textbf{Sanity checks on mechanism claims.} If we claim that alignment and transmission are jointly sufficient to maintain a category, the model can show whether those mechanisms produce stable clustering or whether additional stabilizers are required. Failure to converge is informative: it means the mechanism story is incomplete.
    \item \textbf{Predictions about boundary cases.} The model can locate the parameter regimes where categories should fray, where intermediate equilibria should appear, and where drift should accelerate. These are testable predictions about corpus distributions and acceptability gradients.
    \item \textbf{Contrastive explanations.} Why does English maintain a tight count/mass distinction while Mandarin relies on classifiers? Why do quasi-count nouns like \mention{cattle} stabilise rather than regularising? Agent-based models can simulate both outcomes and show which stabilizer configurations produce which results.
\end{enumerate}

The framework isn't committed to any particular computational architecture. What matters is that the model respects the multi-timescale, multi-mechanism, multi-agent structure that HPCs require. A single-agent model of grammatical competence can't capture alignment; a model without transmission can't capture the filtering effects of iterated learning; a model without institutional agents can't capture prescriptive pressure or register stratification. The HPC story constrains the simulation the same way mereological consistency constrains Newtonian mechanics: not by dictating the equations, but by requiring that parts and wholes cohere.

This isn't a call for linguistics to become computational. it's a claim about what computation is good for. Agent-based models are the natural testing ground for multi-mechanism claims precisely because they force explicitness. You can't simulate entrenchment without specifying what updates and how fast. You can't simulate alignment without specifying what agents observe and what they infer. The model is a proof that the verbal story has enough structure to generate the phenomena~-- or a diagnosis of where it doesn't.


\section{No level privilege}
\label{sec:15:no-level-privilege}

I don't assume that linguistic kinds must be grounded at a uniquely fundamental level of description. On a homeostatic property cluster view, the standing of a category isn't conferred by its position in a hierarchy~-- phonetic, phonological, morphosyntactic, semantic, discourse~-- but by whether it supports projectible generalizations and whether there is a plausible account of the stabilizers that keep its properties clustered. Different explanatory projects may legitimately treat different grains as locally foundational, and those choices can be evaluated empirically.

This pluralism is constrained. Some proposed categories correspond to robust clusters sustained by recognizable mechanisms; others are weak, local, or artefactual. The point isn't that any category is as good as any other, but that no level is privileged in advance. The \term{meta-Occam} principle from Chapter~\ref{ch:kinds-without-essences} applies here: we should expect parsimony in the stabilizers, not in the categories. Linguists can stop expecting tidy category definitions and start expecting tidy mechanism inventories.

\subsection{What earns foundational status}

A category is a better candidate for kindhood~-- and a better foundation for a local explanatory project~-- when it shows more of the following:

\begin{enumerate}
    \item \textbf{Cluster stability across contexts.} The cluster persists across speakers, registers, tasks, and modest perturbations in methodology.
    \item \textbf{Projectibility.} It supports reliable generalizations (including predictable failure modes) beyond the dataset that suggested it.
    \item \textbf{Mechanistic anchoring.} You can point to stabilizers~-- learning biases, articulatory or perceptual constraints, communicative pressures, institutional norms, processing limitations, interactional routines~-- that make the cluster non-accidental.
    \item \textbf{Cross-level consilience.} It's compatible with adjacent-level regularities without being reducible to them; tensions are diagnostically useful rather than merely inconsistent.
    \item \textbf{Intervention sensitivity.} Changing relevant conditions predictably shifts the cluster (even if the intervention is only observational or quasi-experimental).
    \item \textbf{Typological and diachronic tractability.} It supports comparative work without collapsing into stipulation.
\end{enumerate}

These criteria are graded, not binary. A category can score high on some dimensions and low on others. The framework doesn't issue licenses; it calibrates confidence.

\section{The nominalist challenge revisited}
\label{sec:15:nominalism}

This returns us to the challenge from Chapter~\ref{ch:what-we-havent-been-asking}. Haspelmath and Croft argued that because categories lack definitions, they can't be cross-linguistically real (Haspelmath) or even language-internally global (Croft). We are now in a position to see what they got right, and where the HPC framework allows us to go further.

They were right about the failure of definitions. If \textsc{noun} requires a set of necessary and sufficient conditions that holds across all languages, then \textsc{noun} doesn't exist. If \textsc{subject} requires a definition that covers every construction in English without exception, then \textsc{subject} doesn't exist.

But they were wrong to conclude that the only alternative is stipulation.

To Haspelmath, we can now say: \term{Comparative concept}s aren't merely yardsticks we invent. They are \term{basin recognition}. When we find that the concept \textsc{adjective} is useful for describing unrelated languages, it isn't because we have forced the data into a box, but because the functional pressure to modify referents creates a recurring stabilizer basin. Languages slide into this basin repeatedly. The cross-linguistic category is real not because it has an essence, but because the forces that shape grammars are themselves consistent. The \enquote{comparative concept} tracks a mechanism-maintained attractor in the design space.

To Croft, we can say: \term{Construction}s are indeed the primary units of form-meaning pairing, but they aren't islands. They are braided together by economy and alignment. \textsc{subject} is real in English not because it's a Platonic essence instantiated in every clause, but because a massive web of constructions has stabilized around a shared anchor. The properties cluster because the mechanisms of learning and production favour reuse. The \enquote{generalization} isn't a fiction of the analyst; it's the causal adhesive of the grammar.

To both, we can say: The complexity of sociolinguistic variation isn't evidence against categories, but evidence of \term{mixture}. Speakers don't condition on a single homogeneous grammar but on latent variables~-- \term{discourse communities}~-- that shift the parameters. In Bayesian terms, the learner infers a mixture component $C$ and learns $P(Y \mid C)$. The category exists within the component. The fact that \mention{I done it} is grammatical in one community and ungrammatical in another doesn't mean the category \textsc{Perfect} is a fiction; it means it's parameterised by a latent variable. Social meaning, then, is simply the inverse of this conditioning: if production is $P(Y \mid C)$, then social indexicality is the inference $P(C \mid Y)$. Structure and social meaning are the same math read in opposite directions.

Nominalism was a necessary corrective to essentialist overreach. But it threw out the baby with the bathwater. We can admit that categories are constructed~-- built by history, maintained by interaction, variable at the margins~-- without admitting they are arbitrary. They are \term{maintained kinds}. And because they are maintained, they are real.

\section{Conclusion: The zipper at scale}
\label{sec:15:zipper-at-scale}

We ended Chapter~\ref{ch:the-category-zipper} with the image of grammatical categories as zippers~-- mechanisms that couple distinct feature systems into functional alignments. The form-side teeth (morphosyntax) lock into the meaning-side teeth (semantics) not because they are perfectly identical, but because the coupling is tight enough to hold.

Disciplinary unity works the same way.

The "syntax--semantics interface" isn't a line on a map. it's a zipper. Syntax involves one cluster of mechanisms (combinatorial efficiency, structural parsing); semantics involves another (compositionality, inference). They are autonomous systems, maintained by different pressures. But they are coupled. Communication forces them into alignment. The subfields that study them~-- and the theories we build~-- are attempts to describe that coupling.

If we treat \term{Syntax} and \term{Semantics} as essentialist territories with fixed borders, the mismatch will always look like a failure of theory. If we treat them as homeostatic property clusters, coupled by functional necessity, the mismatch is exactly what we expect. This integration relies on \term{reciprocity}. Syntax cues meaning; meaning selects structure. The feedback loop between them is what stabilizes both. As \textcite{nefdt2023} argues, following \textcite{dennett1991}, linguistic structures are \term{real patterns} because they are efficient compressions of this reciprocal data. Syntax doesn't need to access the full causal density of a semantic category; it only needs the compressed schema~-- the \term{value}. The zipper works because the teeth are simplified encodings, not raw complexity. The field of linguistics holds together for the same reason language does: not because it has a single essence, but because its parts are zipped together by the work they do.

The words, it turns out, never did hold still. We just mistook careful work for effortless fact. What looked like stasis was maintenance all the way down: mechanisms spinning categories into being, holding them upright long enough for speakers to learn them, and letting them drift when the work stopped. The top keeps spinning because something keeps pushing.