\chapter{What changes}
\label{ch:what-changes}
\epigraph{At the quantum scale there are no cats; at scales appropriate for astrophysics there are no mountains.}{â€” James Ladyman \& Don Ross, \textit{Every Thing Must Go} (2007)}

\section{Introduction: the gauntlet}
\label{sec:16:intro-gauntlet}

A theory that survives every possible result explains nothing.

For most of this book, the argument has been constructive. I argued that strict definitions fail, that mechanisms do explanatory work, and that projectibility is indexed to purpose (Chapter~\ref{ch:projectibility}). Then the case studies did their part: countability, definiteness, lexical categories, gender, grammaticality, social stabilization.

Now comes the expensive part. A framework earns its keep only when it names the outcomes that would kill it.

So this final chapter is short on purpose. It doesn't add another slogan. It states a gauntlet: what this view predicts, what would falsify it, and what work follows if we take those risks seriously.

One running case will keep this honest: English \mention{data}. It behaves count-like in some contexts and mass-like in others. That boundary behaviour is not a side issue. It's the test bench.

Three diagnostics make the case concrete from the start: agreement (\mention{the data are} vs \mention{the data is}), tight count packaging (\mention{a datum}, \mention{three data}), and loose packaging (\mention{many data} vs \mention{much data}).

\section{No level privilege, no level immunity}
\label{sec:16:no-level-privilege}

The framework keeps one structural claim from start to finish: no level is privileged in advance. A category can be real at phonological, morphosyntactic, semantic, discourse, or social grain, provided it is maintained and projectible at that grain.

But \enquote{no level privilege} never meant \enquote{anything goes}. It means no level gets a free pass.

Every proposed category owes the same three debts. First, show a profile that actually clusters. Second, identify stabilizers that could maintain that clustering. Third, show forward projection~-- what learning one case buys you for the next.

That is the discipline behind the flexibility. Traditional labels don't get grandfathered in. New decompositions don't get rewarded just for being novel. The \term{meta-Occam} principle from Chapter~\ref{ch:kinds-without-essences} still applies: be parsimonious about mechanisms, not about labels.

This also means comparative concepts can stay useful without guaranteeing a language-particular category in every grammar. We can compare \term{adjective}-like behaviour across languages even where no dedicated adjective category is maintained inside a given language.
That is a null case, not a framework failure: comparative structure can be projectible even when a matching language-particular category is absent.

\section{The empirical commitments}
\label{sec:16:empirical-commitments}

The maintenance view is only worth anything if it yields risky expectations. Here are two.

\subsection{Prediction 1: Fraying is asymmetric, not random}
\label{sec:16:prediction-fraying}

Margins fray. But they don't fray at random.

When stabilizers uncouple, we shouldn't see all diagnostics collapse at once, and we shouldn't see all edge items wobble in the same direction. We should see patterned decay keyed to which mechanism weakened first.

Chapter~\ref{ch:countability} gave the template: when a noun's anchor weakens, it doesn't jump instantly from one clean box to another. Some cues destabilize first; others hold. Chapter~\ref{ch:definiteness-and-deitality} showed the same architecture with a different split: semantic and morphosyntactic behaviour can drift out of step in constrained ways. Chapter~\ref{ch:lexical-categories} showed the typological analogue: edges fray where support is thin.

The measurement signature is concrete. At population level, boundary regions should show graded distributions. At individual level, speakers should still show mostly sharp commitments, with variance concentrated \emph{between} speakers more than \emph{within} speakers. If fraying turns out to be uniform free noise across diagnostics and speakers, the maintenance account has overfit.

In the \mention{data} case, the framework predicts specific decouplings: tight frames should erode first as singulative anchoring weakens, while looser frames and agreement can remain stable longer. The key point is not \enquote{some variation}. The key point is structured non-synchrony.

\subsection{Prediction 2: Drift has an order (the sticky-lock effect)}
\label{sec:16:prediction-sticky-lock}

A second prediction concerns time. Drift should have an order.

The expected sequence is this: usage and semantic pressure move first; morphosyntactic locks lag; then those locks fail in bursts rather than smoothing out forever. The lock is \enquote{sticky}: it can persist after the pressure profile that built it has shifted.

This is a diachronic and cross-linguistic claim. It predicts staged decoupling, not instant category death and not indefinite linear blur. It also predicts homology without identity: unrelated languages under similar communicative and processing pressures should converge on similar local fixes, even when the material details differ.

The lag claim is architectural, not hierarchical. Semantic, morphosyntactic, and discourse representations can update at different rates without any one level being ontologically primary.

Scope matters here, and it has three tiers. Language-particular predictions target concrete diagnostics inside one grammar. Comparative predictions target recurring \emph{shapes} of drift across grammars. Constructional predictions target specific form--meaning pairings regardless of lexical label, such as competition between \mention{these data are} and \mention{this data is} as partially distinct constructions.

Again, \mention{data} is useful as a stress test. If anchor loss (\mention{datum} recession) yields staged erosion~-- tight before loose~-- we get sticky lag. If everything moves in lockstep with no lag and no local bursts, the prediction fails.

Methodologically, this means trajectories, not snapshots. The right tools are timeline corpora, age-graded acquisition data, perturbation designs, and multi-timescale simulations (Chapter~\ref{ch:projectibility}).
For \mention{data}, this yields a developmental ordering prediction: in cohorts where \mention{datum} is absent from child-directed input, loose packaging (\mention{many data}) should stabilize before tight singulative packaging (\mention{a datum}).

\section{Falsification conditions}
\label{sec:16:falsification}

These aren't decorative falsifiers. They are defeat conditions.

\subsection{Falsifier A: The essentialist victory}
\label{sec:16:falsifier-essentialist}

Find a robust, cross-linguistically stable grammatical category that projects strongly while showing no identifiable maintenance structure: no acquisition bias, no alignment pressure, no functional anchoring, no institutional reinforcement, no attractor-like distributional profile. If that result survives replication, maintenance theory is wrong. Essentialism wins.

\subsection{Falsifier B: The radical nominalist victory}
\label{sec:16:falsifier-nominalist}

Show that usage variation is effectively a random walk once sampling noise is controlled: no reliable conditioning by frequency, no alignment effects, no stable register or community attractors, no basin-like recovery after perturbation. If categories don't predict beyond convenience labels, maintenance theory is wrong. Nominalism wins.

Neither defeat condition is a one-off anomaly test. Both require durable, cross-dataset failure of the mechanism~-- projection link. But if that failure arrives, the framework should be retired, not patched.

\subsection{Serious downgrading before full defeat}
\label{sec:16:serious-downgrade}

Total defeat is not the only informative outcome. There is also serious downgrading.

Partial pressure is expected in any live research program: one mechanism claim can fail in one domain while others still carry predictive weight elsewhere. That is correction, not collapse.

Serious downgrading is stronger. If repeated studies show weak or inconsistent support for patterned fraying, no reliable lag structure in drift, and no predictive lift over convenience labels in major domains (\eg\ countability, definiteness, lexical categories), then the framework should be demoted from explanatory ontology to heuristic vocabulary. That is not \enquote{business as usual}. It is a substantive loss of status.

\section{Operationalizing the gauntlet}
\label{sec:16:operationalizing}

To keep the gauntlet empirical, each claim needs an observable and a defeat threshold.

For the running case \mention{data}, the core variables are straightforward: agreement choice, tight-frame acceptance (\mention{a datum}, low cardinals), and loose-frame acceptance (\mention{many} vs \mention{much}), conditioned by register, community, cohort, and constructional context.

\begin{table}[htbp]
\centering
\caption{Gauntlet claims, observables, and defeat thresholds.}
\label{tab:16:gauntlet}
\small
\begin{tabular}{@{}p{0.20\textwidth}p{0.36\textwidth}p{0.36\textwidth}@{}}
\toprule
Claim & Observable signature & What would count against it \\
\midrule
Asymmetric fraying & Boundary diagnostics drift in structured, non-synchronous ways; variance partitions by speaker/context more than pure item noise. & Diagnostics move as unstructured free noise; no stable variance structure across replications. \\
\addlinespace
Sticky-lock drift & Diachronic lag between usage/semantic shift and morphosyntactic lock shift; local burst dynamics near boundary transitions. & No lag and no burst structure: all dimensions move in lockstep or diffuse linearly without staged decoupling. \\
\addlinespace
Essentialist falsifier & Category projects strongly under perturbation despite no identifiable maintenance architecture. & Once mechanisms are measured, projection depends on them; no mechanism-free robust projector appears. \\
\addlinespace
Nominalist falsifier & Baseline random-walk or convenience-label models match mechanistic models on out-of-sample prediction and perturbation recovery. & Mechanistic models show persistent predictive lift and recovery signatures absent from random-walk baselines. \\
\bottomrule
\end{tabular}
\end{table}

Operational note. For asymmetric fraying, fit hierarchical models that separate speaker, item, and context variance; the key test is stable variance structure, not residual free noise. For sticky-lock drift, estimate lag with changepoint or state-space methods; staged lag with local burst transitions supports the claim. For the nominalist challenge, compare mechanistic models against random-walk and convenience-label baselines on held-out time slices and perturbation-recovery tasks. If mechanistic predictors repeatedly fail to beat those baselines across domains, that is serious downgrading; if they never beat them in durable replication, that is defeat.
One explicit preregisterable rule can make that concrete: if calibrated mechanistic models fail to clear a 5\% held-out predictive-gain threshold over random-walk baselines across three independent corpus or behavioural datasets in a domain, treat that domain as no-lift.
As Chapters~\ref{ch:projectibility} and \ref{ch:countability} stress, didactic ABMs are mechanism sketches; defeat decisions should rest on replicated empirical datasets and multi-seed sensitivity summaries, not on one toy trajectory.
Within the modeling lane, seed distributions and mechanism-ablation controls should be treated as internal consistency checks, not as standalone evidence of category reality.

\section{Conclusion: the zipper under load}
\label{sec:16:zipper-at-scale}

Chapter~\ref{ch:the-category-zipper} gave an image. This chapter treats it as an instrument.

Put the zipper under load. Ask what keeps the teeth meshed. Ask where slip begins. Ask which perturbation makes the slider skip. Then test those claims across timescales, populations, and methods.

That is the shift. The question is no longer \enquote{what are categories, in themselves?} It is: what keeps this category together, for whom, under which pressures, and for how long?

In practical terms, this changes tomorrow's workflow. Instead of arguing first about labels, we should preregister stabilizer-linked predictions, measure boundary variance explicitly, and treat mismatch patterns as evidence rather than embarrassment. The point is simple: design studies that can make the framework wrong.

If the maintenance story survives that gauntlet, it earns realism. If it doesn't, it should be replaced.

The words never held still. They held together~-- until they didn't.
