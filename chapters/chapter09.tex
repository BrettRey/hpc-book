\chapter{Countability}
\label{ch:countability}

% Chapter 9: First Part III case study
% Target: ~8,000 words
% Metaphor: Lock-and-key hook → basin integration

\epigraph{\textit{The word LEGO® is a brand name and is very special to all of us in the LEGO Group Companies. We would sincerely like your help in keeping it special. Please always refer to our bricks as `LEGO Bricks or Toys' and not `LEGOS'.}}{LEGO Group consumer catalog, c.~1980s}

The lawyers at the LEGO Group have been fighting this battle for forty years. They have trademark law on their side. They have the Chicago Manual of Style. They have millions of dollars in brand-management budget.

And they are losing.

To a lawyer, \mention{LEGO} should only ever modify~-- \mention{LEGO bricks}, \mention{LEGO toys}~-- never stand alone as a count noun. To a six-year-old, a plastic brick is a discrete, bounded, manipulable object. And English has a deeply entrenched pattern for dealing with discrete, bounded, manipulable objects: it counts them. It adds an \mention{-s}. It makes them plural. When a child asks for \mention{three Legos}, they aren't making a mistake; they are applying an inference they've learned from thousands of similar cases. The count cluster is claiming the word.

This chapter is about that mechanism~-- the control loop that decides what gets counted. The count/non-count distinction isn't a static binary. It's a dynamic negotiation. And as we'll see, the HPC framework we built in Part II handles this messy distinction cleanly.

Countability, I'll argue, isn't a definition. It's a homeostatic property cluster~-- and it illustrates something general about how HPCs work. At a finer grain, categories often reveal internal structure: sub-clusters maintained by different mechanisms, held together by interface processes. Countability makes this structure visible because the two clusters~-- one semantic (individuation), one morphosyntactic (the count cluster)~-- are saliently distinct. What looks like a single category is a coupling.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{One word, two categories}
\label{sec:9:two-categories}

The word \textit{countability} hides a structural fact. At one grain, it names a single category~-- a coherent cluster that English speakers navigate daily. At a finer grain, it reveals itself as a coupling between two: the \term{individuation cluster} (semantic) and the \term{count cluster} (morphosyntactic). Each is maintained by its own mechanisms. Countability is both the whole and the joint.

This matters because it explains something puzzling. Chapter~\ref{ch:failure-modes} argued that lexical semantics alone rarely produces HPC kinds: categories like \term{furniture} (conceptual similarity) or \term{animacy} (referent property) don't generate the tight clustering that projectibility requires. But countability does. English speakers reliably extend count morphosyntax to novel words, predict quantifier compatibility from plural marking, and judge intermediate cases with surprising consistency. Why?

The answer is that countability isn't just lexical semantics. It's an \textit{interface system}~-- a place where a semantic distinction receives repeated grammatical reinforcement. When constructions select for individuation, when morphology marks it, when processing routines exploit it, when acquisition converges on it~-- then the semantic distinction starts to behave like a kind. This is what makes countability different from \term{furniture}.

The next two sections unpack the two clusters. First, the semantic side: what is individuation, and what maintains it as a cluster of properties? Then, the morphosyntactic side: what is the count cluster, and what keeps its properties together? Only after both are on the table can we ask how they couple.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The individuation cluster}
\label{sec:9:individuation-cluster}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Individuation isn't a single property. It's a cluster of semantic properties that typically travel together~-- what the formal semantics literature calls the accessibility of atoms to quantificational operations, formalized by \citet{link1983} and developed by \citet{chierchia1998}, \citet{rothstein2010}, and \citet{grimm2018}:

The cluster typically comprises four properties. \term{Boundedness} entails that the referent has discrete edges~-- a spatial or conceptual boundary that separates it from its environment. \term{Atomicity} means the referent is accessible as a unit that can be singled out and tracked. \term{Enumerability} makes the referent compatible with exact counting~-- three of them, not just much of it. Finally, \term{homogeneity resistance} means that parts of the referent are not the same kind as the whole: half a cat is not a cat, whereas half of water is still water.

These properties cluster. When a referent is bounded, it's usually atomic; when it's atomic, it's usually enumerable; when it's enumerable, it usually resists homogeneous subdivision. The clustering isn't accidental. But it is not absolute. A \mention{cat} is maximally individuated: bounded, atomic, enumerable. But a \mention{cloud} is bounded while lacking stable atomicity. \mention{Cattle} denotes discrete animals, but the noun construes them as an aggregate. The cluster represents a prototype, not a rigid definition.

\subsection{What maintains the cluster}

The individuation cluster is maintained by perceptual and cognitive mechanisms that operate across modalities~-- and, strikingly, across species.

\paragraph{Edge detection.} Visual object perception depends on detecting boundaries. The visual cortex uses orientation-selective cells, Gabor-like filters, and mid-level grouping principles to extract edges from the visual field. These edges are then grouped into coherent object representations~-- what \citet{kahneman1992} called \term{object files}: temporary episodic representations that bind features to locations and track objects across time.

But edge detection isn't vision-specific. Blind echolocators produce tongue clicks and interpret the returning echoes to perceive objects. Research shows that they can identify object shape, size, and location with remarkable precision \citep{KolarikEtAl2014}~-- and fMRI studies reveal that they activate visual cortex when doing so \citep{ThalerArnottGoodale2011}. The perceptual system has found a different input modality but the same computational goal: bounded individuals.

The pattern extends beyond perception. The macrophages we met in Chapter~\ref{ch:stabilisers} face the same computational problem at a different scale: distinguishing self from non-self. They use pattern recognition receptors~-- toll-like receptors, scavenger receptors~-- to detect molecular edges, boundaries between what belongs in the tissue and what doesn't. Edge detection~-- the computational problem of finding where one thing ends and another begins~-- recurs from molecular biology to perception to cognition. This recurrence is not accidental. It suggests that \term{individuation} is what \citet{dennett1991} calls a \mention{real pattern}: a compression of information that pays for itself by supporting prediction. Whether for a macrophage identifying pathogens or a speaker identifying objects, the computational cost of defining a boundary is outweighed by the inductive utility of the category.

\paragraph{Infant core knowledge.} \citet{spelke2007} argues\footnote{Or should that be \mention{argue}?} that human infants are equipped with core knowledge systems for objects~-- systems that operate from the first months of life. Infants as young as four months perceive objects as cohesive (parts move together), bounded (edges mark identity), and continuous (objects persist through occlusion). These are the same properties that define the individuation cluster in adult conceptual systems. The mechanisms that maintain individuation are not learned from scratch; they're built on an early foundation.

\paragraph{Cross-modal integration.} Object individuation isn't locked to a single sensory channel. When you see a cup and hear it being set down, you don't perceive two entities~-- one visual, one auditory. You perceive one object with multimodal properties. The binding is maintained by cross-modal integration mechanisms that enforce coherence: what looks bounded should sound bounded, feel bounded, behave as a unit. This integration reinforces the cluster. The properties travel together because multiple systems expect them to travel together.


This gradient nature of individuation is crucial for what comes next. The count cluster~-- the morphosyntactic syndrome~-- tracks individuation. When individuation is strong, the full count cluster applies. When it dissolves, the count cluster dissolves with it. But it does so in an orderly way.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The count cluster}
\label{sec:9:count-cluster}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

The distinction seems intuitive. \mention{Cat}, \mention{chair}, and \mention{idea} differ from \mention{water}, \mention{mud}, and \mention{traffic}. The first group denotes discrete individuals; the second denotes undifferentiated substances or aggregates.

Otto Jespersen first formalized the distinction in 1924, coining the term \term{mass-word} for nouns that resist counting:
\begin{quote}
There are a great many words which do not call up the idea of some definite thing with a certain shape or precise limits. I call these `mass-words'; they may be either material, in which case they denote some substance in itself independent of form, such as \term{silver}, \term{water}, \term{butter}, \term{gas}, \term{air}, etc., or else immaterial, such as \term{leisure}, \term{music}, \term{traffic}, \term{success} \citep[198]{jespersen1924}.
\end{quote}
But Jespersen's insight goes back further. The chapter was adapted from a lecture he delivered to the Copenhagen Academy of Sciences in 1911~-- thirteen years before publication, and decades before the cognitive revolution.\footnote{Jespersen also documented the flexibility of individual nouns across count and mass uses: \mention{a little more cheese} vs. \mention{two big cheeses}; \mention{it is hard as iron} vs. \mention{a hot iron}. The phenomenon was recognized a century ago.} Even then, he saw that the grammar doesn't just mirror the world; it imposes a perspective on it. While we can say \mention{two apples} or \mention{two fruits}, we cannot take a magazine and a stethoscope and call them \ungram{\mention{two objects}}~-- unless we first categorize them under a concept that supplies a unit. The individuation is in the construal, not the furniture of reality.

Standard grammars like \textit{CGEL} \citep{huddleston2002} identify a cluster of morphosyntactic properties that typically align. For a prototypical count noun like \mention{cat}:
Standard grammars like \textit{CGEL} \citep{huddleston2002} identify a cluster of morphosyntactic properties that typically align. For a prototypical count noun like \mention{cat}, the properties reinforce the individuation. The noun denotes discrete, bounded entities; it inflects for number (\mention{cats}); it accepts exact cardinals (\mention{one cat}, \mention{two cats}) and fuzzy count quantifiers (\mention{many}, \mention{fewer}); and it triggers agreement matching its morphology.

By contrast, for a prototypical non-count noun like \mention{water}, the cluster prevents individuation. The noun denotes undifferentiated substance; it lacks a plural form (outside special interpretations); it rejects cardinals (*\mention{two waters}) and takes mass quantifiers (\mention{much}, \mention{less}); and it invariably triggers singular agreement.

This is the \term{count cluster}: the set of formal properties that normally travel together. When you hear a word has a plural form, you can predict it will take \mention{many} and reject \mention{much}. You can predict it will refer to something conceptualized as an individual.

\subsection{The problem: Object-mass nouns}

The problem is that the world doesn't always cooperate. \citet{rothstein2010} identified the class of `object-mass nouns'~-- words like \mention{furniture}, \mention{footwear}, \mention{cutlery}, and \mention{jewelry}. Semantically, these refer to discrete, countable artifacts. A chair is as discrete as a cat. Yet \mention{furniture} is grammatically mass:
\begin{exe}
    \ex \ungram{three furnitures}
    \ex \ungram{many furniture}
    \ex \mention{much furniture}
\end{exe}

Why? If the grammar just tracked ontological discreteness, \mention{furniture} should be count. The fact that it isn't~-- that English forces us to say \mention{pieces of furniture} while French speakers comfortably say \mention{meubles} (plural)~-- proves that countability is an autonomous grammatical system, not a direct read-out of physics.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The coupling}
\label{sec:9:coupling}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Now we can ask: what connects the two clusters? Why does the individuation cluster (semantic) and the count cluster (morphosyntactic) behave as a unified system?

The answer is \term{bidirectional inference}~-- the mechanism that couples semantic construal to grammatical form.

Each count property is a cue~-- a piece of morphosyntactic evidence that hearers use to infer how the speaker is construing the referent. When you hear \mention{three dogs}, you infer that the speaker has individuated the referents~-- construed them as discrete, enumerable units. When you hear \mention{much water}, you infer a non-individuated construal: stuff, not atoms. The morphosyntax generates expectations about the construal; the construal, in turn, constrains morphosyntactic choice.

The coupling works in both directions. In comprehension, count morphosyntax (\mention{a}, \mention{three}, \mention{many}) leads the hearer to expect atomic, enumerable referents. In production, an individuated construal prompts the speaker to select count morphosyntax. The properties cluster because they are cues to the same underlying condition.

The properties cluster because they're all coupled to the same semantic variable. Knowing that a noun takes \mention{a(n)} tells you it supports individuation~-- and if it supports individuation, it should take \mention{three} and \mention{many} and plural agreement too. The generalisation runs through the construal, not through the morphosyntax directly. That's why the properties are mutually predictive: not because any one \textit{causes} the others, but because they're all symptoms of the same underlying condition.

\subsection{Why the coupling produces an HPC}

This is where the payoff from Chapter~\ref{ch:failure-modes} arrives. Lexical semantics alone rarely produces HPC kinds: categories like \term{furniture} (conceptual similarity) don't generate the tight clustering that projectibility requires. Why should countability be different?

The answer is that countability operates at the syntax-semantics interface. The individuation cluster is reinforced by perceptual mechanisms (edge detection, object files, cross-modal integration). The count cluster is reinforced by morphosyntactic mechanisms (acquisition, entrenchment, transmission). But the coupling between them~-- the bidirectional inference mechanism~-- provides a third layer of reinforcement. Each time construal and form align, both clusters are strengthened. Each time they're transmitted together to a new learner, the coupling tightens.

\term{Furniture} lacks this reinforcement. The conceptual similarity exists~-- chairs and tables share properties~-- but English provides no morphosyntactic system that selects for that similarity and reinforces it with every utterance. There's no grammatical slot that says `furniture-like things go here'. The category exists in the lexicon but not in the grammar.

Countability is the opposite. The grammar provides slots that select for individuation: \mention{a} requires atomic reference; \mention{three} requires enumerable atoms; \mention{many} tolerates weaker individuation. Every time a speaker uses these constructions, they reinforce the link between semantic construal and grammatical form. This is why countability passes the HPC diagnostics: the coupling mechanism provides the homeostasis that lexical semantics alone cannot.

\subsection{Multi-timescale maintenance}

The bidirectional inference mechanism operates at multiple timescales, each reinforcing the others:

\paragraph{Processing (milliseconds).} Every time a speaker produces or comprehends a count frame, the form--meaning link is activated. \mention{Three dogs} primes the expectation of individuation; individuation primes the expectation of count morphosyntax. Mismatches~-- \ungram{\mention{three furnitures}}, say~-- incur processing costs. They feel wrong because they violate entrenched expectations.

\paragraph{Acquisition (years).} Children don't learn count properties one by one. They learn that count morphosyntax correlates with individuation. Classic acquisition work shows that children overgeneralise: encountering a noun in one count frame, they extend it to others \citep{bloom1994a,gordon1985}. A child who hears \mention{many police} may try \ungram{\mention{three police}}, treating the loose property as evidence for the tight ones. This is exactly what you'd expect if the cluster is acquired as a unit~-- if learners are tracking construal, not memorising property lists.

\paragraph{Semantic to Morphosyntactic (S $\to$ M)}
When a speaker conceptualizes a referent as individuated (bounded, atomic), they reach for the tool that signals individuation: the count syntax.

This is why children say \mention{I saw three sheeps} or \mention{two mouses} or \mention{my foots hurt}. The child perceives discrete animals, discrete body parts. The grammar provides a slot for discrete entities (the regular plural). The child fills the slot, ignoring the lexical exception. Such overgeneralisation is systematic: children treat one count property as evidence for the rest. The error proves the mechanism works~-- they're running the algorithm perfectly, just on irregular input.

\paragraph{Transmission (decades).} Institutional forces~-- style guides, copy editors, teachers, grammar checkers~-- enforce canonical patterns and resist drift. The \mention{data}/\mention{datum} debate lives here. As \mention{datum} recedes from editorial practice, the normative anchor weakens and \mention{data} drifts toward mass status. The institutional layer doesn't \textit{create} the cluster, but it stabilises it at the community level.

The mechanisms interlock: the fast loop generates usage patterns; the slow loop crystallises them into community standards; acquisition transmits the crystallised patterns to new learners, who then participate in the fast loop. Perturbation to any level~-- a semantic shift that weakens individuation, a prescriptive shift that sanctions formerly deviant forms~-- ripples through the system and changes the equilibrium. The cluster isn't static. It's dynamically maintained.

\subsection{The chunking story}

There's a cognitive dimension to why high-frequency patterns resist change. Following \textcite{bybee2010}, we can understand entrenchment in terms of \textit{chunking}: high-frequency sequences get stored as units and accessed as wholes, bypassing compositional assembly.

\mention{Many cattle} is a chunk. English speakers have encountered it often enough that it's stored in memory and retrieved directly. \mention{Three cattle} isn't a chunk~-- it has to be assembled online, and the assembly fails because the components don't fit. The noun's stored profile resists the frame the speaker is trying to build.

This is why quasi-count nouns feel unstable in tight frames but natural in loose ones. \mention{Many cattle} is retrieved; \mention{three cattle} is constructed and rejected. The difference between the two isn't a difference in categorical membership. It's a difference in processing: one path is entrenched; the other isn't.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The hierarchy: Tight before loose}
\label{sec:9:hierarchy}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If all count properties were equally sensitive to individuation, any weakening would collapse the cluster entirely. A noun would be fully count or fully mass, nothing in between. But that's not what we see. \mention{Cattle} and \mention{police} sit in the middle: they accept \textit{some} count properties while rejecting others. And the pattern isn't random. The properties peel off in order.

Here's why. Although all count properties are coupled to individuation, they're not equally demanding. Some require high-precision individuation~-- exact atomic units, sharply bounded. Others tolerate lower precision~-- approximate magnitude, vague plurality. When individuation weakens, the demanding properties fail first.

\subsection{Locks with different tolerances}

Think of it this way. Each count construction~-- \mention{a} N, \mention{three} N, \mention{many} N~-- is a lock with a specific tolerance. The noun provides a key: its individuation profile. If the key clears the tolerance, the construction is licensed.

\mention{A} is a precision lock. It requires identifying exactly one atomic unit. The key must be cut to exact specifications.

\mention{Three} is nearly as precise. It requires enumerating exactly three atoms~-- no approximation, no vagueness.

\mention{Many} is a forgiving lock. It requires only magnitude assessment: \enquote{a contextually large quantity.} The atoms don't need sharp boundaries; they just need to be roughly atom-shaped.

\mention{Plural agreement} is the most forgiving. It requires only that the referent be construed as non-singular. Virtually anything that's in the count basin clears this one.

\mention{Book} is a precision-machined key. It opens every lock in the system. \mention{Cattle} is a blunter key. It gets through the forgiving locks~-- \mention{many cattle}, plural agreement~-- but can't open the precision locks: \ungram{\mention{a cattle}}, \ungram{\mention{three cattle}}.

\subsection{From locks to basins}

But here's how this connects to what we've seen before. Remember the spinning top from Chapter~\ref{ch:kinds-without-essences}? A category isn't a container you're in or out of~-- it's an attractor keeping a top upright. The count cluster is that basin. The locks? They're positions \textit{within} the basin. Tight locks sit at the centre; loose locks ring the edges.

When we say \mention{cattle} clears the loose locks but fails the tight ones, we're saying its top spins stably in the basin~-- but off-centre. It's count-ish. It's in the attractor. But it's not at the core.

\mention{Book} spins at the dead centre, satisfying every precision demand. \mention{Cattle} spins off-centre but stably~-- it's in the count basin, but it can't reach the precision locks at the centre. \mention{Folks}~-- we'll come to this~-- wobbles. Sometimes it clears the \mention{three} lock, sometimes it doesn't. Its position in the basin is unstable.

This integrated image~-- locks as positions in a basin, precision as distance from centre~-- unifies the immediate intuition (locks and keys) with the dynamic stability framework (spinning tops and attractors). The hierarchy of count properties \textit{is} the geometry of the basin. The further from centre a lock sits, the more tolerance it has.

\subsection{The implicational pattern}

The hierarchy generates a prediction: for any noun and any two properties, if it accepts the tighter one, it accepts the looser one; if it rejects the looser one, it rejects the tighter one. The distribution should be triangular. No noun should stably accept \mention{three} while rejecting \mention{many}. No noun should require \mention{a(n)} while taking \mention{much}.

This is falsifiable. Finding a noun that reverses the pattern~-- tight without loose~-- would challenge the account. The empirical record, as far as I can determine, contains no such case.

Here's the hierarchy, ordered from tightest to loosest. \term{Singular forms} and \mention{a(n)} are the strictest, requiring the identification of exactly one atomic unit. \term{Low cardinals} like \mention{three} and \mention{five} are similarly precise, requiring exact enumeration. Relaxing the tolerance slightly, \mention{several} requires multiple units but permits approximate quantity. \term{Distributives} like \mention{each} require discreteness but not enumeration. Further down, \mention{many} and \mention{few} require only relative magnitude assessment. \term{High round numerals} often function as approximate measures rather than exact counts. Finally, \term{plural agreement} is the most permissive, requiring only a non-singular construal.

A noun that loses \mention{several} will already have lost \mention{a(n)} and low cardinals. A noun that retains only plural agreement will have lost everything above it. The properties form an implicational scale, and a noun's countability profile is its position on that scale.

Table~\ref{tab:9:matrix} shows the triangular pattern at a glance.

\begin{table}[htbp]
\centering
\caption{Implicational matrix for count properties. Checkmarks indicate acceptable combinations; crosses indicate ungrammatical. The triangular pattern shows: if a noun accepts a tight property, it accepts all looser ones.}
\label{tab:9:matrix}
\begin{tabular}{lcccc}
\toprule
& \mention{a(n)} & \mention{three} & \mention{many} & Agreement \\
\midrule
\mention{book} & \checkmark & \checkmark & \checkmark & \checkmark \\
\mention{cattle} & $\times$ & $\times$ & \checkmark & \checkmark \\
\mention{police} & $\times$ & $\times$ & \checkmark & \checkmark \\
\mention{furniture} & $\times$ & $\times$ & $\times$ & sg. \\
\mention{water} & $\times$ & $\times$ & $\times$ & sg. \\
\bottomrule
\end{tabular}
\end{table}

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Quasi-count nouns: The stable intermediates}
\label{sec:9:quasi-count}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

\textit{CGEL} identifies a class of \term{quasi-count nouns}: plural-only nouns that take plural agreement and accept \mention{many} but resist singular forms, \mention{a(n)}, and low cardinals \citep[p.~345]{huddleston2002}. The core cases are \mention{cattle}, \mention{police}, \mention{poultry}, \mention{vermin}, \mention{livestock}, and \mention{clergy}.

They occupy exactly the position the hierarchy predicts: loose properties accepted, tight properties rejected.

\begin{itemize}
\item \mention{Many cattle} — \checkmark
\item \mention{The cattle are grazing} — \checkmark
\item \mention{Several cattle} — marginal for some speakers
\item \mention{Three cattle} — $\times$
\item \mention{A cattle} — $\times$
\end{itemize}

The same pattern holds for \mention{police}: \mention{many police}, \mention{the police are investigating}, but \ungram{\mention{three police}}, \ungram{\mention{a police}}.

Crucially, none of these nouns reverses the hierarchy. None accepts tight properties while rejecting loose ones. The triangular pattern holds.

\subsection{Why are they stable?}

If the homeostatic mechanism creates pressure toward coherence~-- pushing nouns toward full count or full mass~-- why haven't \mention{cattle} and \mention{police} drifted? They've been quasi-count for centuries.

The answer is \term{functional anchoring}. When speakers need singulative reference~-- when they need to name \textit{one} cow or \textit{one} officer~-- they don't attempt \ungram{\mention{a cattle}} or \ungram{\mention{a police}}. They use \mention{cow}, \mention{bull}, \mention{head of cattle}, or \mention{officer}. These alternative lexemes handle the singulative function, relieving pressure on the quasi-count noun to develop tight-linkage properties.

Bidirectional inference generates expectations. If a noun accepts \mention{many}, hearers may expect it to accept \mention{three} and \mention{a}. When those expectations fail, pressure arises either to regularise (extend tight properties) or to avoid the construction. If an alternative lexeme satisfies the singulative function, speakers have no reason to force the quasi-count noun into tight frames. The pressure dissipates.

This is \textit{passive} persistence, not active maintenance. \mention{Cattle} doesn't serve some function that requires it to stay quasi-count. It simply faces no pressure to change, because \mention{cow} is handling the job that analogy would otherwise push \mention{cattle} to fill. We asked in Chapter~\ref{ch:words-wont-hold-still} why \mention{cattle} has persisted for five centuries. Now we have an answer: the equilibrium is stable because the communicative ecology absorbs the force that would otherwise destabilise it.

Compare this to \mention{police}. The register variation is instructive: \mention{the police} is formal and aggregate; \mention{cops} is informal and count-friendly (\mention{three cops}); \mention{officer} is the robust singulative. Speakers who need to refer to a single police officer have an obvious lexical resource. Result: \mention{police} has remained quasi-count for as long as we have reliable records.

\subsection{The unstable case: \mention{folks}}

Not all intermediates are stable. \mention{Folks} occupies the boundary zone where the hierarchy predicts variability.

American English \mention{folks} sits between \mention{people} (fully count: \mention{three people}, \mention{a person}) and the quasi-count class. For many speakers, \mention{many folks} and \mention{several folks} are fully acceptable. But \mention{three folks} is often judged marked, informal, or slightly odd. Some speakers reject it; others accept it readily; still others accept it but hear it as slangier than \mention{three people}.

The HPC account predicts this instability. \mention{Folks} lacks a functional anchor. There's no singulative \ungram{\mention{a folk}} in ordinary use, and \mention{person} is semantically distinct~-- neutral rather than in-group. Unlike \mention{police} (anchored by \mention{officer}) and \mention{cattle} (anchored by \mention{cow}), \mention{folks} has nothing to bleed the pressure for regularisation.

And corpus data confirms that \mention{folks} is suppressed relative to \mention{people} in tight frames. In COCA, \mention{three people} occurs at 2,226 per million tokens of \mention{people}; \mention{three folks} occurs at only 258 per million tokens of \mention{folks}~-- an 8.6-fold suppression. The loose property (\mention{many}) is suppressed only about 2.3-fold. Tight properties are hit harder than loose ones, exactly as the hierarchy predicts.

The instability of \mention{folks} is not noise. It's structure. The noun is wobbling in the basin, uncertain whether it will stabilise off-centre (like \mention{cattle}), drift outward (toward mass), or regularise toward the centre (developing tight properties). The outcome depends on whether a functional anchor emerges or prescriptive pressure crystallises.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Diachronic signatures}
\label{sec:9:diachronic}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If the homeostatic mechanism is real, it should leave traces in the historical record. Two cases illuminate how the cluster extends and how it erodes.

\subsection{How the cluster self-completes: \mention{pea}}

We mentioned \mention{pea} in Chapter~\ref{ch:words-wont-hold-still}. Here's the fuller story.

Speakers of Middle English heard \mention{pease} with a final /z/ sound~-- the word for the vegetable, used in \mention{pease porridge} and \mention{pease pudding}. The noun was non-count: you had \mention{much pease}, not \mention{many pease}. But the final /z/ was phonologically identical to the plural suffix, and at some point speakers reanalysed it as such.

Once \mention{pease} was heard as a plural, the bidirectional inference mechanism kicked in. Plural morphology cues individuation. If \mention{pease} is plural, where's the singular? Speakers who expected the cluster to cohere created a gap~-- and filled it. They back-formed \mention{pea}. Then they extended the count cluster: \mention{a pea}, \mention{three peas}, \mention{many peas}. The count cluster didn't just emerge. Speakers \textit{built} it, one inference at a time.

The mechanism is visible here: cluster pressure creates gaps; gaps get filled. If the bidirectional inference story is right, we'd expect the historical record to show loose properties established before tight ones~-- the cluster building from the outside in. The evidence is suggestive though incomplete; what's clear is that the reanalysis triggered a cascade of count-property adoption.

\subsection{The unstable hybrids: Data}

Then there is \mention{data}.

On March 11, 2015, Minnesota legislators halted work on a license-plate reader bill to debate grammar. The bill text read \mention{the data are private}; Representative John Lesch insisted \mention{data} is singular. The committee voted; the motion passed unanimously; Lesch pumped his fist. The law was amended: \mention{data is}.

Lesch won the vote, but he entered what usage expert Bryan Garner calls a \mention{skunked} term argument \citep{garner2016}. \mention{Data} is currently moving from the count basin (Latin plural of \textit{datum}) to the mass basin (synonym for \textit{information}). In the transition, it exhibits the chaotic behavior of a system seeking a new equilibrium. Scientists say \mention{data are}. Tech CEOS say \mention{data is}. The cluster is unraveling, property by property. Historically the plural of \mention{datum}, it's shifting toward mass status: \mention{this data is}, \mention{much data} are now common, especially in informal and spoken registers. Corpus studies confirm that singular agreement with \mention{data} now predominates in most registers \citep{garner2016}.

Why is \mention{data} drifting? Because its functional anchor is disappearing. \mention{Datum}~-- the singulative~-- has become archaic, confined to philosophy-of-science contexts and pedantic style guides. Without a robust singulative in active use, the tight-linkage properties have nothing to attach to. Speakers who need to refer to a single piece of information say \mention{data point}, not \mention{datum}. But \mention{data point} is a compound, not a singulative of \mention{data}. It doesn't anchor the count cluster the way \mention{officer} anchors \mention{police}.

Result: \mention{data} drifts toward the loose end of the scale. Plural agreement weakens (\mention{this data is} becomes standard). Tight properties erode (\mention{three data} was always rare). Eventually, if the drift continues, \mention{data} will be fully mass~-- \mention{much data}, \mention{this data is}, \mention{some data}~-- with \mention{data point} handling any count function.

This is the quasi-count pattern run in reverse. \mention{Cattle} is stable because \mention{cow} exists. \mention{Data} is drifting because \mention{datum} is dying.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Cross-linguistic parallels}
\label{sec:9:cross-linguistic}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If bidirectional inference is a general mechanism~-- not an English quirk~-- languages with different morphological resources should show analogous patterns.

Welsh and Arabic mark singulatives morphologically. In Welsh, the base form of \mention{adar} (\enquote{birds}) is collective~-- grammatically singular, semantically aggregate. A suffix derives the singulative: \mention{aderyn} (\enquote{a bird}). \textcite{grimm2018} documents exactly the predicted pattern: bare collectives accept loose quantifiers but resist low numerals and distributives, while singulative-marked forms accept the full count cluster. The parallel to English quasi-count nouns is striking: Welsh collectives occupy the same position in the hierarchy that \mention{cattle} and \mention{police} occupy in English.

Classifier languages (Mandarin, Japanese) encode individuation differently~-- through classifiers that mediate between numerals and nouns, rather than through inflection on the noun itself. The prediction is that the clustering dynamics should shift to the classifier system: general classifiers should show tight/loose asymmetries parallel to what English quantifiers show. This remains programmatic~-- the cross-linguistic work hasn't been done to the depth needed~-- but the mechanism should apply wherever languages encode individuation morphosyntactically.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Sharp boundaries in fuzzy territory}
\label{sec:9:hyperreal}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

There's a puzzle lurking in the data. The hierarchy is continuous~-- properties shade from tight to loose~-- but judgments are often sharp. \mention{Three cattle} isn't \enquote{slightly bad} for most speakers; it's simply ungrammatical. \mention{Many cattle} isn't \enquote{slightly good}; it's fully acceptable. The boundary between what \mention{cattle} licenses and what it rejects feels determinate, even though the underlying individuation variable is gradient.

Chapter~\ref{ch:dynamic-discreteness} offered a framework for this: sharp-but-unknowable boundaries arising from tolerance dynamics. The hyperreal model lets us have determinate boundaries without anyone knowing precisely where they fall. Speakers act as if the boundary is sharp~-- they judge \mention{three cattle} ungrammatical, not merely marginal~-- but they can't articulate the precise point at which individuation becomes sufficient.

Countability confirms this picture. The \mention{folks} case is revealing: inter-speaker variation in the acceptability of \mention{three folks} is real and robust. Some speakers place the boundary above \mention{folks} (it clears \mention{three}); others place it below (it doesn't). Each speaker's individual grammar has a determinate answer, but the community doesn't. The gradience shows up in population variance, not in graded individual judgments.

This is the empirical signature of tolerance-based boundaries: sharpness within grammars, variance across them. The count/non-count distinction, for all its apparent fuzziness, behaves like a boundary system in which each speaker draws a line~-- just not the same line.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Passing the tests}
\label{sec:9:passing-tests}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Chapter~\ref{ch:failure-modes} introduced the Two-Diagnostic Test for genuine HPC kinds: high projectibility and robust homeostasis. Let's apply it to the count cluster.

\subsection{Projectibility}

The projectibility criterion asks: does recognising this kind support successful induction? For the count cluster, the answer is yes.

If you know a noun is count, you can predict its behaviour across a range of grammatical contexts. It will take \mention{a(n)} and low cardinals. It will take \mention{many}/\mention{few}, not \mention{much}/\mention{little}. It will trigger plural agreement when plural. It will combine with distributives. Every one of these predictions is testable, and for canonical count nouns, every one succeeds.

Crucially, the predictions are \textit{gradient} by position in the hierarchy. Knowing that \mention{cattle} is quasi-count~-- that it accepts loose properties but rejects tight ones~-- lets you predict precisely which frames it will enter. The quasi-count pattern is projectible too. The category structure supports differentiated, not just all-or-nothing, induction.

Compare this to a merely nominal grouping~-- say, \enquote{nouns ending in -tion}. Knowing that \mention{nation} ends in -tion tells you nothing about its grammatical behaviour distinct from knowing it's a noun. The suffix doesn't support induction. Countability does.

\subsection{Homeostasis}

The homeostasis criterion asks: is the clustering maintained by causal mechanisms, or is it merely a surface pattern that might scatter under perturbation?

We've identified the mechanisms:

We have identified five interlocking mechanisms. \term{Bidirectional inference} couples morphosyntax to individuation through a shared semantic variable. \term{Acquisition} transmits this coupling as a unit, as children overgeneralise from one property to the others. \term{Entrenchment} preserves high-frequency patterns as chunks. \term{Institutional norms} stabilise the distribution at the community level. Finally, \term{functional anchoring} bleeds pressure from intermediate cases like \mention{cattle}, allowing them to persist without regularising.

This is robust homeostasis. Perturb the system~-- weaken individuation, remove a singulative anchor, expose learners to non-standard input~-- and the cluster responds in predictable ways. It's not just that count properties co-occur; it's that mechanisms push them toward co-occurrence. The clustering is maintained, not accidental.

\subsection{The verdict}

The count cluster passes both diagnostics. It supports induction (knowing a noun's count status predicts grammatical behaviour) and it's held together by mechanisms (bidirectional inference, acquisition, entrenchment, anchoring). It belongs in the upper-right quadrant of the diagnostic matrix: a genuine HPC kind.

This doesn't mean countability is simple. The hierarchy reveals internal structure; the quasi-count cases reveal tolerated heterogeneity; the diachronic cases reveal dynamic equilibria that can shift. But the claim isn't that HPC kinds are uniform. The claim is that they're real~-- that they're maintained by mechanisms and support induction. The count cluster qualifies.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{What does this buy us?}
\label{sec:9:payoff}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

The HPC analysis of countability does three things that the traditional feature-based account doesn't.

\paragraph{It explains the clustering, not just labels it.} The feature [+count] says that count properties go together. The HPC account says \textit{why}: they're inferentially coupled to individuation, and mechanisms keep them coupled. The clustering isn't a brute fact; it's a consequence of how form and meaning interact in processing, acquisition, and transmission.

\paragraph{It predicts the dissociation order.} When individuation weakens, tight properties fail before loose ones. The hierarchy isn't stipulated; it follows from the precision demands of each property. No version of feature-bundle theory predicts this order. No version of prototype theory explains why the gradience has this particular shape.

\paragraph{It explains stability and instability together.} Quasi-count nouns are stable because of functional anchoring; \mention{folks} is unstable because it lacks anchoring. \mention{Pea} regularised because there was no anchor; \mention{data} is drifting because \mention{datum} is dying. The HPC account doesn't just describe which cases are stable~-- it explains why, and predicts which cases should be vulnerable to change.

The traditional question~-- \enquote{Is \mention{cattle} count or mass?}~-- is the wrong question. The HPC question is: \enquote{Where does \mention{cattle} sit in the count basin, and what's holding it there?} That question has an answer. The first one doesn't.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Looking forward}
\label{sec:9:transition}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Countability was the easy case. Not because it's simple~-- the hierarchy, the quasi-count intermediates, the diachronic dynamics all have genuine complexity~-- but because countability passes the HPC tests cleanly. The clustering is tight, the mechanism is identifiable, the predictions are borne out.

And \mention{emojis}? The regularisation is already underway. Borrowed from Japanese around 2010, the word hit English brains with maximum individuation~-- discrete little pictures, each one distinct. The mass basin couldn't hold it. Within a decade, \mention{three emojis} had overtaken \mention{three emoji} in edited prose. The mechanism processed a new entity exactly as predicted.

The next cases are harder. Definiteness (Chapter~\ref{ch:definiteness-and-deitality}) presents a puzzle: the grammatical form cluster (articles, demonstratives, possessives) and the semantic function cluster (referent identification, familiarity, uniqueness) may not align perfectly. Are they one HPC or two overlapping ones? Word classes (Chapter~\ref{ch:word-classes}) raise similar questions: why are nouns and verbs stable cross-linguistically while adjectives vary? Is the noun/verb distinction one HPC or multiple?

The template developed here~-- identify the cluster, locate the mechanism, test projectibility and homeostasis, trace the fraying~-- applies to all of them. And the structural insight applies too: at finer grain, categories reveal internal structure~-- sub-clusters with their own maintenance, held together by interface processes. Countability made this visible. The remaining chapters show the same analytical move recurring.
