\chapter{Countability}
\label{ch:countability}

% Chapter 9: First Part III case study
% Target: ~8,000 words
% Metaphor: Lock-and-key hook → basin integration

\epigraph{\textit{The word LEGO® is a brand name and is very special to all of us in the LEGO Group Companies. We would sincerely like your help in keeping it special. Please always refer to our bricks as `LEGO Bricks or Toys' and not `LEGOS'.}}{— LEGO Group consumer catalog, c.~1980s}

The lawyers at the LEGO Group have been fighting this battle for forty years. They have trademark law on their side. They have the Chicago Manual of Style. They have millions of dollars in brand-management budget.

And they are losing.

To a lawyer, \mention{LEGO} should only ever modify~-- \mention{LEGO bricks}, \mention{LEGO toys}~-- never stand alone as a count noun. To a six-year-old, a plastic brick is a discrete, bounded, manipulable object. And English has a deeply entrenched pattern for dealing with discrete, bounded, manipulable objects: it counts them. It adds an \mention{-s}. It makes them plural. When a child asks for \mention{three Legos}, they aren't making a mistake; they are applying an inference they've learned from thousands of similar cases. The count cluster is claiming the word.

This chapter is about that mechanism~-- the control loop that decides what gets counted. The count/non-count distinction isn't a static binary. It's a dynamic negotiation. And as we'll see, the HPC framework we built in Part II handles this messy distinction cleanly.

Countability, I'll argue, isn't a definition. It's a homeostatic property cluster~-- and it illustrates something general about how HPCs work. We leave the abstract theory for the concrete terrain. Countability is our first basin~-- a deep, stable attractor in the landscape of grammar. At a finer grain, categories often reveal internal structure: sub-clusters maintained by different mechanisms, held together by interface processes. Countability makes this structure visible because the two clusters~-- one semantic (individuation), one morphosyntactic (the count cluster)~-- are saliently distinct. What looks like a single category is a coupling. (A note: Throughout Part III, I use \term{mechanism} and \term{stabilizer} to refer to causal structures and their functional roles respectively \parencite{illari2012}.)

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{One word, two categories}
\label{sec:9:two-categories}

The word \textit{countability} hides a structural fact. At one grain, it names a single category~-- a coherent cluster that English speakers navigate daily. At a finer grain, it reveals itself as a coupling between two: the \term{count cluster} (morphosyntactic) and the \term{individuation cluster} (semantic). In the triadic terms of Chapter~\ref{ch:the-category-zipper}:

\begin{itemize}
    \item \textbf{Form}: the count cluster~-- plural marking, quantifier selection, agreement.
    \item \textbf{Object}: the individuation construal~-- whether the referent is bounded, atomic, enumerable.
    \item \textbf{Interpretant}: the inferential habit~-- what counting operations, quantificational inferences, and partitioning expectations the construal licenses.
\end{itemize}

Each cluster is maintained by its own mechanisms. What couples them is the interpretant: when you hear count morphology, you expect individuation; when you construe something as individuated, you reach for count morphology. The habit pulls form and object together. Countability is both the whole and the joint.

This matters because it explains something puzzling. Chapter~\ref{ch:failure-modes} argued that lexical semantics alone rarely produces HPC kinds: categories like \mention{furniture} (conceptual similarity) or \term{animacy} (referent property) don't generate the tight clustering that projectibility requires. But countability does. English speakers reliably extend count morphosyntax to novel words, predict quantifier compatibility from plural marking, and judge intermediate cases with surprising consistency. Why?

The answer is that countability isn't just lexical semantics. It's an \textit{interface system}~-- a place where a semantic distinction receives repeated grammatical reinforcement. When constructions select for individuation, when morphology marks it, when processing routines exploit it, when acquisition converges on it~-- then the semantic distinction starts to behave like a kind. This is what makes countability different from \mention{furniture}.

The literature offers several competing diagnoses. \emph{Mereological accounts} treat countability as tracking part-whole structure: count nouns denote atomic entities, mass nouns denote divisible stuff \citep{link1983,chierchia1998}. \emph{Cognitive accounts} ground the distinction in perception: individuation reflects object-file cognition and early-emerging core knowledge \citep{spelke2007,bloom1994a}. \emph{Grammatical accounts} treat count/mass as an autonomous formal feature, assigned lexically and enforced by agreement \citep{borer2005}. Each preserves something~-- ontological grounding, cognitive reality, formal tractability~-- but none explains why the \emph{same set of morphosyntactic properties} clusters with individuation across constructions.

Here is the decision criterion: any account that keeps countability purely semantic must explain why plural marking, quantifier selection, and agreement~-- but not, say, animacy or abstractness~-- cluster into a single syndrome. Conversely, any account that treats object-mass nouns as lexical exceptions still owes an explanation for why the exception pattern is \emph{systematic}. The two-cluster architecture isn't an alternative semantics; it's a demand that any semantics meet.

Individuation without count morphology; count morphology without individuation.

The next two sections unpack the two clusters. First, the semantic side: what is individuation, and what maintains it as a cluster of properties? Then, the morphosyntactic side: what is the count cluster, and what keeps its properties together? Only after both are on the table can we ask how they couple.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The individuation cluster}
\label{sec:9:individuation-cluster}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Individuation isn't a single primitive. It's a cluster of observable properties that typically travel together~-- what the formal semantics literature calls the accessibility of atoms to quantificational operations, formalized by \citet{link1983} and developed by \citet{chierchia1998}, \citet{rothstein2010}, and \citet{grimm2018}. I'll first describe the profile (the symptoms) and then turn to the mechanisms (the causes) that keep them coherent.

The cluster typically comprises four properties. \term{boundedness} entails that the referent has discrete edges~-- a spatial or conceptual boundary that separates it from its environment. \term{atomicity} means the referent is accessible as a unit that can be singled out and tracked. \term{enumerability} makes the referent compatible with exact counting~-- three of them, not just much of it. Finally, \term{homogeneity resistance} means that parts of the referent aren't the same kind as the whole: half a cat isn't a cat, whereas half of water is still water. Half a cat is a problem, not a category.

These properties cluster. When a referent is bounded, it's usually atomic; when it's atomic, it's usually enumerable; when it's enumerable, it usually resists homogeneous subdivision. The clustering isn't accidental. But it isn't absolute. A \mention{ball bearing} is maximally individuated: bounded, atomic, enumerable. But a \mention{cloud} is bounded while lacking stable atomicity. \mention{Cattle} denotes discrete animals, but the noun construes them as an aggregate. The cluster represents a prototype, not a rigid definition.

\subsection{What maintains the cluster}

Descriptively, the cluster looks like a set of logical entailments. Explanatorily, it's maintained by perceptual and cognitive mechanisms that operate across modalities~-- and, strikingly, across species.

Consider \term{edge detection}. Visual object perception depends on detecting boundaries. The visual cortex uses orientation-selective cells, Gabor-like filters, and mid-level grouping principles to extract edges from the visual field. These edges are then grouped into coherent object representations~-- what \citet{kahneman1992} called \term{object files}: temporary episodic representations that bind features to locations and track objects across time.

But edge detection isn't vision-specific. Blind echolocators produce tongue clicks and interpret the returning echoes to perceive objects. Research shows that they can identify object shape, size, and location with remarkable precision \citep{KolarikEtAl2014}~-- and fMRI studies reveal that they activate visual cortex when doing so \citep{ThalerArnottGoodale2011}. The perceptual system has found a different input modality but the same computational goal: bounded individuals.

The pattern extends beyond perception. The macrophages we met in Chapter~\ref{ch:stabilizers} face the same computational problem at a different scale: distinguishing self from non-self. They use pattern recognition receptors~-- toll-like receptors, scavenger receptors~-- to detect molecular edges, boundaries between what belongs in the tissue and what doesn't. Edge detection~-- the computational problem of finding where one thing ends and another begins~-- recurs from molecular biology to perception to cognition. 

This recurrence is predictable rather than mysterious. Evolution keeps rediscovering boundary-making machinery because organisms that can cheaply and reliably carve input into stable units~-- things with edges, persistence, and re-identifiability~-- can move, grasp, avoid, and respond more effectively. The mechanisms aren't homologous across levels, and I am not claiming that macrophages \enquote{do vision}; the point is computational: many systems face a version of the same individuation problem, and the easiest solutions bundle cues that cohere. That bundling is what gives the individuation cluster its tight internal correlations, making it a plausible semantic anchor for grammatical reinforcement. And so, such mechanisms are ancient. 

\citet{spelke2007} argues\footnote{Or should that be \mention{argue}?} that human infants are equipped with \term{core knowledge} systems for objects~-- systems that operate from the first months of life. Infants as young as four months perceive objects as cohesive (parts move together), bounded (edges mark identity), and continuous (objects persist through occlusion). These are the same properties that define the individuation cluster in adult conceptual systems. The mechanisms that maintain individuation aren't learned from scratch; they're built on an early foundation.

\term{Cross-modal integration} reinforces the pattern. Object individuation isn't locked to a single sensory channel. When you set a cup down and see and hear it alighting, you don't perceive thre entities~-- one tactile, one visual, one auditory. You perceive one object with multimodal properties. The binding is maintained by cross-modal integration mechanisms that enforce coherence: what looks bounded should sound bounded, feel bounded, behave as a unit. This integration reinforces the cluster. The properties travel together because multiple systems expect them to travel together.

This gradient nature of individuation is crucial for what comes next. The count cluster~-- the morphosyntactic syndrome~-- tracks individuation. When individuation is strong, the full count cluster applies. When it dissolves, the count cluster dissolves with it. But it does so in an orderly way.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The count cluster}
\label{sec:9:count-cluster}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

The distinction seems intuitive. \mention{Cat}, \mention{chair}, and \mention{idea} differ from \mention{water}, \mention{mud}, and \mention{traffic}. The first group denotes discrete individuals; the second denotes undifferentiated substances or aggregates.

Otto Jespersen first formalized the distinction in 1924, coining the term \term{mass-word} for nouns that resist counting:
\begin{quote}
There are a great many words which do not call up the idea of some definite thing with a certain shape or precise limits. I call these `mass-words'; they may be either material, in which case they denote some substance in itself independent of form, such as \mention{silver}, \mention{water}, \mention{butter}, \mention{gas}, \mention{air}, etc., or else immaterial, such as \mention{leisure}, \mention{music}, \mention{traffic}, \mention{success} \citep[198]{jespersen1924}.
\end{quote}
But Jespersen's insight goes back further. The chapter was adapted from a lecture he delivered to the Copenhagen Academy of Sciences in 1911~-- thirteen years before publication, and decades before the cognitive revolution.\footnote{Jespersen also documented the flexibility of individual nouns across count and mass uses: \mention{a little more cheese} vs. \mention{two big cheeses}; \mention{it's hard as iron} vs. \mention{a hot iron}. The phenomenon was recognized a century ago.} Even then, he saw that grammar doesn't just mirror the world; it makes some construals cheap and others costly~-- not by determining thought, but by defaulting certain ways of packaging experience. While English speakers can say \mention{two apples} or \mention{two fruits}, we can't take a microscope and a stethoscope and call them \ungram\mention{two equipment}~-- unless we supply a unit (\mention{two pieces/items of equipment}). English makes you bring your own measuring cup. The relevant individuation is in the construal, not in the furniture of reality.

Standard grammars like \textit{CGEL} \citep{huddleston2002} identify a cluster of morphosyntactic properties that typically align.\footnote{\textit{CGEL} is careful to distinguish count and non-count \emph{senses} rather than strict lexical subcategories, acknowledging the flexibility of many nouns (e.g., \mention{beer}, \mention{cake}).} Again, we can distinguish the observable profile from the explanatory engine. For a prototypical count noun like \mention{ball}, the cues reinforce the individuation. The noun denotes discrete, bounded entities; it inflects for number (\mention{balls}); it accepts exact cardinals (\mention{one ball}, \mention{two balls}) and fuzzy count quantifiers (\mention{many}, \mention{fewer}); and it triggers agreement matching its morphology.

By contrast, for a prototypical non-count noun like \mention{water}, the cluster prevents individuation. The noun denotes undifferentiated substance; it lacks a plural form (outside special interpretations); it rejects cardinals (*\mention{two waters}) and takes mass quantifiers (\mention{much}, \mention{less}); and it triggers singular agreement.

This is the \term{count cluster}: the set of formal properties that normally travel together. When you hear a word has a plural form, you can predict it will take \mention{many} and reject \mention{much}. You can predict it will refer to something conceptualized as an individual.

\subsection{The problem: Object-mass nouns}

The problem is that the world doesn't always cooperate. \citet{rothstein2010} identified the class of `object-mass nouns'~-- words like \mention{furniture}, \mention{footwear}, \mention{cutlery}, and \mention{jewelry}. Semantically, these refer to discrete, countable artifacts. A chair is as discrete as a ball. But \mention{furniture} is grammatically mass:
\ea
    \ea[*]{\mention{three furnitures}}
    \ex[*]{\mention{many furniture}}
    \ex[]{\mention{much furniture}}
    \z
\z

Why? If the grammar just tracked ontological discreteness, \mention{furniture} should be count. The fact that it isn't~-- that English forces us to say \mention{pieces of furniture} while French speakers comfortably say \mention{trois meubles} (three furnitures)~-- proves that countability is an autonomous grammatical system, not a direct read-out of physics.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The coupling}
\label{sec:9:coupling}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Now we can ask what couples the individuation (semantic) cluster to the count (morphosyntactic) cluster into a single system. The answer is \term{bidirectional inference}~-- the mechanism linking semantic construal to grammatical form.

Each count property is a cue~-- a piece of morphosyntactic evidence that hearers use to infer how the speaker is construing the referent. When you hear \mention{three dogs}, you infer that the speaker has individuated the referents~-- construed them as discrete, enumerable units. When you hear \mention{much water}, you infer a non-individuated construal: stuff, not atoms. The morphosyntax generates expectations about the construal; the construal, in turn, constrains morphosyntactic choice.

The coupling works in both directions. In comprehension, count morphosyntax (\mention{a}, \mention{three}, \mention{many}) leads the hearer to expect atomic, enumerable referents. In production, an individuated construal prompts the speaker to select count morphosyntax. The properties cluster because they are cues to the same underlying condition.

The properties cluster because they're all coupled to the same semantic variable. Knowing that a noun takes \mention{a(n)} tells you it supports individuation~-- and if it supports individuation, it should take \mention{three} and \mention{many} and plural agreement too. The generalisation runs through the construal, not through the morphosyntax directly. That's why the properties are mutually predictive: not because any one \textit{causes} the others, but because they're all symptoms of the same underlying condition.

\subsection{Why the coupling produces an HPC}

This is where the payoff from Chapter~\ref{ch:failure-modes} arrives. Lexical semantics alone rarely produces HPC kinds: categories like \mention{furniture} (conceptual similarity) don't generate the tight clustering that projectibility requires. Why should countability be different?

The answer is that countability operates at the syntax-semantics interface. The individuation cluster is reinforced by perceptual mechanisms (edge detection, object files, cross-modal integration). The count cluster is reinforced by morphosyntactic mechanisms (acquisition, entrenchment, transmission). But the coupling between them~-- the bidirectional inference mechanism~-- provides a third layer of reinforcement. Each time construal and form align, both clusters are strengthened. Each time they're transmitted together to a new learner, the coupling tightens.

\mention{Furniture} lacks this reinforcement. The conceptual similarity exists~-- chairs and tables share properties~-- but English provides no morphosyntactic system that selects for that similarity and reinforces it with every utterance. There's no grammatical slot that says `furniture-like things go here'. The category exists in the lexicon but not in the grammar.

Countability is the opposite. The grammar provides slots that select for individuation: \mention{a} requires atomic reference; \mention{three} requires enumerable atoms; \mention{many} tolerates weaker individuation. Every time a speaker uses these constructions, they reinforce the link between semantic construal and grammatical form. This is why countability passes the HPC diagnostics: the coupling mechanism provides the homeostasis that lexical semantics alone can't.

We'll return to this relationship in Chapter~13. Grammaticality judgements aren't just about form; they're about the successful pairing of a form with a construal. \textbf{Note during drafting: Make sure this cashes out in Ch~13 or is removed.}

\subsection{Multi-timescale maintenance}

The bidirectional inference mechanism operates at multiple timescales, each reinforcing the others.

At the fast timescale of \term{processing} (milliseconds), every time a speaker produces or comprehends a count frame, the form--meaning link is activated. \mention{Three dogs} primes the expectation of individuation; individuation primes the expectation of count morphosyntax. Mismatches~-- \ungram\mention{three furnitures}, say~-- incur processing costs. These costs are \term{error signals} (Chapter~\ref{ch:stabilizers}): they scream that the coupling has been violated, providing the negative feedback that keeps the basin's edges steep. They feel wrong because they violate entrenched expectations.

At the slow timescale of \term{acquisition} (years), children don't learn count properties one by one. They learn that count morphosyntax correlates with individuation. Classic acquisition work shows that children overgeneralise: encountering a noun in one count frame, they extend it to others \citep{bloom1994a,gordon1985}. A child who hears \mention{many police} may try \ungram\mention{three police}, treating the loose property as evidence for the tight ones. When a speaker conceptualizes a referent as individuated (bounded, atomic), they reach for the tool that signals individuation: the count syntax. This is why children say \mention{I saw three sheeps} or \mention{two mouses} or \mention{my foots hurt}. The child perceives discrete animals, discrete body parts. The grammar provides a slot for discrete entities (the regular plural); the child fills the slot, ignoring the lexical exception. Such overgeneralisation is systematic. They're obeying the rule the grammar actually taught them; the exceptions didn't show up to class. The error proves the mechanism works~-- they're running the algorithm perfectly, just on irregular input.

Finally, at the timescale of \term{transmission} (decades), institutional forces~-- style guides, copy editors, teachers, grammar checkers~-- enforce canonical patterns and resist drift. The \mention{data}/\mention{datum} debate lives here. As \mention{datum} recedes from editorial practice, the normative anchor weakens and \mention{data} drifts toward mass status. The institutional layer doesn't \textit{create} the cluster, but it stabilizes it at the community level.

The mechanisms interlock: the fast loop generates usage patterns; the slow loop crystallises them into community standards; acquisition transmits the crystallised patterns to new learners, who then participate in the fast loop. Metalinguistic feedback adds another layer: prescriptive campaigns (like LEGO Group's forty-year effort to block \\mention{Legos}) can stabilize or destabilize construals~-- though as the epigraph suggests, they rarely override entrenched patterns. Perturbation to any level~-- a semantic shift that weakens individuation, a prescriptive shift that sanctions formerly deviant forms~-- ripples through the system and changes the equilibrium. The cluster isn't static. It's dynamically maintained.

\subsection{The chunking story}

There's a cognitive dimension to why high-frequency patterns resist change. Following \textcite{bybee2010}, we can understand entrenchment in terms of \textit{chunking}: high-frequency sequences get stored as units and accessed as wholes, bypassing compositional assembly.

\mention{Many cattle} is a chunk. English speakers have encountered it often enough that it's stored in memory and retrieved directly. \mention{Three cattle} isn't a chunk~-- it has to be assembled online, and the assembly fails because the components don't fit. The noun's stored profile resists the frame the speaker is trying to build.

This is why quasi-count nouns feel unstable in tight frames but natural in loose ones. \mention{Many cattle} is retrieved; \mention{three cattle} is constructed and rejected. The difference between the two isn't a difference in categorical membership. It's a difference in processing: one path is entrenched; the other isn't.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The hierarchy: Tight before loose}
\label{sec:9:hierarchy}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If all count properties were equally sensitive to individuation, any weakening would collapse the cluster entirely. A noun would be fully count or fully mass, nothing in between. But that's not what we see. \mention{Cattle} and \mention{police} sit in the middle: they accept \textit{some} count properties while rejecting others. And the pattern isn't random. The properties peel off in order.

Here's why. Although all count properties are coupled to individuation, they're not equally demanding. Some require high-precision individuation~-- exact atomic units, sharply bounded. Others tolerate lower precision~-- approximate magnitude, vague plurality. When individuation weakens, the demanding properties fail first.

\subsection{Locks with different tolerances}

Think of it this way. Each count construction~-- \mention{a} N, \mention{three} N, \mention{many} N~-- is a lock with a specific tolerance. The noun provides a key: its individuation profile. If the key clears the tolerance, the construction is licensed.

\mention{A} is a precision lock. It requires identifying exactly one atomic unit. The key must be cut to exact specifications.

\mention{Three} is nearly as precise. It requires enumerating exactly three atoms~-- no approximation, no vagueness.

\mention{Many} is a forgiving lock. It requires only magnitude assessment: \enquote{a contextually large quantity.} The atoms don't need sharp boundaries; they just need to be roughly atom-shaped.

\mention{Plural agreement} is the most forgiving. It requires only that the referent be construed as non-singular. Virtually anything that's in the count basin clears this one.

\mention{Book} is a precision-machined key. It opens every lock in the system. \mention{Cattle} is a blunter key. It gets through the forgiving locks~-- \mention{many cattle}, plural agreement~-- but can't open the precision locks: \ungram\mention{a cattle}, \ungram\mention{three cattle}.

\subsection{From locks to basins}

But here's how this connects to what we've seen before. Remember the spinning top from Chapter~\ref{ch:kinds-without-essences}? A category isn't a container you're in or out of~-- it's an attractor keeping a top upright. The count cluster is that basin. The locks? They're positions \textit{within} the basin. Tight locks sit at the centre; loose locks ring the edges.

When we say \mention{cattle} clears the loose locks but fails the tight ones, we're saying its top spins stably in the basin~-- but off-centre. It's count-ish. It's in the attractor. But it's not at the core.

\mention{Book} spins at the dead centre, satisfying every precision demand. \mention{Cattle} spins off-centre but stably~-- it's in the count basin, but it can't reach the precision locks at the centre. \mention{Folks}~-- we'll come to this~-- wobbles. Sometimes it clears the \mention{three} lock, sometimes it doesn't. Its position in the basin is unstable.

This integrated image~-- locks as positions in a basin, precision as distance from centre~-- unifies the immediate intuition (locks and keys) with the dynamic stability framework (spinning tops and attractors). The hierarchy of count properties \textit{is} the geometry of the basin. The further from centre a lock sits, the more tolerance it has.

\subsection{The implicational pattern}

The hierarchy generates a prediction: for any noun and any two properties, if it accepts the tighter one, it accepts the looser one; if it rejects the looser one, it rejects the tighter one. The distribution should be triangular. No noun should stably accept \mention{three} while rejecting \mention{many}. No noun should require \mention{a(n)} while taking \mention{much}.

This is falsifiable. Finding a noun that reverses the pattern~-- tight without loose~-- would challenge the account. The empirical record, as far as I can determine, contains no such case.

Here's the hierarchy, ordered from tightest to loosest. \term{Singular forms} and \mention{a(n)} are the strictest, requiring the identification of exactly one atomic unit. \term{Low cardinals} like \mention{three} and \mention{five} are similarly precise, requiring exact enumeration. Relaxing the tolerance slightly, \mention{several} requires multiple units but permits approximate quantity. \term{Distributives} like \mention{each} require discreteness but not enumeration. Further down, \mention{many} and \mention{few} require only relative magnitude assessment. \term{High round numerals} often function as approximate measures rather than exact counts. Finally, \term{plural agreement} is the most permissive, requiring only a non-singular construal.

A noun that loses \mention{several} will already have lost \mention{a(n)} and low cardinals. A noun that retains only plural agreement will have lost everything above it. The properties form an implicational scale, and a noun's countability profile is its position on that scale.

Table~\ref{tab:9:matrix} shows the triangular pattern at a glance.

\begin{table}[htbp]
\centering
\caption{Implicational matrix for count properties. Checkmarks indicate acceptable combinations; crosses indicate ungrammatical. The triangular pattern shows: if a noun accepts a tight property, it accepts all looser ones.}
\label{tab:9:matrix}
\begin{tabular}{lcccc}
\toprule
& \mention{a(n)} & \mention{three} & \mention{many} & Agreement \\
\midrule
\mention{book} & \checkmark & \checkmark & \checkmark & \checkmark \\
\mention{cattle} & $\times$ & $\times$ & \checkmark & \checkmark \\
\mention{police} & $\times$ & $\times$ & \checkmark & \checkmark \\
\mention{furniture} & $\times$ & $\times$ & $\times$ & sg. \\
\mention{water} & $\times$ & $\times$ & $\times$ & sg. \\
\bottomrule
\end{tabular}
\end{table}

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Quasi-count nouns: The stable intermediates}
\label{sec:9:quasi-count}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

\textit{CGEL} identifies a class of \term{quasi-count noun}s: plural-only nouns that take plural agreement and accept \mention{many} but resist singular forms, \mention{a(n)}, and low cardinals \citep[p.~345]{huddleston2002}. The core cases are \mention{cattle}, \mention{police}, \mention{poultry}, \mention{vermin}, \mention{livestock}, and \mention{clergy}.

They occupy exactly the position the hierarchy predicts: loose properties accepted, tight properties rejected.

\begin{itemize}
\item \mention{Many cattle} — \checkmark
\item \mention{The cattle are grazing} — \checkmark
\item \mention{Several cattle} — marginal for some speakers
\item \mention{Three cattle} — $\times$
\item \mention{A cattle} — $\times$
\end{itemize}

The same pattern holds for \mention{police}: \mention{many police}, \mention{the police are investigating}, but \ungram\mention{three police}, \ungram\mention{a police}.

A related class includes \enquote{pluralia tantum} nouns like \mention{groceries}, \mention{genitals}, \mention{dregs}, and \mention{remains}. Unlike \mention{cattle}, these possess explicit plural morphology. But like \mention{cattle}, they resist exact enumeration: we buy \mention{groceries}, but we don't typically count \ungram\mention{three groceries} or identify \ungram\mention{a genital}. This reinforces the autonomy of the system: even explicit plural morphology doesn't guarantee access to the full count cluster. The \mention{many} and \mention{agreement} locks are open, but the precision locks remain closed.

And none of these nouns reverses the hierarchy. None accepts tight properties while rejecting loose ones. The triangular pattern holds.

\subsection{Why are they stable?}

If the homeostatic mechanism creates pressure toward coherence~-- pushing nouns toward full count or full mass~-- why haven't \mention{cattle} and \mention{police} drifted? They've been quasi-count for centuries.

The answer is \term{functional anchoring}. When speakers need singulative reference~-- when they need to name \textit{one} cow or \textit{one} officer~-- they don't attempt \ungram\mention{a cattle} or \ungram\mention{a police}. They use \mention{cow}, \mention{bull}, \mention{head of cattle}, or \mention{officer}. These alternative lexemes handle the singulative function, relieving pressure on the quasi-count noun to develop tight-linkage properties.

Bidirectional inference generates expectations. If a noun accepts \mention{many}, hearers may expect it to accept \mention{three} and \mention{a}. When those expectations fail, pressure arises either to regularise (extend tight properties) or to avoid the construction. If an alternative lexeme satisfies the singulative function, speakers have no reason to force the quasi-count noun into tight frames. The pressure dissipates.

This is \textit{passive} persistence, not active maintenance. \mention{Cattle} doesn't serve some function that requires it to stay quasi-count. It simply faces no pressure to change, because \mention{cow} is handling the job that analogy would otherwise push \mention{cattle} to fill. We asked in Chapter~\ref{ch:words-wont-hold-still} why \mention{cattle} has persisted for five centuries. Now we have an answer: the equilibrium is stable because the communicative ecology absorbs the force that would otherwise destabilize it.

Compare this to \mention{police}. The register variation is instructive: \mention{the police} is formal and aggregate; \mention{cops} is informal and count-friendly (\mention{three cops}); \mention{officer} is the robust singulative. Speakers who need to refer to a single police officer have an obvious lexical resource. Result: \mention{police} has remained quasi-count for as long as we have reliable records.

\subsection{The unstable case: \mentionhead{folks}}

Not all intermediates are stable. \mention{Folks} occupies the boundary zone where the hierarchy predicts variability.

American English \mention{folks} sits between \mention{people} (fully count: \mention{three people}, \mention{a person}) and the quasi-count class. For many speakers, \mention{many folks} and \mention{several folks} are fully acceptable. But \mention{three folks} is often judged marked, informal, or slightly odd. Some speakers reject it; others accept it readily; still others accept it but hear it as slangier than \mention{three people}.

The HPC account predicts this instability. \mention{Folks} lacks a functional anchor. There's no singulative \ungram\mention{a folk} in ordinary use, and \mention{person} is semantically distinct~-- neutral rather than in-group. Unlike \mention{police} (anchored by \mention{officer}) and \mention{cattle} (anchored by \mention{cow}), \mention{folks} has nothing to bleed the pressure for regularisation.

And corpus data confirms that \mention{folks} is suppressed relative to \mention{people} in tight frames. In COCA, \mention{three people} occurs at 2,226 per million tokens of \mention{people}; \mention{three folks} occurs at only 258 per million tokens of \mention{folks}~-- an 8.6-fold suppression. The loose property (\mention{many}) is suppressed only about 2.3-fold. Tight properties are hit harder than loose ones, exactly as the hierarchy predicts.

The instability of \mention{folks} isn't noise. It's structure. The noun is wobbling in the basin, uncertain whether it will stabilize off-centre (like \mention{cattle}), drift outward (toward mass), or regularise toward the centre (developing tight properties). The outcome depends on whether a functional anchor emerges or prescriptive pressure crystallises.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Diachronic signatures}
\label{sec:9:diachronic}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If the homeostatic mechanism is real, it should leave traces in the historical record. Two cases illuminate how the cluster extends and how it erodes.

\subsection{How the cluster self-completes: \mentionhead{pea}}

We mentioned \mention{pea} in Chapter~\ref{ch:words-wont-hold-still}. Here's the fuller story.

Speakers of Middle English heard \mention{pease} with a final /z/ sound~-- the word for the vegetable, used in \mention{pease porridge} and \mention{pease pudding}. The noun was non-count: you had \mention{much pease}, not \mention{many pease}. But the final /z/ was phonologically identical to the plural suffix, and at some point speakers reanalysed it as such.

Once \mention{pease} was heard as a plural, the bidirectional inference mechanism kicked in. Plural morphology cues individuation. If \mention{pease} is plural, where's the singular? Speakers who expected the cluster to cohere created a gap~-- and filled it. They back-formed \mention{pea}. Then they extended the count cluster: \mention{a pea}, \mention{three peas}, \mention{many peas}. The count cluster didn't just emerge. Speakers \textit{built} it, one inference at a time.

The mechanism is visible here: cluster pressure creates gaps; gaps get filled. If the bidirectional inference story is right, we'd expect the historical record to show loose properties established before tight ones~-- the cluster building from the outside in. The evidence is suggestive though incomplete; what's clear is that the reanalysis triggered a cascade of count-property adoption.

\subsection{The unstable hybrids: Data}

Then there is \mention{data}.

On March 11, 2015, Minnesota legislators halted work on a license-plate reader bill to debate grammar. The bill text read \mention{the data are private}; Representative John Lesch insisted \mention{data} is singular. The committee voted; the motion passed unanimously; Lesch pumped his fist. The law was amended: \mention{data is}. Grammar rarely gets a roll-call vote, so it took the opportunity.

Lesch won the vote, but he entered what usage expert Bryan Garner calls a \mention{skunked} term argument \citep{garner2016}. \mention{Data} is currently moving from the count basin (Latin plural of \textit{datum}) to the mass basin (synonym for \textit{information}). In the transition, it exhibits the chaotic behavior of a system seeking a new equilibrium. Scientists say \mention{data are}. Tech CEOS say \mention{data is}. The cluster is unraveling, property by property. Historically the plural of \mention{datum}, it's shifting toward mass status: \mention{this data is}, \mention{much data} are now common, especially in informal and spoken registers. Corpus studies confirm that singular agreement with \mention{data} now predominates in most registers \citep{garner2016}.

Why is \mention{data} drifting? Because its functional anchor is disappearing. \mention{Datum}~-- the singulative~-- has become archaic, confined to philosophy-of-science contexts and pedantic style guides. Without a robust singulative in active use, the tight-linkage properties have nothing to attach to. Speakers who need to refer to a single piece of information say \mention{data point}, not \mention{datum}. But \mention{data point} is a compound, not a singulative of \mention{data}. It doesn't anchor the count cluster the way \mention{officer} anchors \mention{police}.

Result: \mention{data} drifts toward the loose end of the scale. Plural agreement weakens (\mention{this data is} becomes standard). Tight properties erode (\mention{three data} was always rare). Eventually, if the drift continues, \mention{data} will be fully mass~-- \mention{much data}, \mention{this data is}, \mention{some data}~-- with \mention{data point} handling any count function.

This is the quasi-count pattern run in reverse. \mention{Cattle} is stable because \mention{cow} exists. \mention{Data} is drifting because \mention{datum} is dying.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Cross-linguistic parallels}
\label{sec:9:cross-linguistic}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If bidirectional inference is a general mechanism~-- not an English quirk~-- languages with different morphological resources should show analogous patterns.

Welsh and Arabic mark singulatives morphologically. In Welsh, the base form of \mention{adar} (\enquote{birds}) is collective~-- grammatically singular, semantically aggregate. A suffix derives the singulative: \mention{aderyn} (\enquote{a bird}). \textcite{grimm2018} documents exactly the predicted pattern: bare collectives accept loose quantifiers but resist low numerals and distributives, while singulative-marked forms accept the full count cluster. The parallel to English quasi-count nouns is striking: Welsh collectives occupy the same position in the hierarchy that \mention{cattle} and \mention{police} occupy in English.

Classifier languages (Mandarin, Japanese) encode individuation differently~-- through classifiers that mediate between numerals and nouns, rather than through inflection on the noun itself. The prediction is that the clustering dynamics should shift to the classifier system: general classifiers should show tight/loose asymmetries parallel to what English quantifiers show. This remains programmatic~-- the cross-linguistic work hasn't been done to the depth needed~-- but the mechanism should apply wherever languages encode individuation morphosyntactically.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Sharp boundaries in fuzzy territory}
\label{sec:9:hyperreal}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

There's a puzzle lurking in the data. The hierarchy is continuous~-- properties shade from tight to loose~-- but judgments are often sharp. \mention{Three cattle} isn't \enquote{slightly bad} for most speakers; it's simply ungrammatical. \mention{Many cattle} isn't \enquote{slightly good}; it's fully acceptable. The boundary between what \mention{cattle} licenses and what it rejects feels determinate, even though the underlying individuation variable is gradient.

Chapter~\ref{ch:dynamic-discreteness} offered a framework for this: sharp-but-unknowable boundaries arising from tolerance dynamics. The hyperreal model lets us have determinate boundaries without anyone knowing precisely where they fall. Speakers act as if the boundary is sharp~-- they judge \mention{three cattle} ungrammatical, not merely marginal~-- but they can't articulate the precise point at which individuation becomes sufficient.

Countability confirms this picture. The \mention{folks} case is revealing: inter-speaker variation in the acceptability of \mention{three folks} is real and robust. Some speakers place the boundary above \mention{folks} (it clears \mention{three}); others place it below (it doesn't). Each speaker's individual grammar has a determinate answer, but the community doesn't. The gradience shows up in population variance, not in graded individual judgments.

This is the empirical signature of tolerance-based boundaries: sharpness within grammars, variance across them. The count/non-count distinction, for all its apparent fuzziness, behaves like a boundary system in which each speaker draws a line~-- just not the same line.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Passing the tests}
\label{sec:9:passing-tests}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Chapter~\ref{ch:failure-modes} introduced the Two-Diagnostic Test for genuine HPC kinds: high projectibility and robust homeostasis. Let's apply it to the count cluster.

\subsection{Projectibility}

The projectibility criterion asks: does recognizing this kind support successful induction? For the count cluster, the answer is yes.

If you know a noun is count, you can predict its behaviour across a range of grammatical contexts. It will take \mention{a(n)} and low cardinals. It will take \mention{many}/\mention{few}, not \mention{much}/\mention{little}. It will trigger plural agreement when plural. It will combine with distributives. Every one of these predictions is testable, and for canonical count nouns, every one succeeds.

Crucially, the predictions are \textit{gradient} by position in the hierarchy. Knowing that \mention{cattle} is quasi-count~-- that it accepts loose properties but rejects tight ones~-- lets you predict precisely which frames it will enter. The quasi-count pattern is projectible too. The category structure supports differentiated, not just all-or-nothing, induction.

Compare this to a merely nominal grouping~-- say, \enquote{nouns ending in -tion}. Knowing that \mention{nation} ends in -tion tells you nothing about its grammatical behaviour distinct from knowing it's a noun. The suffix doesn't support induction. Countability does.

\subsection{Homeostasis}

The homeostasis criterion asks: is the clustering maintained by causal mechanisms, or is it merely a surface pattern that might scatter under perturbation?

We've identified the mechanisms:

We have identified five interlocking mechanisms. \term{Bidirectional inference} couples morphosyntax to individuation through a shared semantic variable. \term{Acquisition} transmits this coupling as a unit, as children overgeneralise from one property to the others. \term{Entrenchment} preserves high-frequency patterns as chunks. \term{Institutional norms} stabilize the distribution at the community level. Finally, \term{functional anchoring} bleeds pressure from intermediate cases like \mention{cattle}, allowing them to persist without regularising.

This is robust homeostasis. Perturb the system~-- weaken individuation, remove a singulative anchor, expose learners to non-standard input~-- and the cluster responds in predictable ways. It's not just that count properties co-occur; it's that mechanisms push them toward co-occurrence. The clustering is maintained, not accidental.

\subsection{The verdict}

The count cluster passes both diagnostics. It supports induction (knowing a noun's count status predicts grammatical behaviour) and it's held together by mechanisms (bidirectional inference, acquisition, entrenchment, anchoring). It belongs in the upper-right quadrant of the diagnostic matrix: a genuine HPC kind.

This doesn't mean countability is simple. The hierarchy reveals internal structure; the quasi-count cases reveal tolerated heterogeneity; the diachronic cases reveal dynamic equilibria that can shift. But the claim isn't that HPC kinds are uniform. The claim is that they're real~-- that they're maintained by mechanisms and support induction. The count cluster qualifies.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{What does this buy us?}
\label{sec:9:payoff}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

The HPC analysis of countability does three things that the traditional feature-based account doesn't.

First, it explains the clustering, not just labels it. The feature [+count] says that count properties go together. The HPC account says \textit{why}: they're inferentially coupled to individuation, and mechanisms keep them coupled. The clustering isn't a brute fact; it's a consequence of how form and meaning interact in processing, acquisition, and transmission.

Second, it predicts the dissociation order. When individuation weakens, tight properties fail before loose ones. The hierarchy isn't stipulated; it follows from the precision demands of each property. No version of feature-bundle theory predicts this order. No version of prototype theory explains why the gradience has this particular shape.

Third, it explains stability and instability together. Quasi-count nouns are stable because of functional anchoring; \mention{folks} is unstable because it lacks anchoring. \mention{Pea} regularised because there was no anchor; \mention{data} is drifting because \mention{datum} is dying. The HPC account doesn't just describe which cases are stable~-- it explains why, and predicts which cases should be vulnerable to change.

The traditional question~-- \enquote{Is \mention{cattle} count or mass?}~-- obscures the underlying dynamics. The HPC question is: \enquote{Where does \mention{cattle} sit in the count basin, and what's holding it there?} That question has an answer. The first one doesn't.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Natural experiments}
\label{sec:9:natural-experiments}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If the count cluster is maintained by causal mechanisms, removing or altering those mechanisms should degrade the cluster in predictable ways. In the physical sciences, we would test this by ablation: knock out a gene, sever a neural connection, and observe the failure mode. In linguistics, we rely on natural experiments: languages that ``perturb'' the system by packaging the relevant cues differently (see \textcite{doetjes2012} for a typological overview).

These cross-linguistic comparisons function as \term{ablation by perturbation}. Languages don't typically delete a mechanism entirely; they reallocate or reweight it. A caution is required: real cross-linguistic work is messy. Unlike a controlled lab ablation, comparing languages involves multiple interacting variables (confounds). We can't simply \enquote{switch off} morphology while holding everything else constant. But by triangulating across different languages that ablate different parts of the mechanism, we can strengthen the inference. If the cluster degrades exactly as predicted in three mutually distinct ways, the case for the causal mechanism is robust.

We consider three such perturbations.

\subsection{Perturbation 1: The collective basin (Welsh)}

Here, the mechanism perturbed is the default basin of attraction. Standard Average European languages like English force a binary choice: singular (unmarked) or plural (marked). This obliges speakers to default to the singular for singletons. But what if the default were different?

The prediction is straightforward: if the singular default is removed, nouns denoting aggregates should settle into a \enquote{collective} base state that resists count syntax.

Welsh offers the evidence. For a large class of nouns~-- specifically those denoting things that naturally occur in groups (animals, vegetables, small objects)~-- the morphologically simple base form is \textit{collective} (conceptually plural). To refer to a single unit, speakers must add a \term{singulative} suffix. For example, \mention{\textit{adar}} means `birds' (collective); \mention{\textit{aderyn}} means `a bird' (singulative).

\textcite[532]{grimm2018} shows that Welsh base-form collectives behave exactly like English quasi-count nouns: they accept loose properties (quantifiers like \textit{llawer} `much/many') but reject tight properties (numerals) without singulative marking. The \mention{cattle} profile isn't a quirk of English irregulars; it's the stable attractor state for aggregate nouns when the pressure to act as a singular is removed. English \mention{cattle} is simply a Welsh collective trapped in a number-marking language.

This account offers a clear disconfirmation condition: if Welsh collectives accepted numerals directly (e.g., \mention{three birds-COLL}) without individuating morphology, it would falsify the claim that individuation requires active morphosyntactic maintenance.

\subsection{Perturbation 2: Weakened structural reinforcement}

In this case, we ablate the obligatory structural reinforcement. English countability is reinforced by obligatory number marking on (most) count nouns in argumental use. What happens if we remove this reinforcement? The prediction is that the count cluster should either disperse or naturally dissolve, or the functional load should shift to a new location.

One outcome is \term{reallocation}, as seen in Mandarin Chinese. Mandarin lacks obligatory noun number inflection. Without the constant \mention{book}/\mention{books} pulse trained by morphology, the inference engine builds the cluster differently. The functional load of individuation moves to the \term{classifier} system. The tight/loose clustering shifts to the interface between the numeral and the noun: \enquote{general} classifiers (like \mention{\textit{gè}}) tolerate vague individuation, while specific classifiers demand precise shape properties. The mechanism remains, but the locus of coupling has moved.

Another outcome is \term{weakening}, as argued for Halkomelem (Salish). \textcite{wiltschko2008} argues that in this language, plural marking isn't a functional head but an optional modifier. Speakers can say \mention{three boy} or \mention{three boys} with no truth-conditional change (p. 642). Unlike English, where the grammar forces a countability decision every time a noun is used, Halkomelem makes the decision optional. The prediction here is lower projectibility: without the obligatory morphological pulse, the \enquote{count} category should be less cohesive~-- and it's.

A third possibility is the \term{null case}. Some languages are claimed to lack both obligatory number and obligatory classifiers (e.g., Yoruba, Indonesian). These act as the control group. A pure-mechanism view makes a risky prediction: in the absence of \textit{any} reinforcing mechanism, there should be no rigid count/mass HPC. Nouns should be \term{transnumeral} (neutral), and semantic boundaries shouldn't predict grammatical behaviour. If these languages turned out to have a rigid, English-style count cluster without the mechanism, the HPC account would be falsified.

The disconfirmation condition is finding a language with no morphosyntactic maintenance but high countability projectibility.

\subsection{Perturbation 3: Reweighted semantics (Yudja)}

Finally, we can perturb the segmental semantics. A stricter ablation would be to remove the semantic constraint that numerals count \textit{atoms}.

The prediction is that if numerals can count portions directly, the \enquote{tightness} of the cluster should collapse.

\textcite{lima2014} argues that Yudja (Juruna family, Brazil) represents exactly this state. In Yudja, numerals combine directly with notional mass nouns: \mention{\textit{txabïu apeta}} (`three blood') is grammatical and interpreted as `three drops/spots of blood'. The constraint that numerals demand atomic units is relaxed or reweighted.

The result is a reordered hierarchy. Because the \enquote{tight} property (numerals) no longer demands high-precision atomic individuation, it becomes \enquote{looser} than it's in English. The failure mode here is \term{negative projectibility}: the inference "Accepts Numerals $\to$ Is Atomic Object" becomes unreliable.

The disconfirmation condition is simple: if Yudja speakers processed `three blood' as strictly atomic (coercing it to `three blood-cells' or 'three vials') despite the lack of marking, it would show that atomicity is a cognitive universal independent of language-specific weighting.

\subsection{The value of variation}

These natural experiments don't show that countability ``varies'' in the abstract; they show that the variation is \emph{mechanism-sensitive}. When a language reallocates where individuation cues live (number morphology vs.\ classifiers vs.\ singulatives), the correlational profile shifts in ways that are hard to describe as mere lexical accident. What we can legitimately claim on current evidence is conditional: \emph{if} the count cluster is homeostatically maintained by particular couplings, then perturbing those couplings should produce systematic failure modes. Welsh collectives and singulatives are a clean case in point: when the base form encodes collectivity and the singulative does the individuating work, the English quasi-count profile stops looking idiosyncratic and starts looking like an attractor state for aggregate-denoting nouns.

The broader programme is triangulation. No single language comparison is a controlled ablation, and confounds are unavoidable. But if independent perturbations repeatedly relocate the same functional load~-- or weaken the same implicational tendencies~-- that convergence is exactly what a mechanism story predicts. The risky disconfirmation condition isn't \enquote{countability varies} but the stronger one: finding a language with no comparable morphosyntactic maintenance while retaining English-like projectibility of the count cluster.

We can watch the same tug-of-war in miniature in the reception of \mention{emoji}. The borrowing entered English with an invariant form (as in Japanese), while the referents are maximally individuated: discrete, re-identifiable tokens. Predictably, usage pressures favour ordinary count packaging. Editorial policy hasn't been stable: the Associated Press~-- like the LEGO Group's lawyers fighting \mention{Legos}~-- at one point endorsed \mention{emojis} as the plural, and later reversed to recommend \mention{emoji} as both singular and plural. The oscillation is itself diagnostic. Institutions can slow drift, but they rarely get to stipulate equilibrium points; they respond to them. The basin doesn't dictate what anyone must think; it dictates which packages become effortless.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Audit output}
\label{sec:9:audit}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

\begin{table}[H]
\centering
\caption{Abbreviated HPC-kind audit for the count cluster.}
\label{tab:9:audit}
\small
\begin{tabular}{@{}p{3.5cm}p{10.5cm}@{}}
\toprule
\textbf{Step} & \textbf{Content} \\
\midrule
1. Target \& Scope & The count cluster in contemporary English; register-general; noun-level grain. \\
\addlinespace
2. Profile & Singular forms, cardinal selection, quantifier patterns (\mention{a(n)}, low cardinals, \mention{many}/\mention{few}), plural agreement. Properties form an implicational hierarchy from tight (precision-demanding) to loose (forgiving). \\
\addlinespace
3. Stabilizers & Bidirectional inference (couples form to individuation); acquisition (overgeneralization transmits cluster); entrenchment (high-frequency chunks resist change); functional anchoring (singulative lexemes bleed pressure); institutional norms (prescriptive feedback). \\
\addlinespace
4. Boundary behaviour & Quasi-count nouns (\mention{cattle}, \mention{police}) accept loose properties, reject tight ones. Object-mass nouns (\mention{furniture}) show form-cluster without semantic individuation. These are predicted peripheral positions, not anomalies. \\
\addlinespace
5. Failure-mode gate & The mechanism story isn't analyst convenience: bidirectional inference is testable via semantic priming and acquisition overgeneralization. Classification: \textbf{HPC kind}. \\
\addlinespace
6. Stress tests & (i) If quasi-count nouns show no correlation between functional-anchor presence and diachronic stability, reclassify them as frozen accidents. (ii) If a language lacks morphosyntactic reinforcement yet shows English-like count projectibility, the mechanism story is falsified. \\
\bottomrule
\end{tabular}
\end{table}

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Looking forward}
\label{sec:9:transition}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Countability is the clean baseline case. Not because it's simple~-- the hierarchy, the quasi-count intermediates, and the diachronic equilibria are all genuine complexity~-- but because the coupling is unusually tight. A semantic variable~-- individuation, which languages get \enquote{for free} from domain-general object cognition~-- is repeatedly reinforced by overt morphosyntax (number, quantifiers, agreement), so the cluster is highly projectible and the maintenance story is comparatively easy to see.

The next cases stress the framework in a different way. Definiteness (Chapter~\ref{ch:definiteness-and-deitality}) is an interface system too, but the form cluster and the function cluster may not line up as neatly. Articles, demonstratives, and possessives pattern together morphosyntactically, but the semantic work they do~-- identifiability, familiarity, uniqueness, deixis~-- doesn't always co-occur. The question won't be whether there \emph{is} a cluster, but whether we're looking at one HPC with internal fissures or two partially coupled ones.

Lexical categories (Chapter~\ref{ch:lexical-categories}) push in another direction. The noun/verb contrast looks strikingly stable cross-linguistically, while adjectives and adpositions vary in both inventory and behaviour. If lexical categories are HPC kinds, the mechanisms that maintain them can't be uniform: some must be deep and widely shared (acquisition pressures, constructional scaffolding), while others are local and reconfigurable. The payoff of the countability chapter was that a category can be both real and internally structured. The remaining chapters test the same claim under weaker couplings and noisier maintenance.
