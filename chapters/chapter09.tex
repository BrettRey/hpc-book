\chapter{Countability}
\label{ch:countability}

% Chapter 9: First Part III case study
% Target: ~8,000 words
% Metaphor: Lock-and-key hook → basin integration

\begin{quote}
\textit{A student learning English writes} \ungram{I bought three furnitures}. \textit{A copy editor debates whether to keep} this data \textit{or change to} these data. \textit{A Texan says} you folks \textit{where a Bostonian says} you guys. \textit{What do these have in common?}
\end{quote}

They're all negotiations at the boundary of English countability~-- a boundary that turns out to be more structured than it first appears. If the count/mass distinction were a simple binary feature, we'd expect either tidy uniformity or random scatter. What we actually see is stranger: a tightly clustered system that frays in an orderly way. The student who writes \mention{three furnitures} isn't making a random error. She's extending a pattern that usually works~-- and discovering, through the teacher's red ink, where the pattern stops.

This chapter applies the HPC framework to that pattern. Countability, I'll argue, isn't a definition waiting to be discovered or a convenient fiction imposed by grammarians. It's a homeostatic property cluster: a constellation of grammatical properties held together by mechanisms, not essence. The properties cluster because they're inferentially coupled to a common semantic variable~-- individuation~-- and they dissociate in a predictable order when that variable weakens.

If the argument succeeds, it does two things. First, it shows the machinery from Part II actually working on a real grammatical category. Second, it provides a template. By the end of this chapter, you should be able to look at any candidate category and ask: Does it pass the tests? What maintains its clustering? Where does it fray?

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The cluster}
\label{sec:9:cluster}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Count nouns in English share a constellation of properties. You can list them:

\begin{itemize}
\item \textbf{Singular--plural contrast:} \mention{book}/\mention{books}, \mention{dog}/\mention{dogs}
\item \textbf{Indefinite article:} \mention{a book}, \mention{an apple}
\item \textbf{Low cardinals:} \mention{three books}, \mention{five dogs}
\item \textbf{Count quantifiers:} \mention{many books}, \mention{few dogs}, \mention{several ideas}
\item \textbf{Distributives:} \mention{each book}, \mention{every dog}
\item \textbf{Plural agreement:} \mention{the books are}, not \ungram{\mention{the books is}}
\item \textbf{Demonstratives tracking number:} \mention{this book}/\mention{these books}
\end{itemize}

Mass nouns reject these frames: \ungram{\mention{a furniture}}, \ungram{\mention{three rices}}, \ungram{\mention{many equipments}}. They take their own quantifiers instead: \mention{much furniture}, \mention{some rice}, \mention{a lot of equipment}.

The striking thing isn't the distinction itself~-- that's Linguistics 101~-- but how tightly the count properties cluster. If you know a noun takes \mention{a(n)}, you can confidently predict it takes \mention{three}, \mention{many}, plural agreement, and the rest. The properties travel together. Grammars encode this as a feature: [\textpm count]. But features, as we saw in Chapter~2, are labels, not explanations. The question isn't whether count nouns share properties. The question is: why do these particular properties hang together? What keeps them bundled?

An essentialist answer would posit something all count nouns share~-- some property or feature that \textit{makes} them count nouns, from which the clustering follows. The problem is that no such essence has been found. The semantic correlate (individuation, atomicity, boundedness) comes in degrees, varies across contexts, and dissociates from the morphosyntax in systematic ways. \mention{Furniture} denotes discrete objects, yet it's grammatically mass. \mention{Cattle} denotes discrete animals, yet it resists \mention{a} and low cardinals.

The HPC view offers a different answer. The properties cluster not because they share an essence but because they're \textit{inferentially coupled} to a common semantic variable~-- and mechanisms at multiple timescales keep them coupled. The clustering is maintained, not given.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Three levels}
\label{sec:9:three-levels}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Before we can see the mechanism, we need a distinction. The word \textit{countability} conflates three different things:

\paragraph{Ontological discreteness.} Whether the referents exist as bounded units in the world. Rice grains, cattle, furniture items, and books are all physically discrete. But \mention{rice} and \mention{furniture} are grammatically mass. Physical discreteness doesn't determine grammatical behaviour.

\paragraph{Individuation.} Whether speakers \textit{construe} the referents as discrete, atomic units~-- units accessible to enumeration. This is a property of the construal, not the world. \mention{Furniture} denotes discrete objects, but the noun packages them as an unbounded superordinate category. You can point at a table and a chair and say \enquote{That's furniture}~-- but the noun doesn't foreground the table-ness and chair-ness as countable atoms. \mention{Cattle} similarly: the animals are ontologically discrete, but the noun construes them as an aggregate~-- a group, not a collection of enumerable individuals.

\paragraph{The count cluster.} The morphosyntactic syndrome: singular--plural contrast, article selection, numeral compatibility, quantifier choice, agreement. This is what grammars describe when they mark a noun as [+count] or [$-$count]. I'll call this constellation the \textbf{count cluster}~-- the set of formal properties that typically travel together where count morphosyntax is involved.

These three levels dissociate systematically. \mention{Rice}: ontologically discrete (grains are bounded units), low individuation (construed as granular aggregate), outside the count cluster. \mention{Cattle}: ontologically discrete (cows are bounded animals), weak individuation (construed as group rather than atoms), \textit{partially} in the count cluster~-- it accepts \mention{many cattle} but rejects \mention{three cattle}. \mention{Book}: aligned across all three levels~-- ontologically discrete, strongly individuated, fully in the count cluster.

The distinction matters because it locates the mechanism. The homeostasis isn't between world and grammar; it's between \textit{construal} and grammar. Individuation~-- the degree to which a noun packages its referents as accessible atoms~-- is the semantic variable that the count cluster tracks. When individuation is strong, the cluster holds. When it weakens, the cluster frays. But it frays in an orderly way.

A terminological note. I'll use \textbf{individuation} for the semantic variable and \textbf{count cluster} for the morphosyntactic syndrome. \textit{Countability}, then, names what emerges when the two meet~-- the characteristic pattern of English nouns that construe their referents as atomic and select the corresponding grammatical frames. The chapter title isn't a level; it's the phenomenon that arises when individuation and the count cluster are coupled.


%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The mechanism: Bidirectional inference}
\label{sec:9:mechanism}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Why do count properties cluster? Here's the short answer: because they all point to the same thing.

Each count property is a cue~-- a piece of morphosyntactic evidence that hearers use to infer how the speaker is construing the referent. When you hear \mention{three dogs}, you infer that the speaker has individuated the referents~-- construed them as discrete, enumerable units. When you hear \mention{much water}, you infer a mass construal: stuff, not atoms. The morphosyntax generates expectations about the construal; the construal, in turn, constrains morphosyntactic choice.

This is \textbf{bidirectional inference}:

\begin{itemize}
\item \textbf{Comprehension direction:} Count morphosyntax $\to$ individuated construal. Hearing \mention{a}, \mention{three}, or \mention{many} with a noun leads the hearer to expect atomic, enumerable referents.
\item \textbf{Production direction:} Individuated construal $\to$ count morphosyntax. A speaker who has individuated the referents will reach for \mention{a}, \mention{three}, or \mention{many}.
\end{itemize}

The properties cluster because they're all coupled to the same semantic variable. Knowing that a noun takes \mention{a(n)} tells you it supports individuation~-- and if it supports individuation, it should take \mention{three} and \mention{many} and plural agreement too. The generalisation runs through the construal, not through the morphosyntax directly. That's why the properties are mutually predictive: not because any one \textit{causes} the others, but because they're all symptoms of the same underlying condition.

\subsection{Multi-timescale maintenance}

The bidirectional inference mechanism operates at multiple timescales, each reinforcing the others:

\paragraph{Processing (milliseconds).} Every time a speaker produces or comprehends a count frame, the form--meaning link is activated. \mention{Three dogs} primes the expectation of individuation; individuation primes the expectation of count morphosyntax. Mismatches~-- \ungram{\mention{three furnitures}}, say~-- incur processing costs. They feel wrong because they violate entrenched expectations.

\paragraph{Acquisition (years).} Children don't learn count properties one by one. They learn that count morphosyntax correlates with individuation. Classic acquisition work shows that children overgeneralise: encountering a noun in one count frame, they extend it to others \citep{bloom1994a,gordon1985}. A child who hears \mention{many police} may try \ungram{\mention{three police}}, treating the loose property as evidence for the tight ones. This is exactly what you'd expect if the cluster is acquired as a unit~-- if learners are tracking construal, not memorising property lists.

\paragraph{Transmission (decades).} Institutional forces~-- style guides, copy editors, teachers, grammar checkers~-- enforce canonical patterns and resist drift. The \mention{data}/\mention{datum} debate lives here. As \mention{datum} recedes from editorial practice, the normative anchor weakens and \mention{data} drifts toward mass status. The institutional layer doesn't \textit{create} the cluster, but it stabilises it at the community level.

The mechanisms interlock: the fast loop generates usage patterns; the slow loop crystallises them into community standards; acquisition transmits the crystallised patterns to new learners, who then participate in the fast loop. Perturbation to any level~-- a semantic shift that weakens individuation, a prescriptive shift that sanctions formerly deviant forms~-- ripples through the system and changes the equilibrium. The cluster isn't static. It's dynamically maintained.

\subsection{The chunking story}

There's a cognitive dimension to why high-frequency patterns resist change. Following \textcite{bybee2010}, we can understand entrenchment in terms of \textit{chunking}: high-frequency sequences get stored as units and accessed as wholes, bypassing compositional assembly.

\mention{Many cattle} is a chunk. English speakers have encountered it often enough that it's stored in memory and retrieved directly. \mention{Three cattle} isn't a chunk~-- it has to be assembled online, and the assembly fails because the components don't fit. The noun's stored profile resists the frame the speaker is trying to build.

This is why quasi-count nouns feel unstable in tight frames but natural in loose ones. \mention{Many cattle} is retrieved; \mention{three cattle} is constructed and rejected. The difference between the two isn't a difference in categorical membership. It's a difference in processing: one path is entrenched; the other isn't.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{The hierarchy: Tight before loose}
\label{sec:9:hierarchy}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If all count properties were equally sensitive to individuation, any weakening would collapse the cluster entirely. A noun would be fully count or fully mass, nothing in between. But that's not what we see. \mention{Cattle} and \mention{police} sit in the middle: they accept \textit{some} count properties while rejecting others. And the pattern isn't random. The properties peel off in order.

Here's why. Although all count properties are coupled to individuation, they're not equally demanding. Some require high-precision individuation~-- exact atomic units, sharply bounded. Others tolerate lower precision~-- approximate magnitude, vague plurality. When individuation weakens, the demanding properties fail first.

\subsection{Locks with different tolerances}

Think of it this way. Each count construction~-- \mention{a} N, \mention{three} N, \mention{many} N~-- is a lock with a specific tolerance. The noun provides a key: its individuation profile. If the key clears the tolerance, the construction is licensed.

\mention{A} is a precision lock. It requires identifying exactly one atomic unit. The key must be cut to exact specifications.

\mention{Three} is nearly as precise. It requires enumerating exactly three atoms~-- no approximation, no vagueness.

\mention{Many} is a forgiving lock. It requires only magnitude assessment: \enquote{a contextually large quantity.} The atoms don't need sharp boundaries; they just need to be roughly atom-shaped.

\mention{Plural agreement} is the most forgiving. It requires only that the referent be construed as non-singular. Virtually anything that's in the count basin clears this one.

\mention{Book} is a precision-machined key. It opens every lock in the system. \mention{Cattle} is a blunter key. It gets through the forgiving locks~-- \mention{many cattle}, plural agreement~-- but can't open the precision locks: \ungram{\mention{a cattle}}, \ungram{\mention{three cattle}}.

\subsection{From locks to basins}

But here's how this connects to what we've seen before. Remember the spinning top from Chapter~\ref{ch:kinds-without-essences}? A category isn't a container you're in or out of~-- it's an attractor keeping a top upright. The count cluster is that basin. The locks? They're positions \textit{within} the basin. Tight locks sit at the centre; loose locks ring the edges.

When we say \mention{cattle} clears the loose locks but fails the tight ones, we're saying its top spins stably in the basin~-- but off-centre. It's count-ish. It's in the attractor. But it's not at the core.

\mention{Book} spins at the dead centre, satisfying every precision demand. \mention{Cattle} spins off-centre but stably~-- it's clearly in the count basin, but it can't reach the precision locks at the centre. \mention{Folks}~-- we'll come to this~-- wobbles. Sometimes it clears the \mention{three} lock, sometimes it doesn't. Its position in the basin is unstable.

This integrated image~-- locks as positions in a basin, precision as distance from centre~-- unifies the immediate intuition (locks and keys) with the dynamic stability framework (spinning tops and attractors). The hierarchy of count properties \textit{is} the geometry of the basin. The further from centre a lock sits, the more tolerance it has.

\subsection{The implicational pattern}

The hierarchy generates a prediction: for any noun and any two properties, if it accepts the tighter one, it accepts the looser one; if it rejects the looser one, it rejects the tighter one. The distribution should be triangular. No noun should stably accept \mention{three} while rejecting \mention{many}. No noun should require \mention{a(n)} while taking \mention{much}.

This is falsifiable. Finding a noun that reverses the pattern~-- tight without loose~-- would challenge the account. The empirical record, as far as I can determine, contains no such case.

Here's the hierarchy, ordered from tightest to loosest:

\begin{enumerate}
\item \textbf{Singular form / \mention{a(n)}} — Requires identifying exactly one atomic unit
\item \textbf{Low cardinals (\mention{three}, \mention{five})} — Requires enumerating precise atomic units
\item \textbf{\mention{Several}} — Requires multiple discrete units, tolerates approximate quantity (\enquote{more than two, not many})
\item \textbf{Distributives (\mention{each}, \mention{every})} — Requires discrete units for distribution
\item \textbf{\mention{Many}/\mention{few}} — Requires only relative magnitude assessment, not precise enumeration
\item \textbf{High round numerals (\mention{a hundred}, \mention{thousands of})} — Function as approximate measures
\item \textbf{Plural agreement} — Requires only non-singular construal
\end{enumerate}

A noun that loses \mention{several} will already have lost \mention{a(n)} and low cardinals. A noun that retains only plural agreement will have lost everything above it. The properties form an implicational scale, and a noun's countability profile is its position on that scale.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Quasi-count nouns: The stable intermediates}
\label{sec:9:quasi-count}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

\textit{CGEL} identifies a class of \textbf{quasi-count nouns}: plural-only nouns that take plural agreement and accept \mention{many} but resist singular forms, \mention{a(n)}, and low cardinals \citep[p.~345]{huddleston2002}. The core cases are \mention{cattle}, \mention{police}, \mention{poultry}, \mention{vermin}, \mention{livestock}, and \mention{clergy}.

They occupy exactly the position the hierarchy predicts: loose properties accepted, tight properties rejected.

\begin{itemize}
\item \mention{Many cattle} — \cmark
\item \mention{The cattle are grazing} — \cmark
\item \mention{Several cattle} — marginal for some speakers
\item \mention{Three cattle} — \xmark
\item \mention{A cattle} — \xmark
\end{itemize}

The same pattern holds for \mention{police}: \mention{many police}, \mention{the police are investigating}, but \ungram{\mention{three police}}, \ungram{\mention{a police}}.

Crucially, none of these nouns reverses the hierarchy. None accepts tight properties while rejecting loose ones. The triangular pattern holds.

\subsection{Why are they stable?}

If the homeostatic mechanism creates pressure toward coherence~-- pushing nouns toward full count or full mass~-- why haven't \mention{cattle} and \mention{police} drifted? They've been quasi-count for centuries.

The answer is \textbf{functional anchoring}. When speakers need singulative reference~-- when they need to name \textit{one} cow or \textit{one} officer~-- they don't attempt \ungram{\mention{a cattle}} or \ungram{\mention{a police}}. They use \mention{cow}, \mention{bull}, \mention{head of cattle}, or \mention{officer}. These alternative lexemes handle the singulative function, relieving pressure on the quasi-count noun to develop tight-linkage properties.

Bidirectional inference generates expectations. If a noun accepts \mention{many}, hearers may expect it to accept \mention{three} and \mention{a}. When those expectations fail, pressure arises either to regularise (extend tight properties) or to avoid the construction. If an alternative lexeme satisfies the singulative function, speakers have no reason to force the quasi-count noun into tight frames. The pressure dissipates.

This is \textit{passive} persistence, not active maintenance. \mention{Cattle} doesn't serve some function that requires it to stay quasi-count. It simply faces no pressure to change, because \mention{cow} is handling the job that analogy would otherwise push \mention{cattle} to fill. The equilibrium is stable because the communicative ecology absorbs the force that would otherwise destabilise it.

Compare this to \mention{police}. \mention{Officer} is robust, frequent, and semantically close. Speakers who need to refer to a single police officer have an obvious lexical resource. Result: \mention{police} has remained quasi-count for as long as we have reliable records.

\subsection{The unstable case: \mention{folks}}

Not all intermediates are stable. \mention{Folks} occupies the boundary zone where the hierarchy predicts variability.

American English \mention{folks} sits between \mention{people} (fully count: \mention{three people}, \mention{a person}) and the quasi-count class. For many speakers, \mention{many folks} and \mention{several folks} are fully acceptable. But \mention{three folks} is often judged marked, informal, or slightly odd. Some speakers reject it; others accept it readily; still others accept it but hear it as slangier than \mention{three people}.

The HPC account predicts this instability. \mention{Folks} lacks a functional anchor. There's no singulative \ungram{\mention{a folk}} in ordinary use, and \mention{person} is semantically distinct~-- neutral rather than in-group. Unlike \mention{police} (anchored by \mention{officer}) and \mention{cattle} (anchored by \mention{cow}), \mention{folks} has nothing to bleed the pressure for regularisation.

Moreover, corpus data confirms that \mention{folks} is suppressed relative to \mention{people} in tight frames. In COCA, \mention{three people} occurs at 2,226 per million tokens of \mention{people}; \mention{three folks} occurs at only 258 per million tokens of \mention{folks}~-- an 8.6-fold suppression. The loose property (\mention{many}) is suppressed only about 2.3-fold. Tight properties are hit harder than loose ones, exactly as the hierarchy predicts.

The instability of \mention{folks} is not noise. It's structure. The noun is wobbling in the basin, uncertain whether it will stabilise off-centre (like \mention{cattle}), drift outward (toward mass), or regularise toward the centre (developing tight properties). The outcome depends on whether a functional anchor emerges or prescriptive pressure crystallises.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Diachronic signatures}
\label{sec:9:diachronic}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If the homeostatic mechanism is real, it should leave traces in the historical record. Two cases illuminate how the cluster extends and how it erodes.

\subsection{How the cluster self-completes: \mention{pea}}

We mentioned \mention{pea} in Chapter~\ref{ch:words-wont-hold-still}. Here's the fuller story.

Speakers of Middle English heard \mention{pease} with a final /z/ sound~-- the word for the vegetable, used in \mention{pease porridge} and \mention{pease pudding}. The noun was non-count: you had \mention{much pease}, not \mention{many pease}. But the final /z/ was phonologically identical to the plural suffix, and at some point speakers reanalysed it as such.

Once \mention{pease} was heard as a plural, the bidirectional inference mechanism kicked in. Plural morphology cues individuation. If \mention{pease} is plural, where's the singular? Speakers who expected the cluster to cohere created a gap~-- and filled it. They back-formed \mention{pea}. Then they extended the count cluster: \mention{a pea}, \mention{three peas}, \mention{many peas}. The count cluster didn't just emerge. Speakers \textit{built} it, one inference at a time.

The mechanism is visible here: cluster pressure creates gaps; gaps get filled. If the bidirectional inference story is right, we'd expect the historical record to show loose properties established before tight ones~-- the cluster building from the outside in. The evidence is suggestive though incomplete; what's clear is that the reanalysis triggered a cascade of count-property adoption.

\subsection{How the cluster erodes: \mention{data}}

The reverse trajectory is visible in \mention{data}. Historically the plural of \mention{datum}, it's shifting toward mass status: \mention{this data is}, \mention{much data} are now common, especially in informal and spoken registers. Corpus studies confirm that singular agreement with \mention{data} now predominates in most registers \citep{garner2016}.

Why is \mention{data} drifting? Because its functional anchor is disappearing. \mention{Datum}~-- the singulative~-- has become archaic, confined to philosophy-of-science contexts and pedantic style guides. Without a robust singulative in active use, the tight-linkage properties have nothing to attach to. Speakers who need to refer to a single piece of information say \mention{data point}, not \mention{datum}. But \mention{data point} is a compound, not a singulative of \mention{data}. It doesn't anchor the count cluster the way \mention{officer} anchors \mention{police}.

Result: \mention{data} drifts toward the loose end of the scale. Plural agreement weakens (\mention{this data is} becomes standard). Tight properties erode (\mention{three data} was always rare). Eventually, if the drift continues, \mention{data} will be fully mass~-- \mention{much data}, \mention{this data is}, \mention{some data}~-- with \mention{data point} handling any count function.

This is the quasi-count pattern run in reverse. \mention{Cattle} is stable because \mention{cow} exists. \mention{Data} is drifting because \mention{datum} is dying.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Cross-linguistic parallels}
\label{sec:9:cross-linguistic}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

If bidirectional inference is a general mechanism~-- not an English quirk~-- languages with different morphological resources should show analogous patterns.

Welsh and Arabic mark singulatives morphologically. In Welsh, the base form of \mention{adar} (\enquote{birds}) is collective~-- grammatically singular, semantically aggregate. A suffix derives the singulative: \mention{aderyn} (\enquote{a bird}). \textcite{grimm2018} documents exactly the predicted pattern: bare collectives accept loose quantifiers but resist low numerals and distributives, while singulative-marked forms accept the full count cluster. The parallel to English quasi-count nouns is striking: Welsh collectives occupy the same position in the hierarchy that \mention{cattle} and \mention{police} occupy in English.

Classifier languages (Mandarin, Japanese) encode individuation differently~-- through classifiers that mediate between numerals and nouns, rather than through inflection on the noun itself. The prediction is that the clustering dynamics should shift to the classifier system: general classifiers should show tight/loose asymmetries parallel to what English quantifiers show. This remains programmatic~-- the cross-linguistic work hasn't been done to the depth needed~-- but the mechanism should apply wherever languages encode individuation morphosyntactically.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Sharp boundaries in fuzzy territory}
\label{sec:9:hyperreal}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

There's a puzzle lurking in the data. The hierarchy is continuous~-- properties shade from tight to loose~-- but judgments are often sharp. \mention{Three cattle} isn't \enquote{slightly bad} for most speakers; it's simply ungrammatical. \mention{Many cattle} isn't \enquote{slightly good}; it's fully acceptable. The boundary between what \mention{cattle} licenses and what it rejects feels determinate, even though the underlying individuation variable is gradient.

Chapter~\ref{ch:dynamic-discreteness} offered a framework for this: sharp-but-unknowable boundaries arising from tolerance dynamics. The hyperreal model lets us have determinate boundaries without anyone knowing precisely where they fall. Speakers act as if the boundary is sharp~-- they judge \mention{three cattle} ungrammatical, not merely marginal~-- but they can't articulate the precise point at which individuation becomes sufficient.

Countability confirms this picture. The \mention{folks} case is revealing: inter-speaker variation in the acceptability of \mention{three folks} is real and robust. Some speakers place the boundary above \mention{folks} (it clears \mention{three}); others place it below (it doesn't). Each speaker's individual grammar has a determinate answer, but the community doesn't. The gradience shows up in population variance, not in graded individual judgments.

This is the empirical signature of tolerance-based boundaries: sharpness within grammars, variance across them. The count/mass distinction, for all its apparent fuzziness, behaves like a boundary system in which each speaker draws a line~-- just not the same line.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Passing the tests}
\label{sec:9:passing-tests}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Chapter~\ref{ch:failure-modes} introduced the Two-Diagnostic Test for genuine HPC kinds: high projectibility and robust homeostasis. Let's apply it to the count cluster.

\subsection{Projectibility}

The projectibility criterion asks: does recognising this kind support successful induction? For the count cluster, the answer is clearly yes.

If you know a noun is count, you can predict its behaviour across a range of grammatical contexts. It will take \mention{a(n)} and low cardinals. It will take \mention{many}/\mention{few}, not \mention{much}/\mention{little}. It will trigger plural agreement when plural. It will combine with distributives. Every one of these predictions is testable, and for canonical count nouns, every one succeeds.

Crucially, the predictions are \textit{gradient} by position in the hierarchy. Knowing that \mention{cattle} is quasi-count~-- that it accepts loose properties but rejects tight ones~-- lets you predict precisely which frames it will enter. The quasi-count pattern is projectible too. The category structure supports differentiated, not just all-or-nothing, induction.

Compare this to a merely nominal grouping~-- say, \enquote{nouns ending in -tion}. Knowing that \mention{nation} ends in -tion tells you nothing about its grammatical behaviour distinct from knowing it's a noun. The suffix doesn't support induction. Countability does.

\subsection{Homeostasis}

The homeostasis criterion asks: is the clustering maintained by causal mechanisms, or is it merely a surface pattern that might scatter under perturbation?

We've identified the mechanisms:

\begin{itemize}
\item \textbf{Bidirectional inference} couples morphosyntax to individuation, making the properties mutually reinforcing through a shared semantic variable.
\item \textbf{Acquisition} transmits the cluster as a unit: children overgeneralise, treating one count property as evidence for the rest.
\item \textbf{Entrenchment} preserves high-frequency patterns as chunks, resisting analytic decomposition.
\item \textbf{Institutional norms} stabilise the community-level distribution.
\item \textbf{Functional anchoring} bleeds pressure on intermediates, explaining why quasi-count nouns persist rather than regularising.
\end{itemize}

This is robust homeostasis. Perturb the system~-- weaken individuation, remove a singulative anchor, expose learners to non-standard input~-- and the cluster responds in predictable ways. It's not just that count properties co-occur; it's that mechanisms push them toward co-occurrence. The clustering is maintained, not accidental.

\subsection{The verdict}

The count cluster passes both diagnostics. It supports induction (knowing a noun's count status predicts grammatical behaviour) and it's held together by mechanisms (bidirectional inference, acquisition, entrenchment, anchoring). It belongs in the upper-right quadrant of the diagnostic matrix: a genuine HPC kind.

This doesn't mean countability is simple. The hierarchy reveals internal structure; the quasi-count cases reveal tolerated heterogeneity; the diachronic cases reveal dynamic equilibria that can shift. But the claim isn't that HPC kinds are uniform. The claim is that they're real~-- that they're maintained by mechanisms and support induction. The count cluster qualifies.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{What does this buy us?}
\label{sec:9:payoff}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

The HPC analysis of countability does three things that the traditional feature-based account doesn't.

\paragraph{It explains the clustering, not just labels it.} The feature [+count] says that count properties go together. The HPC account says \textit{why}: they're inferentially coupled to individuation, and mechanisms keep them coupled. The clustering isn't a brute fact; it's a consequence of how form and meaning interact in processing, acquisition, and transmission.

\paragraph{It predicts the dissociation order.} When individuation weakens, tight properties fail before loose ones. The hierarchy isn't stipulated; it follows from the precision demands of each property. No version of feature-bundle theory predicts this order. No version of prototype theory explains why the gradience has this particular shape.

\paragraph{It explains stability and instability together.} Quasi-count nouns are stable because of functional anchoring; \mention{folks} is unstable because it lacks anchoring. \mention{Pea} regularised because there was no anchor; \mention{data} is drifting because \mention{datum} is dying. The HPC account doesn't just describe which cases are stable~-- it explains why, and predicts which cases should be vulnerable to change.

The traditional question~-- \enquote{Is \mention{cattle} count or mass?}~-- is the wrong question. The HPC question is: \enquote{Where does \mention{cattle} sit in the count basin, and what's holding it there?} That question has an answer. The first one doesn't.

%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --
\section{Looking forward}
\label{sec:9:transition}
%~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- ~-- --

Countability was the easy case. Not because it's simple~-- the hierarchy, the quasi-count intermediates, the diachronic dynamics all have genuine complexity~-- but because countability passes the HPC tests cleanly. The clustering is tight, the mechanism is identifiable, the predictions are borne out.

The next cases are harder. Definiteness (Chapter~\ref{ch:definiteness-and-deitality}) presents a puzzle: the grammatical form cluster (articles, demonstratives, possessives) and the semantic function cluster (referent identification, familiarity, uniqueness) may not align perfectly. Are they one HPC or two overlapping ones? Word classes (Chapter~\ref{ch:word-classes}) raise similar questions: why are nouns and verbs stable cross-linguistically while adjectives vary? Is the noun/verb distinction one HPC or multiple?

The template developed here~-- identify the cluster, locate the mechanism, test projectibility and homeostasis, trace the fraying~-- applies to all of them. Countability shows the method working. The remaining chapters show it being tested.
