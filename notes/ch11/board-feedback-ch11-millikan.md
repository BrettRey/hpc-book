# Board Feedback: Chapter 11 (Word Classes)
## Ruth Millikan

This chapter's framework sits perfectly at the intersection of proper function theory and evolutionary stability. Let me probe three dimensions.

### 1. The Proper Function of Word Classes

Word classes inherit their proper function from *information flow across the learning system*. A noun's proper function isn't "to refer to entities"—that's too thin and makes malfunction too easy to hide. Instead: **nouns stabilize the mapping between acoustic patterns and learnable distributed patterns in the input**. Specifically, nouns aggregate over distinct perceptual objects in ways that systematically co-occur with particular morphosyntactic positions and agreement patterns.

The critical move: the function is *relational and ecological*. Nouns succeed when they carve learnable joints in speakers' experience—joints that other parts of the grammar (determiners, verbs, agreement systems) have been shaped to exploit. A noun that didn't support reliable generalizations would be failing its proper function, even if it referred.

This connects to your HPC framework: the mechanisms stabilizing word class clusters *just are* the mechanisms by which proper functions get established. Entrenchment, acquisition success, interactive alignment—these aren't separate from function; they're what function *looks like from inside a language system*.

### 2. Reproduction Across Speakers

Here your account should emphasize: word classes reproduce because they serve coordinated functions in *learning bottlenecks*. When children acquire language, they must infer distributional properties from finite exposure. Nouns are reliable; they mark certain generalizable patterns. A child who mistakenly treats "furniture" as a noun succeeds more than one who treats "glibberish" as a noun.

The reproduction is *design-like* without designers. Each speaker's acquisition system is shaped by the preceding generation's linguistic output. Word classes survive because they solve a real problem: **making distributed grammatical patterns learnable from exposure**. The morphosyntactic "payoff" that your failure-modes chapter mentions isn't arbitrary—it's grounded in what acquisition systems can actually extract from the signal.

Cross-linguistic stability of nouns and verbs follows. These classes capture distinctions that are deeply constrained by (a) how humans perceive objects vs. events, and (b) what distributional patterns children can reliably track. Adjectives and adverbs vary because they solve that learning problem less uniformly.

### 3. Malfunction and Detection

Malfunction would look like: **reliable failure in the learning bottleneck**. A word class malfunctions when speakers can't use it to predict or generalize beyond the teaching set. This is why you're right to emphasize projectibility as diagnostic.

Signs of malfunction:
- Learners misclassify consistently (not randomly or context-sensitively, but *systematically*).
- The class fails to support compositional inference (you can't predict the behavior of novel members from old members).
- Stabilizing mechanisms withdraw: adults stop reinforcing it, children stop adopting it, discourse stops exploiting it.

Your "thin clustering" maps directly onto proper function failure: too little homeostasis means the class drifts below the threshold where it can reliably guide induction. Your "fat clustering" is different—multiple proper functions collapsed into one category, so removing one mechanism destabilizes everything.

The adjective-adverb variation you're tracking isn't malfunction; it's *equilibrium under different architectural constraints*. Some languages "solve" the problem one way, others differently. Both work because both carve learnable joints—just at different granularities.

**The upshot:** Your HPC machinery *just is* the machinery of proper function in language systems. You don't need to add teleology; the account already has it built in through the learning bottleneck.
