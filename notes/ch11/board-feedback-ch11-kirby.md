# Board Member Feedback: Chapter 11 (Word Classes)
**Reviewer:** Simon Kirby
**Date:** 25 December 2025

The application of Homeostatic Property Cluster (HPC) theory to word classes is a compelling move. It aligns neatly with the view of language as a complex adaptive system. However, to truly explain the "architectural equilibrium" you posit, you need to explicitly integrate the dynamics of cultural evolution.

### 1. Cultural Transmission as the Primary Stabilizer
You discuss various mechanisms stabilizing these categories. I urge you to centralize **cultural transmission** as the ultimate sieve. In our iterated learning simulations, stability (homeostasis) isn't inherent to the signal; it arises because certain structures pass through the "bottleneck" of learning more easily. A word class becomes a stable HPC because it is "contagious"—highly compressible and easily inferable from sparse data. The clustering of properties (morphological, distributional, semantic) exists because that specific clustering maximizes learnability. The "mechanism" is the transmission chain itself filtering out the hard-to-learn variants.

### 2. Noun/Verb vs. Adjective: Basins of Attraction
Iterated learning predicts that the strongest attractors in the landscape of possible grammars will be the most cross-linguistically universal. Nouns and Verbs likely map onto deep cognitive priors (reference and predication) that, when fed into the transmission bottleneck, rapidly crystallize into distinct categories. 

Adjectives and adverbs, however, represent a functionally secondary layer. In ILM terms, the "selection pressure" for a dedicated Adjective class is weaker. The system can often solve the communicative task by treating property-words as verbs (stative) or nouns. Consequently, the Adjective category is a **shallower basin of attraction**—stable enough to exist in many systems, but fragile enough to drift, merge, or behave "continuously" in others. This explains the asymmetry without needing a new "essence" for adjectives.

### 3. Framing Cross-Generational Mechanisms
You should frame cross-generational mechanisms not as static "enforcers" or "policing," but as **dynamic filters**. The "homeostasis" you describe is exactly what we call convergence to an attractor. The mechanism is the learner's inductive bias amplifying slight statistical tendencies in the input, correcting drift and preventing the category from dissolving into noise.
